[
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html",
    "title": "Vulnerability Analysis Calculations",
    "section": "",
    "text": "In the context of the development of the Country Climate and Development Report (CCDR) for Armenia, the poverty team is contributing with inputs for vulnerability analysis at the household level. The methods for these inputs are in active development and benefit greatly from the practical applications and interdisciplinary discussions that take place during the creation of these CCDRs. This guide aims to document the steps carried out to link vulnerability impacts and household survey data.\nAs a convention, code is presented in the following format in this guide:\n\n# Some comment that is not evaluated by R\nsome_variable &lt;- some_function(some_object, some_parameter = TRUE)\n\nWe assume that the reader has created an Rstudio project and is familiar with basic R functions. Within that project we recommend the following file structure:\nroot/\n├── scripts\n│   └── my_script.R\n├── data/\n|   ├── my_data.sav\n|   ├── my_data.dta\n|   └── my_data.csv\n└── output\n    ├── my_output1.csv\n    └── my_output2.xlsx\nUsing RStudio project makes it possible to not use setwd() to establish the root directory and refer to subdirectories in a relative manner, making interoperability easier within teams and not hard coding a particular computer’s file structure into the code. If you are not using RStudio, just add setwd(r'(C:\\My\\path\\to\\project\\root)') at the beginning of your coding session.",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html#introduction",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html#introduction",
    "title": "Vulnerability Analysis Calculations",
    "section": "",
    "text": "In the context of the development of the Country Climate and Development Report (CCDR) for Armenia, the poverty team is contributing with inputs for vulnerability analysis at the household level. The methods for these inputs are in active development and benefit greatly from the practical applications and interdisciplinary discussions that take place during the creation of these CCDRs. This guide aims to document the steps carried out to link vulnerability impacts and household survey data.\nAs a convention, code is presented in the following format in this guide:\n\n# Some comment that is not evaluated by R\nsome_variable &lt;- some_function(some_object, some_parameter = TRUE)\n\nWe assume that the reader has created an Rstudio project and is familiar with basic R functions. Within that project we recommend the following file structure:\nroot/\n├── scripts\n│   └── my_script.R\n├── data/\n|   ├── my_data.sav\n|   ├── my_data.dta\n|   └── my_data.csv\n└── output\n    ├── my_output1.csv\n    └── my_output2.xlsx\nUsing RStudio project makes it possible to not use setwd() to establish the root directory and refer to subdirectories in a relative manner, making interoperability easier within teams and not hard coding a particular computer’s file structure into the code. If you are not using RStudio, just add setwd(r'(C:\\My\\path\\to\\project\\root)') at the beginning of your coding session.",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html#preamble",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html#preamble",
    "title": "Vulnerability Analysis Calculations",
    "section": "2 Preamble",
    "text": "2 Preamble\nWe start with a clean environment, making sure that any objects from a previous session are not present. We take this opportunity to keep our country ISO code in a variable iso in case we need it later.\n\n# Clean workspace\nrm(list = ls())\n\n# Armenia country ISO code\niso &lt;- \"ARM\"\n\n# Exchange rate USD per dram\ner &lt;- 0.002310\n\nRather than calling our libraries as we go, we will make sure we have everything we need from the beginning.\n\n# Load packages\nlibrary(tidyverse) # includes dplyr, ggplot2 and others\nlibrary(haven)     # to read SPSS and Stata datasets\nlibrary(readxl)    # to read from MS-Excel\nlibrary(openxlsx)  # to write to MS-Excel.\nlibrary(gt)        # pretty tables\nlibrary(car)       # Companion to applied regression\nlibrary(modelr)    # regression models\nlibrary(janitor)   # pretty subtotals\nlibrary(purrr)     # map vectors (aggregation)\n\n# Geopackages\nlibrary(sf)        # to read and write shapefile maps\nlibrary(terra)     # to perform geocalculations\nlibrary(tmap)      # for static and interactive maps",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html#datasets",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html#datasets",
    "title": "Vulnerability Analysis Calculations",
    "section": "3 Datasets",
    "text": "3 Datasets\nWe then load the datasets that we need for this study. We are lucky that the World Bank has processed some of these already for poverty analysis and so we have the original SPSS datasets with all variables for Households hh and for Individuals pp, as well as a consumption aggregate ca and a household income ic dataset, which are Stata datasets. This is for the year 2022. These are imported using the haven package. These are based on Armenia Integrated Living Conditions Survey 2022 (ARMSTAT, 2023).\n\n# Original SPSS datasets\n# Households (hh)\nhh &lt;- read_sav(\n  \"data/ARM-HH-survey/original-spss-files/ILCS-ARM-2022-Households.sav\")\n# Persons (pp)\npp &lt;- read_sav(\n  \"data/ARM-HH-survey/original-spss-files/ILCS-ARM-2022-Persons.sav\")\n\n# Processed WB datasets\n# Consumption aggregate at household level (ca)\nca  &lt;- read_dta(\"data/ARM-HH-survey/CONSAGG2022.dta\")\n# Processed income at household level (ic)\nic  &lt;- read_dta(\"data/ARM-HH-survey/totinc.dta\") \n\nWe will work non-destructively, meaning we will not rewrite these data sets and we will only create intermediate data frame objects from them to perform transformations, selections and other data management tasks. For example, we will keep household assignment to poverty status and consumption deciles handy by creating a subset of our ca data with only our household identifiers, deciles, and poverty.\n\n# From the WB processed dataset, we extract deciles and poverty\ndeciles &lt;- ca |&gt; \n  select( hhid, decile, poor_Avpovln2022, \n          poor_Foodpovln2022, poor_Lpovln2022, poor_Upovln2022)\n\nWe also have geographical information for level 1 in Shapefile format, which we import with the sf package. We rename the column with the name of the administrative region to match our household survey data set conventions to ease mergers. The dplyr package from the tidyverse meta package allows us to “pipe” or link processing steps using the |&gt; pipe, which can be inserted using Ctrl + m. Although there is no geoprocessing in this analysis, this will come in handy for graphical presentations. Let’s have a look at it.\n\n# Geodata\n# Armenia marzes or administrative level 1 shapefile\nadm1 &lt;- read_sf(\"data/ARM-Geodata/ARM-ADM1.shp\") |&gt; \n  select(NAM_1, COD_HH_SVY, geometry) |&gt; \n    # Make sure that names match the rest of datasets\n  mutate(NAM_1 = if_else(NAM_1 == \"Gergharkunik\", \"Gegharkunik\", NAM_1))\nnames(adm1)[2] &lt;- \"hh_02\"\n\ntm_shape(adm1)+\n  tm_polygons(\"NAM_1\", legend.show = FALSE) +\n  tm_text(\"NAM_1\", size = 3/4)\n\n\n\n\n\n\n\n\nMarzes names are more accurate in the shapefile than in the survey. We will use them from here on instead of the survey factor labels.\n\nhh &lt;- hh |&gt; \n  left_join(adm1, join_by(hh_02 == hh_02)) |&gt; \n  select(-geometry)\n\nic &lt;- ic |&gt; \n  left_join(adm1, join_by(hh_02 == hh_02)) |&gt; \n  select(-geometry)\n\nFinally, but not least important, we have our vulnerability information.\n\nbuildings_aal &lt;- \n  read_xlsx(\"data/ARM-Vulnerability-Analysis/Data_AAL_AAE.xlsx\",\n            sheet = \"Building_AAL\") |&gt; \n    # Make sure that names match the rest of datasets\n  mutate(NAM_1 = if_else(NAM_1 == \"Gergharkunik\", \"Gegharkunik\", NAM_1))\nbuildings_1in100 &lt;-\n  read_xlsx(\"data/ARM-Vulnerability-Analysis/Data_AAL_AAE.xlsx\",\n            sheet = \"Building_1in100\")\ncrops_productivity &lt;- \n  read.csv(\"data/ARM-Vulnerability-Analysis/ARM_crops_combined_REF_shock_admin1.csv\") |&gt; \n  rename(NAM_1 = Province)\ncrops_aal &lt;- \n  read_xlsx(\"data/ARM-Vulnerability-Analysis/Data_AAL_AAE.xlsx\",\n            sheet = \"Agriculture_AAL\")\ncrops_1in100 &lt;-\n  read_xlsx(\"data/ARM-Vulnerability-Analysis/Data_AAL_AAE.xlsx\",\n            sheet = \"Agriculture_1in100\")",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html#asset-value-of-income-flows",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html#asset-value-of-income-flows",
    "title": "Vulnerability Analysis Calculations",
    "section": "4 Asset value of income flows",
    "text": "4 Asset value of income flows\n\n4.1 Imputed rent\n“Housing, measured as the welfare value of the flow of services households derive from their dwelling, is one of the most relevant components of households’ welfare aggregate, which is used as a basis for distributional analysis” (Deaton & Zaidi, 2002; cited by Ceriani, Olivieri, & Ranzani, 2019). In Armenia, most households own their home, so the emergent rental market information is used to impute rent to non-renters using a log linear modeling approach described by (Ceriani et al., 2019), in which imputed rent is predicted using a combination of household characteristics (urban/rural, Marz, number of rooms, presence of an indoor toilet, number of household, square meters, type of dwelling, household members) and head of household characteristics (i.e. sex, highest completed schooling level, age group). The first step is to identify these characteristics for the regression.\nWe first extract relevant characteristics of the heads of household and create a heads subset of our person’s database, which we call heads. It has our household id (interview__key), sex (mem_02), age (mem_05), and education level.\n\nheads &lt;- pp |&gt; \n  filter(mem_03 == 1) |&gt; \n  select( interview__key ,mem_02, mem_05,ed_03)\n\nSince we only have one head of household per household, we can join this data with our household information. We now create a subset of our household data, which we call imputed_rent with the relevant dwelling and head of household variables according to the model suggested by Ceriani et al. (2019).\n\nimputed_rent &lt;- hh |&gt; \n  left_join( heads , join_by(interview__key == interview__key)) |&gt; \n  select( interview__key, hh_02, hh_03, hous_02, hous_10, hous_04,mem_02, \n          mem_05, ed_03, mem_num, hous_41, hous_19,hous_09, weight)\n\nTo save on the creation of unnecessary dummy variables for our regression, we take advantage of the factors present in the original SPSS files, which carry over when importing into R and are used by it to create them automatically at prediction time. Pay attention to the creation of age groups using cut() .\n\n# Convert categorical variables to factors and create dummy variables\nimputed_rent &lt;- imputed_rent  |&gt; \n  mutate(hh_02 = as.factor(hh_02),          # Marz\n         hh_03 = as.factor(hh_03),          # Urban / Rural\n         hous_02 = as.factor(hous_02),      # Ownership or rental\n         mem_02 = as.factor(mem_02),        # Sex\n         ed_03 = as.factor(ed_03),          # Education level\n         hous_41 = as.factor(hous_41),      # Type of toilet\n         hous_19 = as.factor(hous_19))  |&gt;  # Source of electricity\n  mutate(age_group = cut(mem_05, breaks = c(0, 24, 34, 44, \n                                            54, 64, Inf), \n                         labels = c(\"15-24\", \"25-34\", \"35-44\",\n                                    \"45-54\", \"55-64\", \"65+\"),\n                         right = TRUE)) |&gt;\n  mutate(age_group = as.factor(age_group)) |&gt;\n  mutate(bathroom_dummy = ifelse(hous_41 == 1, 1, 0)) |&gt; \n  mutate(bathroom_dummy = as.factor(bathroom_dummy)) |&gt; \n  select(-mem_05, -hous_41)  # Remove the original age variable\n\nFor our model, we need to concentrate on tenants who pay rent. So we subset further creating a data set called renters_df. Variable hous_02 asks whether the household owns this dwelling or it is rented (with possible values 1. own, 2. rent, 3. other). And for renters, we want those whose value is larger than zero.\n\nrenters_df &lt;- imputed_rent |&gt; \n  filter(hous_02 == 2) |&gt; \n  filter(!is.na(hous_04)) |&gt; \n  filter(hous_04 &gt;0)\n\nWe are now ready to build our model:\n\nlog_linear_model &lt;- lm(log(hous_04) ~       # Rent, which depends on:\n                         hh_02 +            # Marz\n                         hh_03 +            # Urban / Rural\n                         hous_10 +          # Number of rooms\n                         mem_02 +           # Sex of head of HH\n                         ed_03 +            # Education level\n                         mem_num +          # Number of HH members\n                         bathroom_dummy +   # Flushing toilet dummy\n                         hous_09 +          # Total square meters\n                         age_group,         # Age brackets\n                       data = renters_df)\n\nFor space considerations, we omit the output of the model, but you can inspect the results of the model with summary(log_linear_model) . This particular application for Armenia results in small positive significance for total square meters and having a flushing toilet, small negative significance for being female and high negative significance for the Marzes in relation to Yerevan, as well as high negative significance for rural areas (Multiple R-squared: 0.4883). In other words, rent for Armenians will be higher if they live in urban areas, have a working toilet, have a larger imputed_rent and the head of household is male. With our coefficients we can now impute rent for our non renters.\nBefore we move on, we need to de-factor some variables and re-code them so that our predictions run smoothly. We did not get predictions for education level 0 in our renters database, but there are some in our non_renters_df data set. Since they are factors (ie. categorical values) and not years of education, when R is running the regression, it creates dummy variables in the background for each level that it encounters in the data. Since that level was missing in the renters data, the prediction does not include it. So when it encounters that value in the non-renters data frame, R does not know how to handle it. This might not happen in your data set, but beware that if it does, this is the reason why your model won’t predict. The error that gave this away read.\nError in model.frame.default(Terms, newdata, \nna.action = na.action, xlev = object$xlevels): \nfactor ed_03 has new levels 0 \nSo let’s take care of non-trained values by making the decision to change the 12 cases that responded “none” to “primary”. Another option would be to change it to “other”, but since the prediction there was made with 1 observation we felt it was less of a disturbance this way. We find the values to change by indexing in square brackets; a powerful way of base R to slice data sets in multiple ways.\n\n# Take care of non training values in  the original data set\n# Convert to numeric to perform the operation\nimputed_rent$ed_03 &lt;- as.numeric(as.character(imputed_rent$ed_03)) \nimputed_rent$ed_03[imputed_rent$ed_03 == 0] &lt;- 1 # Re-code 0 to 1\nimputed_rent$ed_03 &lt;- as.factor(imputed_rent$ed_03) # Convert back to factor\n\nWith everything in place, we can now predict the imputed rent. Actually, the log of predicted rent, so we transform the log value to value in the next pipe. We can do two things. One is to create a non-renters data set, predict rent there and then join with the renters data frame. Another is just to apply the prediction to the entire imputed_rent data set and then just replace the result with missing values for the renters. We will do the latter.\n\nimputed_rent &lt;- imputed_rent |&gt; \n  add_predictions(log_linear_model, \n                  var = \"log_rent_predicted\") |&gt; \n  mutate(imputed_rent = exp(log_rent_predicted)) |&gt; \n  # Replace renters imputed value with \"missing\"\n  mutate(imputed_rent = if_else(hous_02 %in% c(\"2\", \"3\"),\n                                NA, imputed_rent)) |&gt;\n  # We just keep the household id and the imputed value going forward\n  select( interview__key, imputed_rent)\n\n# Remove intermediate products\nrm(heads, log_linear_model, renters_df)\n\nAt this point we can save the prediction if we wish to do so to disk, but it is not necessary for our purposes here as we can continue using the created object imputed_rent in our calculations going forward. For example to output to Excel, Stata, SPSS, and CSV we would write (make sure your outputs directory exists):\n\n# Stata\nwrite_dta(imputed_rent, \"outputs/imputed_rent.dta\", version = 10)\n# Excel\nwrite.xlsx(imputed_rent,\"ouptuts/imputed_rent.xlsx\",\n           sheetName = \"imputed_rent\",\n           rowNames = FALSE,\n           colnames = FALSE,\n           overwrite = TRUE,\n           asTable = FALSE\n)\n# SPSS\nwrite_sav(data, \"outputs/imputed_rent.sav\")  \n# Comma Separated Values\nwrite.csv(imputed_rent,\"outputs/imputed_rent.csv\" sep = \",\")\n\nLet’s explore the results, by first summarizing the data.\n\n# Average imputed rent by marz\nimputed_rent_marz &lt;- hh |&gt; \n  left_join( deciles, join_by( interview__key == hhid)) |&gt;\n  left_join( imputed_rent, join_by( interview__key == interview__key)) |&gt;\n  select(decile, hh_02, hh_03, hous_10, imputed_rent,weight, NAM_1) |&gt; \n  group_by(NAM_1) |&gt; \n  summarize(avg_dwelling_m2 = \n              weighted.mean(hous_10, as.integer(weight), na.rm = TRUE),\n            avg_imputed_rent = \n              weighted.mean(imputed_rent, as.integer(weight), \n                            na.rm = TRUE),\n            avg_imputed_rent_us = \n              weighted.mean(imputed_rent, as.integer(weight), \n                            na.rm = TRUE)*er)\n\nAnd then making a table.\n\nimputed_rent_marz |&gt; \n  gt() |&gt; \n   tab_header(\n    title = \"Imputed rent in Armenia\",\n    subtitle = \"Average dwelling area and imputed rent (Year 2022)\"\n  ) |&gt; \n  # grand_summary_rows(\n  #   columns = c(avg_dwelling_m2,avg_imputed_rent),\n  #   fns= list(\n  #     Average = ~mean(., na.rm = TRUE)\n  #     ),\n  #   fmt = list(~ fmt_number(., decimals = 1))\n  # ) |&gt; \n  fmt_number(\n    columns = c(avg_dwelling_m2, avg_imputed_rent, avg_imputed_rent_us),\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    NAM_1 = \"Marz\",\n    avg_dwelling_m2 = \"Average dwelling area ({{m^2}})\",\n    avg_imputed_rent = \"Average imputed monthly rent (AMD)\",\n    avg_imputed_rent_us = \"Average imputed monthly rent (USD)\"\n  ) |&gt; \n    tab_source_note(\n    source_note = md(\"Own elaboration based on Armenia Integrated Living Conditions Survey (ARMSTAT, 2023).\")\n  )\n\n\n\n\n\n\n\nImputed rent in Armenia\n\n\nAverage dwelling area and imputed rent (Year 2022)\n\n\nMarz\nAverage dwelling area (m2)\nAverage imputed monthly rent (AMD)\nAverage imputed monthly rent (USD)\n\n\n\n\nAragatsotn\n103.8\n37,769.4\n87.2\n\n\nArarat\n102.6\n40,390.4\n93.3\n\n\nArmavir\n109.3\n40,237.3\n92.9\n\n\nGegharkunik\n115.4\n29,480.5\n68.1\n\n\nKotayk\n96.2\n43,472.8\n100.4\n\n\nLori\n87.9\n22,414.2\n51.8\n\n\nShirak\n82.7\n28,517.6\n65.9\n\n\nSyunik\n95.1\n38,606.5\n89.2\n\n\nTavush\n97.5\n23,128.6\n53.4\n\n\nVayots Dzor\n104.7\n28,811.5\n66.6\n\n\nYerevan\n70.9\n93,590.9\n216.2\n\n\n\nOwn elaboration based on Armenia Integrated Living Conditions Survey (ARMSTAT, 2023).\n\n\n\n\n\n\n\n\n\n\n4.2 Net present value of imputed rent\nThe previous steps help us determine the imputed monthly rent for home owners. We can treat this income as an asset by considering the net present value of future rents. We use the traditional formula:\n\\[\n\\text{NPV} = \\sum_{t=0}^{N} \\frac{C_t}{(1 + r)^t}\n\\tag{1}\\]\nWhere:\n\n\\(NPV\\) = Net Present Value\n\\(C_t\\) = Net cash inflow during the period \\(t\\)\n\\(r\\) = Discount rate\n\\(t\\) = Time period\n\\(N\\) = Total number of periods\n\nWe will use an annual discount rate of 6%, which is customary for homes, an inflation of 5% for 27 years, since the survey was conducted in the last few months of 2022 and we are making the calculation from January 01 2023 to December 31, 2050.\n\n# Parameters\ndiscount_rate &lt;- 0.06 # Annual discount rate, for example, 5%\ninflation_rate &lt;- 0.05 # Annual inflation rate, for example, 5%\nyears &lt;- 28 # Number of years to discount\n\n# Adjust rates for monthly compounding, to avoid overestimation\nmonthly_discount_rate &lt;- (1 + discount_rate)^(1/12) - 1\nmonthly_inflation_rate &lt;- (1 + inflation_rate)^(1/12) - 1\n\n# Annual imputed rent\nimputed_rent$annual_imputed_rent &lt;- imputed_rent$imputed_rent * 12 \n\nWe can do two things, either annualize the monthly income or divide our rates by 12 and have the periods in the formula be months. It depends on the kind of shocks that we want to do. For example, if we know that a 1 in a 100 year event will have an impact that will last, let’s say one and a half year, then having months is useful as we can introduce the shock as a tax that has an effect on 18 months worth of net present value. However, if we know that our shocks will have annual consequences, then doing our calculations year by year is enough.\nTo calculate each months worth of discounted present value, we use sapply() to perform the calculation over \\(years * 12\\) months. This is similar to using for loops in other languages, but it is much more efficient, because it works hard to summarize results as vectors, and avoids iterations. In this case, since we are operating the formula over the entire vector of imputed rents, month, the result return(present_value) is not a vector but a matrix called present_value that gets attached to our data set at once (not column by column) where each column represents a month’s worth of discounted present value for each household in the rows. Notice that we are using our modified monthly rates, which are adjusted (not just the annual divided by 12) to more accurately reflect the compounding value of money.\n\n# Monthly periods\nimputed_rent$present_value_rent &lt;- sapply(1:(years * 12), function(n) {\n  future_rent &lt;- imputed_rent$imputed_rent * (1 + monthly_inflation_rate)^n\n  present_value &lt;- future_rent / ((1 + monthly_discount_rate)^n)\n  return(present_value)\n})\n\nThat results in the creation of a matrix containing the monthly discounted values by month. We can see a snippet of this attached matrix, filtering for the observations that are not missing values and showing only the first four valid households and months using:\n\nimputed_rent$present_value_rent[\n  !is.na(imputed_rent$present_value_rent[, 1]), ][1:4, 1:4]\n\n      [,1]     [,2]     [,3]     [,4]\n1 20118.27 20102.39 20086.51 20070.65\n2 20619.64 20603.36 20587.09 20570.84\n3 20475.11 20458.94 20442.78 20426.64\n4 25308.52 25288.54 25268.57 25248.62\n\n\nAfter that we sum over the columns corresponding to our monthly discounted values:\n\n# Sum up the present values for the total present value over the period\nimputed_rent$net_present_value_rent &lt;- rowSums(imputed_rent$present_value_rent)\n\nFor comparison, we can do it annually as well. Here we use our annual rates (discount, and inflation) to generate a similar matrix, where all columns refer to years.\n\n# Annual periods\nimputed_rent$present_value_rent2 &lt;- sapply(1:(years), function(n) {\n  future_rent2 &lt;- imputed_rent$annual_imputed_rent * (1 + inflation_rate)^(n)\n  present_value2 &lt;- future_rent2 / ((1 + discount_rate)^n)\n  return(present_value2)\n})\n\n\n# Sum up the present values for the total present value over the period\nimputed_rent$net_present_value_rent2 &lt;- rowSums(imputed_rent$present_value_rent2)\n\n\n# Delete partial calculations\nimputed_rent &lt;- imputed_rent |&gt; \n  select(interview__key, imputed_rent, net_present_value_rent, net_present_value_rent2)\n\nFor a comparison between the two calculations of net present value, we can compare their means and their difference should be zero:\n\nhh |&gt; \n  left_join( deciles, join_by( interview__key == hhid)) |&gt;\n  left_join( imputed_rent, join_by( interview__key == interview__key)) |&gt;\n  select(decile, hh_02, hh_03, net_present_value_rent, net_present_value_rent2, NAM_1, weight) |&gt; \n  group_by(NAM_1) |&gt; \n  summarize(avg_net_present_value1 = weighted.mean(net_present_value_rent, as.integer(weight), na.rm = TRUE)*er,\n            avg_net_present_value2 = weighted.mean(net_present_value_rent2, as.integer(weight), na.rm = TRUE)*er,\n            difference = \n              round(avg_net_present_value1 - avg_net_present_value2)) |&gt;\n  gt() |&gt; \n  fmt_number(\n    columns = everything(),\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    NAM_1 = \"Marz\",\n    avg_net_present_value1 = \"Average net present value in USD (calculated monthly)\",\n    avg_net_present_value2 = \"Average net present value in USD (calculated annual)\",\n    difference = \"Difference\"\n  )\n\n\n\n\n\n\n\nMarz\nAverage net present value in USD (calculated monthly)\nAverage net present value in USD (calculated annual)\nDifference\n\n\n\n\nAragatsotn\n25,737.3\n25,625.6\n112.0\n\n\nArarat\n27,523.3\n27,403.9\n119.0\n\n\nArmavir\n27,418.9\n27,300.0\n119.0\n\n\nGegharkunik\n20,088.9\n20,001.8\n87.0\n\n\nKotayk\n29,623.7\n29,495.1\n129.0\n\n\nLori\n15,273.7\n15,207.5\n66.0\n\n\nShirak\n19,432.8\n19,348.5\n84.0\n\n\nSyunik\n26,307.7\n26,193.5\n114.0\n\n\nTavush\n15,760.6\n15,692.2\n68.0\n\n\nVayots Dzor\n19,633.0\n19,547.9\n85.0\n\n\nYerevan\n63,775.7\n63,499.0\n277.0\n\n\n\n\n\n\n\nUsing our Shapefile, we can explore how the average net present value of imputed rent distributes geographically (labels in million dram).\n\nnpv &lt;- hh |&gt; \n  left_join( imputed_rent, join_by( interview__key == interview__key)) |&gt;\n  select(hh_02, hh_03, net_present_value_rent, \n         net_present_value_rent2, NAM_1, weight) |&gt; \n  group_by(NAM_1) |&gt; \n  summarize(\n    avg_npv = weighted.mean(\n      net_present_value_rent, as.integer(weight), na.rm = TRUE\n      ),\n    avg_npv_labels =\n      formatC(\n        weighted.mean(net_present_value_rent, \n                      as.integer(weight), na.rm = TRUE)*er, \n        big.mark = \",\", format = \"f\", digits = 1)\n      )\n\nnpv_map &lt;- adm1 |&gt; \n  left_join(npv, join_by(NAM_1 == NAM_1))\n\nnpv_map &lt;-tm_shape(npv_map)+\n  tm_polygons(\"avg_npv\", legend.show = FALSE) +\n  tm_text(\"avg_npv_labels\", size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"Average Imputed Rent NPV (USD)\", \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\nnpv_map\n\n\n\n\n\n\n\nFigure 1: Average Net Present Value of Imputed Rent by Marz (Year 2022, USD)\n\n\n\n\n\nIn the next section we will perform the same calculations to treat agricultural income as a future discounted asset so that we can implement shocks, according to vulnerability data.\n\n\n4.3 Net Present Value of Agricultural Income\nFor the Armenian case, we have access to an already processed data set with income aggregations from hired employment, self-employment, income on property, public pensions, transfers, other income, and, especially important for this section, agricultural income. Let’s calculate the net present value for that income in the same way we did our imputed rent. We will focus on the annual version of our calculations. There is one caveat, which is related to the fact that agricultural income shows seasonal variations. So multiplying the monthly income by 12 is likely to overestimate the net present value so we use a scaling factor which should reflect that seasonality. We will use the same rates as before so we will not create new ones.\n\nag_scaling &lt;- 0.65\nag_income &lt;- ic |&gt; \n  mutate(lvstk_onlyinc = if_else(lvstk_onlyinc&lt;0, 0, lvstk_onlyinc)) |&gt; \n  mutate(annual_ag_income = ((inc4) * 12 * ag_scaling))\n\nAnd we perform the same Net Present Value calculations as before (see Equation 1).\n\n# Annual periods\nag_income$present_value_ag_income &lt;- sapply(1:(years), function(n) {\n  future_ag_income &lt;- ag_income$annual_ag_income * (1 + inflation_rate)^(n)\n  present_ag_value &lt;- future_ag_income / ((1 + discount_rate)^n)\n  return(present_ag_value)\n})\n\n\n# Sum up the present values for the total present value over the period\nag_income$net_present_ag_value &lt;- rowSums(ag_income$present_value_ag_income)\n\n\n# Delete partial calculations\nag_income &lt;-ag_income |&gt; \n  select(interview__key, present_value_ag_income, annual_ag_income, net_present_ag_value)\n\nWe will create a map object to show a side-by-side comparison of both NPVs.\n\nnpv_ag &lt;- hh |&gt; \n  left_join( ag_income, join_by( interview__key == interview__key)) |&gt; \n  select(hh_02, hh_03, annual_ag_income, net_present_ag_value, NAM_1, weight) |&gt;\n  group_by(NAM_1) |&gt; \n  summarize( \n    avg_ag_npv = weighted.mean( \n      net_present_ag_value, as.integer(weight), na.rm = TRUE ),\n    avg_ag_npv_labels =  \n        formatC( \n          weighted.mean(\n            net_present_ag_value *er, \n            as.integer(weight), na.rm = TRUE), \n          big.mark = \",\", \n          format = \"f\", digits = 1) )\n\nnpv_ag_map &lt;- adm1 |&gt; \n  left_join(npv_ag, join_by(NAM_1 == NAM_1))\n\nnpv_ag_map &lt;- tm_shape(npv_ag_map)+ \n  tm_polygons(\"avg_ag_npv\", legend.show = FALSE) + \n  tm_text(\"avg_ag_npv_labels\", size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"Average Ag. Income NPV (USD)\", \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\nAnd now we can compare the spatial distributions of both Net Present Values from the map objects npv_map and npv_ag_map that we created before.\n\ntmap_arrange(npv_map, npv_ag_map)\n\n\n\n\n\n\n\nFigure 2: Average Net Present Value of Imputed Rent and Agricultural Income (Year 2022, million AMD)\n\n\n\n\n\nWith both Net Present Values calculated, in the following section we will apply our vulnerability shocks to a selection of Armenian households.",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html#vulnerability-shocks",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html#vulnerability-shocks",
    "title": "Vulnerability Analysis Calculations",
    "section": "5 Vulnerability shocks",
    "text": "5 Vulnerability shocks\nThis section was replaced by new selection method. See Section 6 below.\n\n5.1 Buildings\nWe previously estimated the imputed rent values for households that own their homes, assuming that they derive welfare from owning those assets. We then treated that discounted future income flow as an asset value. Our data suggests that some of those buildings are damaged due to increased rain and flood events. In each administrative region a percentage of buildings receive these shocks, effectively taxing their monthly imputed value by a percentage. Let’s find a way to randomly select from our data set a number of weighted households that matches the shocks. Let’s move step by step.\nWe first merge the household data set with the imputed_rent data set to have the descriptive variables per household.\n\n# Create a placeholder for our chosen HH's\nrent_dataset &lt;- hh |&gt; \n  select(hous_45__7, interview__key, NAM_1, weight, hh_02)  |&gt;  \n  mutate(selected_for_tax = FALSE, # initialize with FALSE\n         is_dilapidated = if_else(hous_45__7 == 1, TRUE, FALSE)\n         ) |&gt; \n  mutate(is_dilapidated = if_else(is.na(hous_45__7), FALSE, is_dilapidated)) |&gt; \n  left_join(imputed_rent, join_by(interview__key == interview__key )) |&gt; \n  rename(household_id = interview__key)\n\nexposure_dataset &lt;- buildings_aal\n\nWe merge with the exposure data set and prepare the necessary columns.\n\nrent_dataset &lt;- rent_dataset |&gt;\n  left_join(exposure_dataset, by = \"NAM_1\") |&gt;\n  mutate(is_dilapidated = if_else(is.na(hous_45__7) | hous_45__7 == 0, FALSE, TRUE))\n\nThe next step involves calculating the target weight for each marz (NAM_1) and randomly selecting households based on their weight until the cumulative sum matches the target.\n\nset.seed(123)  # Ensure reproducibility\n\nrent_dataset &lt;- rent_dataset |&gt;\n  group_by(NAM_1) |&gt;\n  mutate(total_weight = sum(weight, na.rm = TRUE),\n         target_weight = total_weight * pct_AA_exposed_buildings / 100) |&gt;\n  ungroup() |&gt;\n  arrange(NAM_1, runif(n())) |&gt;\n  group_by(NAM_1) |&gt;\n  mutate(cum_weight = cumsum(weight),\n         selected_for_tax = cum_weight &lt;= target_weight) |&gt;\n  ungroup()\n\nWe can explore how many observations were targeted and their weighted values.\n\nrent_dataset |&gt; \n  filter(selected_for_tax == TRUE) |&gt; \n  group_by(hh_02,NAM_1)  |&gt; \n  summarise(Selected_Cases = sum(selected_for_tax, na.rm = TRUE),\n            Weighted_no_HHs = as.integer(sum(weight, na.rm = TRUE))) |&gt; \n  arrange(hh_02) |&gt; \n  adorn_totals(\"row\") |&gt; \n  ungroup() |&gt; \n  gt() |&gt; \n  fmt_number(\n    columns = everything(),\n    decimals = 0\n  ) |&gt; \n  cols_label(\n    hh_02 = \"\",\n    NAM_1 = \"Marz\",\n    Selected_Cases = \"No. of targeted observations\",\n    Weighted_no_HHs = \"No. of targeted weighted households\"\n  )\n\n\n\n\n\n\n\n\nMarz\nNo. of targeted observations\nNo. of targeted weighted households\n\n\n\n\n1\nYerevan\n31\n5,973\n\n\n2\nAragatsotn\n10\n1,135\n\n\n3\nArarat\n19\n3,434\n\n\n4\nArmavir\n14\n1,866\n\n\n5\nGegharkunik\n13\n2,217\n\n\n6\nLori\n15\n2,286\n\n\n7\nKotayk\n9\n2,155\n\n\n8\nShirak\n13\n2,060\n\n\n9\nSyunik\n13\n1,526\n\n\n10\nVayots Dzor\n14\n623\n\n\n11\nTavush\n6\n752\n\n\nTotal\n-\n157\n24,027\n\n\n\n\n\n\n\nNow, we apply the “vulnerability tax” to the imputed_rent according to the building’s state of dilapidation and the specific tax rates for normal and dilapidated conditions.\n\nrent_dataset &lt;- rent_dataset |&gt;\n  mutate(adjusted_rent = case_when(\n    selected_for_tax & is_dilapidated ~ imputed_rent * (1 - perc_AAL_dilapidated / 100),\n    selected_for_tax & !is_dilapidated ~ imputed_rent * (1 - perc_AAL_normal / 100),\n    TRUE ~ imputed_rent\n  ))\n\nWe then compare our values in a table. In this case, we see that, although the percentages of impacted households are small per marz, the mean imputed rent decrease, which is a value that is expected to compound over time.\nAnd we can now estimate a new net present value with the adjusted values in the same manner as before.\n\n# Annual adjusted imputed rent\nrent_dataset$annual_adjusted_rent &lt;- rent_dataset$adjusted_rent * 12 \n\n\nrent_dataset$adjusted_present_value_rent &lt;- sapply(1:(years), function(n) {\n  future_adjusted_rent &lt;- rent_dataset$annual_adjusted_rent * (1 + inflation_rate)^(n)\n  present_adjusted_value &lt;- future_adjusted_rent / ((1 + discount_rate)^n)\n  return(present_adjusted_value)\n})\n\n\n# Sum up the present values for the total present value over the period\nrent_dataset$adjusted_net_present_value_rent &lt;-\n  rowSums(rent_dataset$adjusted_present_value_rent)\n\nAnd we can view the compounded differences in NPVs for those affected by Marz and Poverty condition. Note that the weighted sum of households (24,069-24,075) in the tables differs slightly, because of rounding during the sliced calculations to avoid showing fractions of individual homes.\n\nbuilding_losses &lt;- rent_dataset |&gt; \n  left_join(deciles, join_by(household_id == hhid)) |&gt; \n  rename(poor = poor_Avpovln2022) |&gt; \n  filter(selected_for_tax == TRUE) |&gt; \n  group_by(hh_02,NAM_1) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_value_rent, weight, na.rm = TRUE)*er,\n    Average_Adjusted_NPV = weighted.mean(\n      adjusted_net_present_value_rent, weight, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight, na.rm = TRUE))\n  ) |&gt;\n   ungroup()\n\nbuilding_losses |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    hh_02 = \"\",\n    NAM_1 = \"Marz\",\n    Average_NPV = \"Average NPV of imputed rent (USD)\",\n    Average_Adjusted_NPV = \"Adjusted average NPV of imputed rent (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\n\nMarz\nAverage NPV of imputed rent (USD)\nAdjusted average NPV of imputed rent (USD)\nLoss (USD)\nNo. HH\n\n\n\n\n1\nYerevan\n69,393.2\n63,836.2\n5,556.9\n5974\n\n\n2\nAragatsotn\n25,113.2\n23,068.3\n2,044.9\n1135\n\n\n3\nArarat\n21,198.6\n19,480.6\n1,718.0\n3434\n\n\n4\nArmavir\n32,639.3\n29,604.2\n3,035.1\n1867\n\n\n5\nGegharkunik\n21,037.9\n19,259.3\n1,778.6\n2218\n\n\n6\nLori\n15,147.7\n11,898.6\n3,249.1\n2287\n\n\n7\nKotayk\n23,241.7\n20,949.7\n2,292.1\n2156\n\n\n8\nShirak\n22,164.2\n20,083.8\n2,080.4\n2060\n\n\n9\nSyunik\n26,799.7\n21,865.6\n4,934.1\n1526\n\n\n10\nVayots Dzor\n20,955.0\n17,532.2\n3,422.9\n624\n\n\n11\nTavush\n21,176.9\n16,922.1\n4,254.9\n752\n\n\n\n\n\n\n\n\nrent_dataset |&gt; \n  left_join(deciles, join_by(household_id == hhid)) |&gt; \n  #rename(poor = poor_Avpovln2022) |&gt; \n  filter(selected_for_tax == TRUE) |&gt; \n  mutate(poor = if_else(poor_Avpovln2022==1, \n                        \"Below poverty line\", \n                        \"Above poverty line\")) |&gt; \n  group_by( poor) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_value_rent, weight, na.rm = TRUE) * er,\n    Average_Adjusted_NPV = weighted.mean(\n      adjusted_net_present_value_rent, weight, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight, na.rm = TRUE))\n  ) |&gt;\n   ungroup() |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    poor = \"Poverty\",\n    Average_NPV = \"Average NPV of imputed rent (USD)\",\n    Average_Adjusted_NPV = \"Adjusted average NPV of imputed rent (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\nPoverty\nAverage NPV of imputed rent (USD)\nAdjusted average NPV of imputed rent (USD)\nLoss (USD)\nNo. HH\n\n\n\n\nAbove poverty line\n30,949.6\n27,781.6\n3,167.9\n19095\n\n\nBelow poverty line\n41,032.3\n37,253.2\n3,779.1\n4939\n\n\n\n\n\n\n\n\n\n5.2 Agriculture\nIn the case of agriculture, we have percentage losses in agricultural productivity per year and per climate scenario from our crops_productivity data set.\n\ncrops_productivity |&gt; \n     select(-GID_1) |&gt;\n  group_by(climate_scenario) |&gt; \n  summarize(\"Mean percent change all years\" = mean(pct_change_prod)) |&gt; \n  gt()\n\n\n\n\n\n\n\nclimate_scenario\nMean percent change all years\n\n\n\n\nDry/Hot mean\n-0.19475663\n\n\nSSP1-1.9 mean\n-0.04033261\n\n\nSSP2-4.5 CAMS-CSM1-0\n-0.06785020\n\n\nSSP2-4.5 CMCC-CM2-SR5\n-0.03269785\n\n\nSSP2-4.5 CNRM-CM6-1\n-0.08440299\n\n\nSSP3-7.0 BCC-CSM2-MR\n-0.15777422\n\n\nSSP3-7.0 KACE-1-0-g\n-0.20606020\n\n\nSSP3-7.0 UKESM1-0-LL\n-0.21341230\n\n\nSSP3-7.0 mean\n-0.08120867\n\n\nWet/Warm mean\n-0.06320857\n\n\n\n\n\n\n\nSo we will apply this to our agricultural net present value calculations per year. Remember that when we calculated the present values of each year we were left with a matrix identifying each year’s value per household id. So we are going to pivot the data set into long format, so that we can match the appropriate loss in productivity according to year and marz. To test out our methodology we will keep one scenario only in our ag_exposure data set. Also, we will change the percent change to decimal so that we are not over-estimating productivity losses. We also leave out 2021-2022 because our NPV calculations started from 2023 to 2050.\n\nag_exposure &lt;- crops_productivity |&gt; \n  filter(climate_scenario==\"Dry/Hot mean\" & year &gt; 2022 ) |&gt; \n     select(-GID_1, -climate_scenario) #|&gt; \n  #mutate(pct_change_prod = pct_change_prod / 100)\n\nOur ag_income data set from before has the present values of each individual household in columns that represent each year. We can rename those columns with our year variables so that we can match them with our exposure data set.\n\n# Define the years range\nyear_names &lt;- as.character(2023:2050)\n\n# Rename the columns of the matrix\ncolnames(ag_income$present_value_ag_income) &lt;- year_names\n\nWe extract our present value calculations from the matrix column and convert it into a data set on its own, which we can manipulate into long format to merge with our exposure data set.\n\n# Convert the matrix to a data.frame.\nag_income_long &lt;- as.data.frame(ag_income$present_value_ag_income)  |&gt;\n  mutate(household_id = ag_income$interview__key) |&gt;\n  pivot_longer(cols = -household_id, names_to = \"year\", \n               values_to = \"present_value_ag_income\") |&gt; \n  left_join(hh, join_by(household_id == interview__key)) |&gt; \n  select(household_id, present_value_ag_income, year, hh_02, NAM_1)\n\nag_income_long$year &lt;- as.integer(ag_income_long$year)\n\n# inspect the final result\nhead(ag_income_long[!is.na(\n  ag_income_long$present_value_ag_income),])\n\n# A tibble: 6 × 5\n  household_id present_value_ag_income  year hh_02            NAM_1      \n  &lt;chr&gt;                          &lt;dbl&gt; &lt;int&gt; &lt;dbl+lbl&gt;        &lt;chr&gt;      \n1 00-03-45-50                  193160.  2023 10 [VAYOTS DZOR] Vayots Dzor\n2 00-03-45-50                  191338.  2024 10 [VAYOTS DZOR] Vayots Dzor\n3 00-03-45-50                  189533.  2025 10 [VAYOTS DZOR] Vayots Dzor\n4 00-03-45-50                  187745.  2026 10 [VAYOTS DZOR] Vayots Dzor\n5 00-03-45-50                  185974.  2027 10 [VAYOTS DZOR] Vayots Dzor\n6 00-03-45-50                  184219.  2028 10 [VAYOTS DZOR] Vayots Dzor\n\n\nSo now we can match our exposure values according to year and marz and modify our annual present values.\n\nag_income_long &lt;- ag_income_long |&gt; \n  left_join(ag_exposure, join_by(NAM_1, year)) |&gt; \n  mutate(adjusted_present_value_ag = present_value_ag_income * (1 + pct_change_prod))\n\nAnd we can collapse our dataset again to sum over the years for each household and join it back with our ag_income dataset.\n\nag_income_long &lt;- ag_income_long |&gt; \n  select(household_id, adjusted_present_value_ag) |&gt; \n  group_by(household_id) |&gt; \n  summarize( net_present_ag_value_adjusted =\n               sum(adjusted_present_value_ag, na.rm = TRUE))\n\nag_income &lt;- ag_income |&gt;\n  left_join(ag_income_long, join_by(interview__key == household_id)) |&gt; \n  select(interview__key, annual_ag_income, net_present_ag_value, net_present_ag_value_adjusted)\n\nLet’s compare the two mean values.\n\nag_losses &lt;- hh |&gt; \n  left_join(deciles, join_by(interview__key == hhid)) |&gt; \n  left_join(ag_income, join_by(interview__key==interview__key)) |&gt;\n  filter(!is.na(net_present_ag_value)) |&gt; \n  rename(poor = poor_Avpovln2022) |&gt; \n  group_by(hh_02,NAM_1) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_ag_value, weight, na.rm = TRUE)*er,\n    Average_Adjusted_NPV = weighted.mean(\n      net_present_ag_value_adjusted, weight, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight, na.rm = TRUE))\n  ) |&gt; \n  ungroup()\n\nag_losses |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    hh_02 = \"\",\n    NAM_1 = \"Marz\",\n    Average_NPV = \"Avg. NPV of ag income (USD)\",\n    Average_Adjusted_NPV = \"Adjusted avg. NPV of ag income (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\n\nMarz\nAvg. NPV of ag income (USD)\nAdjusted avg. NPV of ag income (USD)\nLoss (USD)\nNo. HH\n\n\n\n\n1\nYerevan\n3,639.9\n2,949.5\n690.4\n1005\n\n\n2\nAragatsotn\n37,721.3\n30,321.9\n7,399.4\n20865\n\n\n3\nArarat\n29,091.2\n22,750.0\n6,341.2\n22185\n\n\n4\nArmavir\n46,713.9\n37,001.0\n9,713.0\n24788\n\n\n5\nGegharkunik\n14,437.6\n11,755.4\n2,682.2\n28854\n\n\n6\nLori\n15,100.1\n12,399.5\n2,700.6\n21479\n\n\n7\nKotayk\n28,639.2\n23,425.6\n5,213.6\n17288\n\n\n8\nShirak\n55,604.9\n45,065.4\n10,539.5\n21000\n\n\n9\nSyunik\n38,137.7\n32,267.9\n5,869.8\n10796\n\n\n10\nVayots Dzor\n25,405.5\n19,010.7\n6,394.8\n5228\n\n\n11\nTavush\n18,736.1\n15,316.2\n3,419.9\n14471\n\n\n\n\n\n\n\nAnd like in our previous example, we can see how the change affects the poor. It appears that in this case those above the poverty line have the greatest average impact.\n\nhh |&gt; \n  left_join(deciles, join_by(interview__key == hhid)) |&gt; \n  left_join(ag_income, join_by(interview__key==interview__key)) |&gt;\n  filter(!is.na(net_present_ag_value)) |&gt; \n  mutate(poor = if_else(poor_Avpovln2022==1, \n                        \"Below poverty line\", \n                        \"Above poverty line\")) |&gt; \n  group_by(poor) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_ag_value, weight, na.rm = TRUE)*er,\n    Average_Adjusted_NPV = weighted.mean(\n      net_present_ag_value_adjusted, weight, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight, na.rm = TRUE))\n  ) |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    poor = \"Poverty\",\n    Average_NPV = \"Average NPV of ag income (USD)\",\n    Average_Adjusted_NPV = \"Adjusted average NPV of ag income (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\nPoverty\nAverage NPV of ag income (USD)\nAdjusted average NPV of ag income (USD)\nLoss (USD)\nNo. HH\n\n\n\n\nAbove poverty line\n33,024.6\n26,621.6\n6,403.0\n147868\n\n\nBelow poverty line\n23,201.8\n18,640.1\n4,561.8\n40090\n\n\n\n\n\n\n\nWe can also explore how both losses are geographically distributed.\n\n# For Buildings\nbuilding_losses &lt;- adm1 |&gt; \n  left_join(building_losses, join_by(NAM_1 == NAM_1)) |&gt; \n  mutate(map_labels = formatC(building_losses$Difference, \n                                           big.mark = \",\", \n                                           format = \"f\", \n                                           digits = 1))\n  \n\nbuilding_losses_map &lt;- tm_shape(building_losses)+\n  tm_polygons(col = \"Difference\", legend.show = FALSE,\n              palette = c(\"YlGn\"))+\n  tm_text(\"map_labels\", size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"Average Imputed Rent NPV Losses (USD)\", \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html#sec-vulshocks",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html#sec-vulshocks",
    "title": "Vulnerability Analysis Calculations",
    "section": "6 Vulnerability Shocks",
    "text": "6 Vulnerability Shocks\n\n6.1 Buildings method two (selection via weights)\nIt was pointed out that the method above, makes a random selection, but if we were to make another selection we might get two completely different results. That means that we are not truly making the selection in an aleatory manner. However, we could argue that a climate event doesn’t either. Here we try a different approach in which we create two calculations from the same household each. One in which the weights are adjusted to match the exposure percentage and one where the weights are adjusted to match the remainder and adjust the value for each type. We continue using the same exposure_dataset we created before.\n\nrent_by_weights &lt;- hh |&gt;\n  select(hous_45__7, interview__key, NAM_1, weight, hh_02) |&gt; \n   mutate(\n         is_dilapidated = if_else(hous_45__7 == 1, TRUE, FALSE)\n         ) |&gt; \n  mutate(is_dilapidated = if_else(is.na(hous_45__7), FALSE, is_dilapidated)) |&gt; \n  left_join(imputed_rent, join_by(interview__key == interview__key )) |&gt; \n  rename(household_id = interview__key) |&gt;\n  left_join(exposure_dataset, by = \"NAM_1\") |&gt;\n  mutate(is_dilapidated = if_else(is.na(hous_45__7) | hous_45__7 == 0, FALSE, TRUE))\n\nWe now create two new weights columns weight_exposed and weight_unexposed.\n\nrent_by_weights &lt;- rent_by_weights |&gt; \n  mutate(weight_exposed = weight * (pct_AA_exposed_buildings/100),\n         weight_unexposed = weight * (1- pct_AA_exposed_buildings/100))\n\nAfter that, we apply the shock to ag income, depending on state of dilapidation.\n\nrent_by_weights &lt;- rent_by_weights |&gt; \n  mutate(adjusted_rent_by_weight = case_when(\n    is_dilapidated ~ imputed_rent * (1 - perc_AAL_dilapidated / 100),\n    !is_dilapidated ~ imputed_rent * (1 - perc_AAL_normal / 100),\n    TRUE ~ imputed_rent\n  ))\n\nAnd we can now estimate a new net present value with the adjusted values in the same manner as before.\n\n# Annual adjusted imputed rent\nrent_by_weights$annual_adjusted_rent_by_weight &lt;-\n  rent_by_weights$adjusted_rent_by_weight * 12 \n\nrent_by_weights$present_value_rent_by_weight &lt;- sapply(1:(years), function(n) {\n  future_adjusted_rent &lt;- rent_by_weights$annual_adjusted_rent_by_weight * (1 + inflation_rate)^(n)\n  present_value_by_weight &lt;- future_adjusted_rent / ((1 + discount_rate)^n)\n  return(present_value_by_weight)\n})\n\n\n# Sum up the present values for the total present value over the period\nrent_by_weights$npv_rent_by_weight &lt;-\n  rowSums(rent_by_weights$present_value_rent_by_weight)\n\nrent_by_weights &lt;- rent_by_weights |&gt; \n  select(-present_value_rent_by_weight)\n\nAnd we can view the compounded differences in NPVs for those affected by Marz and Poverty condition. Note that the weighted sum of households (24,069-24,075) in the tables differs slightly, because of rounding during the sliced calculations to avoid showing fractions of individual homes.\n\nbuilding_losses &lt;- rent_by_weights |&gt; \n  left_join(deciles, join_by(household_id == hhid)) |&gt; \n  rename(poor = poor_Avpovln2022) |&gt;\n  filter(!is.na(imputed_rent)) |&gt; \n  group_by(hh_02,NAM_1) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_value_rent, weight_exposed, na.rm = TRUE)*er,\n    Average_Adjusted_NPV = weighted.mean(\n      npv_rent_by_weight, weight_exposed, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight_exposed, na.rm = TRUE))\n  ) |&gt;\n   ungroup()\n\nbuilding_losses |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    hh_02 = \"\",\n    NAM_1 = \"Marz\",\n    Average_NPV = \"Average NPV of imputed rent (USD)\",\n    Average_Adjusted_NPV = \"Adjusted average NPV of imputed rent (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\n\nMarz\nAverage NPV of imputed rent (USD)\nAdjusted average NPV of imputed rent (USD)\nLoss (USD)\nNo. HH\n\n\n\n\n1\nYerevan\n63,777.4\n58,688.1\n5,089.3\n5004\n\n\n2\nAragatsotn\n25,757.8\n23,636.3\n2,121.5\n1045\n\n\n3\nArarat\n27,523.6\n25,244.3\n2,279.3\n3132\n\n\n4\nArmavir\n27,452.5\n24,979.8\n2,472.8\n1675\n\n\n5\nGegharkunik\n20,080.5\n18,361.4\n1,719.1\n2028\n\n\n6\nLori\n15,268.3\n12,060.6\n3,207.7\n2038\n\n\n7\nKotayk\n29,627.0\n26,766.2\n2,860.8\n2111\n\n\n8\nShirak\n19,430.0\n17,569.2\n1,860.8\n1823\n\n\n9\nSyunik\n26,310.9\n21,452.3\n4,858.6\n1445\n\n\n10\nVayots Dzor\n19,603.1\n16,405.2\n3,197.9\n586\n\n\n11\nTavush\n15,743.5\n12,397.4\n3,346.1\n689\n\n\n\n\n\n\n# write.table(building_losses, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n\nrent_by_weights |&gt; \n  left_join(deciles, join_by(household_id == hhid)) |&gt; \n  #rename(poor = poor_Avpovln2022) |&gt; \n  filter(!is.na(imputed_rent)) |&gt; \n  mutate(poor = if_else(poor_Avpovln2022==1, \n                        \"Below poverty line\", \n                        \"Above poverty line\")) |&gt; \n  group_by( poor) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_value_rent, weight_exposed, na.rm = TRUE) * er,\n    Average_Adjusted_NPV = weighted.mean(\n      npv_rent_by_weight, weight_exposed, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight_exposed, na.rm = TRUE))\n  ) |&gt;\n   ungroup() |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    poor = \"Poverty\",\n    Average_NPV = \"Average NPV of imputed rent (USD)\",\n    Average_Adjusted_NPV = \"Adjusted average NPV of imputed rent (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\nPoverty\nAverage NPV of imputed rent (USD)\nAdjusted average NPV of imputed rent (USD)\nLoss (USD)\nNo. HH\n\n\n\n\nAbove poverty line\n33,373.3\n30,087.4\n3,285.9\n17685\n\n\nBelow poverty line\n30,373.0\n27,415.4\n2,957.7\n3892\n\n\n\n\n\n\n\nAlthough the values are similar for the first method, there are clear differences, the main one being that poor households do not lose more than non-poor households.\n\n\n6.2 Agriculture method two (selection via weights)\nWe now turn our attention to agricultural impacts, using the second method. This time, we also have new flood exposure data for a subset of households, based on a percentage given in the crops_aal dataset (AAL stands for Annual Average Losses), which differs from crops_productivity, in that we only get one productivity loss value for any year. To annualize the monthly ag income variable, we apply a scaling factor of 65%, because with agriculture there is a seasonal component and not all months are equal.\n\nag_income_by_weights &lt;- ag_income |&gt; \n  left_join(ic, join_by(interview__key)) |&gt; \n  select( interview__key, hh_02,NAM_1,totalinc,\n          lvstk_onlyinc, inc4, annual_ag_income, \n          net_present_ag_value ,weight)\n\nAnd now we create two columns for weights, but we focus only on households with agricultural income.\n\nag_income_by_weights &lt;- ag_income_by_weights |&gt; \n  filter(!is.na(annual_ag_income)) |&gt;  # Filter by HHs w. ag inc\n  mutate(lvstk_onlyinc = \n           if_else(is.na(lvstk_onlyinc), 0, lvstk_onlyinc)) |&gt; \n  left_join(crops_aal, join_by(NAM_1)) |&gt; \n  mutate(weight_exposed = weight * (pct_AAE/100),\n         weight_unexposed = weight * (1- pct_AAE/100))\n\nIn this case, we apply the shock to every household with agricultural income. Note that the variable inc4 includes sales of agricultural products and livestock (including imputed value of ag products for own consumption) and so we deduct the livestock component (lvstk_onlyinc) before applying the shock.\n\nag_income_by_weights &lt;- ag_income_by_weights |&gt;\n  # mutate(annual_ag_income = ((inc4 - lvstk_onlyinc) * 12 * ag_scaling))\n  mutate(ag_no_lvstk = (inc4 - lvstk_onlyinc)) |&gt;\n  mutate(adjusted_inc4 = \n           ag_no_lvstk * (1 - pct_AAL/100) +\n           lvstk_onlyinc) |&gt; \n  mutate(adjusted_ag_income_by_weights =\n           ag_no_lvstk * (1 - pct_AAL/100) ) |&gt; \n  mutate(adjusted_ag_income_by_weights = \n           adjusted_ag_income_by_weights + lvstk_onlyinc) |&gt; \n  mutate(adjusted_ag_income_by_weights =\n           adjusted_ag_income_by_weights * 12 * ag_scaling)\n\nAnd then we recalculate NPV for those households affected.\n\nag_income_by_weights$ag_present_value_by_weight &lt;- \n  sapply(1:(years), function(n) {\n  future_adjusted_rent &lt;- \n    ag_income_by_weights$adjusted_ag_income_by_weights * \n    (1 + inflation_rate)^(n)\n  ag_present_value_by_weight &lt;- \n    future_adjusted_rent / ((1 + discount_rate)^n)\n  return(ag_present_value_by_weight)\n})\n\nWe calculate Net Present Value once more.\n\n# Sum up the present values for the total present value over the period\nag_income_by_weights$npv_ag_income_by_weight &lt;-\n  rowSums(ag_income_by_weights$ag_present_value_by_weight) \n\nag_income_by_weights &lt;- ag_income_by_weights |&gt; \n  select(-ag_present_value_by_weight)\n\nAnd compare the losses for those impacted.\n\nag_losses2 &lt;- ag_income_by_weights |&gt; \n  left_join(deciles, join_by(interview__key == hhid)) |&gt; \n  rename(poor = poor_Avpovln2022) |&gt;\n  filter(!is.na(annual_ag_income)) |&gt; \n  group_by(hh_02,NAM_1) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_ag_value, weight_exposed, na.rm = TRUE)*er,\n    Average_Adjusted_NPV = weighted.mean(\n      npv_ag_income_by_weight, weight_exposed, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight_exposed, na.rm = TRUE))\n  ) |&gt;\n   ungroup()\n\nag_losses2 |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    hh_02 = \"\",\n    NAM_1 = \"Marz\",\n    Average_NPV = \"Average NPV of agricultural income (USD)\",\n    Average_Adjusted_NPV = \"Adjusted average NPV of imputed rent (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\n\nMarz\nAverage NPV of agricultural income (USD)\nAdjusted average NPV of imputed rent (USD)\nLoss (USD)\nNo. HH\n\n\n\n\n1\nYerevan\n3,639.9\n3,018.6\n621.3\n42\n\n\n2\nAragatsotn\n37,721.3\n32,543.6\n5,177.7\n804\n\n\n3\nArarat\n29,091.2\n23,422.8\n5,668.4\n1509\n\n\n4\nArmavir\n46,713.9\n39,410.4\n7,303.6\n1220\n\n\n5\nGegharkunik\n14,437.6\n12,817.4\n1,620.3\n1611\n\n\n6\nLori\n15,100.1\n12,984.5\n2,115.6\n733\n\n\n7\nKotayk\n28,639.2\n23,934.7\n4,704.5\n576\n\n\n8\nShirak\n55,604.9\n46,262.5\n9,342.4\n678\n\n\n9\nSyunik\n38,137.7\n32,105.0\n6,032.8\n188\n\n\n10\nVayots Dzor\n25,405.5\n19,522.4\n5,883.1\n141\n\n\n11\nTavush\n18,736.1\n15,077.5\n3,658.7\n208\n\n\n\n\n\n\n\n\nag_income_by_weights |&gt; \n  left_join(deciles, join_by(interview__key == hhid)) |&gt; \n  #rename(poor = poor_Avpovln2022) |&gt; \n  filter(!is.na(annual_ag_income)) |&gt; \n  mutate(poor = if_else(poor_Avpovln2022==1, \n                        \"Below poverty line\", \n                        \"Above poverty line\")) |&gt; \n  group_by( decile) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_ag_value, weight_exposed, na.rm = TRUE) * er,\n    Average_Adjusted_NPV = weighted.mean(\n      npv_ag_income_by_weight, weight_exposed, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight_exposed, na.rm = TRUE))\n  ) |&gt;\n   ungroup() |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    decile = \"Decile\",\n    Average_NPV = \"Average NPV of ag income (USD)\",\n    Average_Adjusted_NPV = \"Adjusted average NPV of ag income (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\nDecile\nAverage NPV of ag income (USD)\nAdjusted average NPV of ag income (USD)\nLoss (USD)\nNo. HH\n\n\n\n\n1\n23,757.7\n20,274.7\n3,483.0\n533\n\n\n2\n22,535.7\n18,812.6\n3,723.1\n800\n\n\n3\n34,730.9\n28,736.7\n5,994.2\n722\n\n\n4\n23,769.6\n19,933.4\n3,836.2\n703\n\n\n5\n28,212.2\n23,805.1\n4,407.1\n818\n\n\n6\n34,301.5\n29,370.4\n4,931.1\n794\n\n\n7\n29,781.4\n25,055.9\n4,725.5\n824\n\n\n8\n32,046.7\n26,483.7\n5,563.0\n852\n\n\n9\n34,380.8\n28,697.7\n5,683.1\n715\n\n\n10\n37,223.9\n31,421.5\n5,802.4\n950\n\n\n\n\n\n\n\nThe government and private sector could use this information as reference to calculate the potential size of a disaster relief fund the size of the potential long term losses.\n\nag_fund &lt;- ag_income_by_weights |&gt; \n  left_join(deciles, join_by(interview__key == hhid)) |&gt; \n  rename(poor = poor_Avpovln2022) |&gt;\n  filter(!is.na(annual_ag_income)) |&gt; \n  group_by(hh_02,NAM_1) |&gt; \n  summarize(\n    Average_NPV = weighted.mean(net_present_ag_value, weight_exposed, na.rm = TRUE)*er,\n    Average_Adjusted_NPV = weighted.mean(\n      npv_ag_income_by_weight, weight_exposed, na.rm = TRUE) * er,\n    Difference =  Average_NPV - Average_Adjusted_NPV,\n    No_HH = round(sum(weight_exposed, na.rm = TRUE))\n  ) |&gt;\n   ungroup()\n\nag_losses2 |&gt; \n    gt() |&gt; \n  fmt_number(\n    columns = c(Average_NPV,Average_Adjusted_NPV, Difference) ,\n    decimals = 1\n  ) |&gt; \n  cols_label(\n    hh_02 = \"\",\n    NAM_1 = \"Marz\",\n    Average_NPV = \"Average NPV of agricultural income (USD)\",\n    Average_Adjusted_NPV = \"Adjusted average NPV of imputed rent (USD)\",\n    Difference = \"Loss (USD)\",\n    No_HH = \"No. HH\"\n  )\n\n\n\n\n\n\n\n\nMarz\nAverage NPV of agricultural income (USD)\nAdjusted average NPV of imputed rent (USD)\nLoss (USD)\nNo. HH\n\n\n\n\n1\nYerevan\n3,639.9\n3,018.6\n621.3\n42\n\n\n2\nAragatsotn\n37,721.3\n32,543.6\n5,177.7\n804\n\n\n3\nArarat\n29,091.2\n23,422.8\n5,668.4\n1509\n\n\n4\nArmavir\n46,713.9\n39,410.4\n7,303.6\n1220\n\n\n5\nGegharkunik\n14,437.6\n12,817.4\n1,620.3\n1611\n\n\n6\nLori\n15,100.1\n12,984.5\n2,115.6\n733\n\n\n7\nKotayk\n28,639.2\n23,934.7\n4,704.5\n576\n\n\n8\nShirak\n55,604.9\n46,262.5\n9,342.4\n678\n\n\n9\nSyunik\n38,137.7\n32,105.0\n6,032.8\n188\n\n\n10\nVayots Dzor\n25,405.5\n19,522.4\n5,883.1\n141\n\n\n11\nTavush\n18,736.1\n15,077.5\n3,658.7\n208",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html#impacts-on-the-income-distribution",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html#impacts-on-the-income-distribution",
    "title": "Vulnerability Analysis Calculations",
    "section": "7 Impacts on the income distribution",
    "text": "7 Impacts on the income distribution\nAfter evaluating the long term impacts to the asset value of imputed rent and agricultural income, it is interesting to evaluate the shorter term repercussions that floods can have in the short term for those affected.\nIn the case of agriculture, the impacts can be readily evaluated, since the variables affected are already part of the income calculations. However, imputed rent is not included in the income calculations so we need to adjust our poverty line to reflect this additional value.\nWe will try two ways of doing this; the first will be to increase the poverty line by the total weighted average of imputed rent. The second will be to increase the poverty line by the regional weighted average of imputed rent.\n\n7.1 Agriculture impacts\nLet’s subtract the monthly impacts from total expenditure on ag income and see how it affects the poverty calculations. We use expenditure here, because official poverty calculations for Armenia are estimated from the consumption side.\nIt was pointed out that these floods are already happening, so showing how many people would get out of poverty if there were measures in place would be more illustrative for the narrative, so we change this here.\n\nag_poverty_impacts &lt;- ca |&gt; \n  left_join(ag_income_by_weights, join_by(hhid == interview__key)) |&gt;\n  filter(!is.na(inc4)) |&gt; \n  mutate(new_totc = totc + (inc4 -\n           adjusted_inc4) ) |&gt;\n  # Equivalized consumption per person per month adjusted by\n  # prices and absentiism:\n  # Total hh consumption / adult equivalent (adj. absent) / price index\n  mutate(new_aec_r = new_totc / ae_r / PI) |&gt; \n  # Recalculate the poverty headcount\n  mutate(new_ag_poorAvpovln2022 = \n           if_else(new_aec_r &lt; 52883, 1, 0)) |&gt;  # Official poverty line\n  ungroup()\n\nLet’s have a look at the two distributions.\n\n# Basic density plot comparing equivalized consumption per capita\nggplot(ag_poverty_impacts, aes(x = aec_r, fill = 'Total Consumption')) + \n  geom_density(alpha = 0.5) + \n  geom_density(\n    data = ag_poverty_impacts, \n    aes(x = new_aec_r, fill = 'Modified Total Consumption'), \n    alpha = 0.5) +\n  labs(\n    fill = \"Consumption Type\", \n    title = \"Comparison of Consumption Distributions\", \n    x = \"Equivalized consumption\", \n    y = \"Density\") +\n  theme_minimal()+\n  coord_cartesian(xlim = c(0, 250000)) + # Zoom in without removing data\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)+\n  geom_vline(xintercept = 55883, \n             color = \"red\", \n             linetype = \"dotted\", \n             size =0.8) +\n  annotate(\"text\", \n           x = 55883, \n           y = 0.0000025, \n           label = \"Poverty line\\nAMD 55,883\", \n           color = \"black\", \n           hjust = -0.1, \n           # vjust = -3.5,\n           #angle = 90, \n           size = 3)\n\n\n\n\n\n\n\n\nWe’ll try to understand the proportion of total income that agriculture (inc4) represents for this group and also, how much the loss represents out of total income. We will also evaluate the average differences between income and expenditure.\n\n# Agriculture income share\nag_poverty_impacts |&gt; \n  mutate(inc4 = if_else(is.na(inc4), 0, inc4)) |&gt;  # Prevent NA divisions\n  mutate(ag_share = inc4 / totalinc) |&gt;  # Ag share\n  mutate(loss_share_ic = (inc4 - adjusted_inc4)/totalinc) |&gt;\n  mutate(loss_share_ca = (inc4 - adjusted_inc4)/totc) |&gt; \n  group_by(NAM_1) |&gt; \n  summarize( avg_ag_share = \n               round(weighted.mean(ag_share,weight_exposed ,na.rm=TRUE)*100, digits=1),\n             avg_total_income = \n               round(weighted.mean(totalinc,weight_exposed, na.rm=TRUE), digits=1),\n             avg_loss_share_ic = \n               round(weighted.mean(loss_share_ic,weight_exposed, na.rm=TRUE)*100, digits=1),\n             avg_total_consumption = \n               round(weighted.mean(totc,weight_exposed, na.rm=TRUE), digits=1),\n             avg_loss_share_ca = \n               round(weighted.mean(loss_share_ca,weight_exposed, na.rm=TRUE)*100, digits=1),\n             ) |&gt; \n  ungroup()|&gt; \n  gt()\n\n\n\n\n\n\n\nNAM_1\navg_ag_share\navg_total_income\navg_loss_share_ic\navg_total_consumption\navg_loss_share_ca\n\n\n\n\nAragatsotn\n24.4\n401463.2\n3.6\n236041.2\n5.3\n\n\nArarat\n22.4\n330124.5\n4.4\n203609.2\n6.1\n\n\nArmavir\n34.8\n315773.4\n5.7\n219602.2\n8.1\n\n\nGegharkunik\n17.5\n259912.3\n2.0\n207924.2\n2.1\n\n\nKotayk\n15.6\n402557.2\n2.4\n249147.2\n4.1\n\n\nLori\n14.5\n212144.4\n2.2\n219069.1\n2.5\n\n\nShirak\n34.3\n281733.7\n5.4\n181769.2\n9.9\n\n\nSyunik\n22.3\n310146.5\n3.8\n223218.2\n5.4\n\n\nTavush\n14.8\n251614.5\n3.0\n208656.7\n3.8\n\n\nVayots Dzor\n23.5\n250903.3\n5.5\n268811.3\n6.0\n\n\nYerevan\n5.3\n222904.9\n0.9\n314544.0\n0.7\n\n\n\n\n\n\n\nWe fixed an earlier mistake in which the poverty calculation methodology differed from the official headcount. Now we can check is if any of the households that were not poor before became poor after the shock.\n\nag_poverty_impacts |&gt;\n  rename(old_poor = poor_Avpovln2022,\n         new_poor = new_ag_poorAvpovln2022) |&gt; \n  group_by(old_poor, new_poor) |&gt;\n  summarize(no_poor = round(sum(weight_exposed*hhsize, na.rm = TRUE))) |&gt; \n  ungroup() |&gt; \n  gt() |&gt; \n  cols_label(\n    old_poor = \"Previous Poor = 1\",\n    new_poor = \"Poor after shock = 1\",\n    no_poor = \"Number of households\"\n  ) |&gt; \n  grand_summary_rows(\n    columns = c(no_poor),\n    fns= list(\n      Total = ~sum(., na.rm = TRUE)\n      ),\n    fmt = list(~ fmt_number(., decimals = 0))\n  ) \n\n\n\n\n\n\n\n\nPrevious Poor = 1\nPoor after shock = 1\nNumber of households\n\n\n\n\n\n0\n0\n24540\n\n\n\n1\n0\n1288\n\n\n\n1\n1\n7928\n\n\nTotal\n—\n—\n33,756\n\n\n\n\n\n\n\nThis shows that 391 out of 7,711 households with agricultural income affected by floods (5%) that were previously non-poor would fall into poverty in 2022.\nIn contrast, if appropriate measures were in place, 225 households that are currently affected by floods, out of 7,711 (2.9%), would fall out of poverty.\n\n\n7.2 Buildings impacts\nFor buildings there is the problem that the original poverty calculations did not include imputed rent as an allowance of the household so there is no variable to impact directly. What we can do is modify the poverty line individually according to the increase in imputed rent and then evaluate who falls into poverty after the shock.\nActually, this is the same as just impacting total consumption by the loss amount and leaving the poverty line where it is.\n\nrent_poverty_impacts &lt;- ca |&gt; \n  left_join(rent_by_weights, join_by(hhid == household_id)) |&gt;\n  #left_join(ic, join_by(hhid == interview__key)) |&gt; \n  filter(!is.na(imputed_rent)) |&gt; \n  mutate(new_totc = totc + (imputed_rent -\n           adjusted_rent_by_weight) ) |&gt;\n  # Equivalized consumption per person per month adjusted by\n  # prices and absentiism:\n  # Total hh consumption / adult equivalent (adj. absent) / price index\n  mutate(new_aec_r = new_totc / ae_r / PI) |&gt; \n  # Recalculate the poverty headcount\n  mutate(new_buildings_poorAvpovln2022 = \n           if_else(new_aec_r &lt; 52883, 1, 0)) |&gt;  # Official poverty line\n  ungroup()\n\nAnd now we can check impacts on poverty.\n\nrent_poverty_impacts |&gt;\n  filter(!is.na(imputed_rent)) |&gt; \n  rename(old_poor = poor_Avpovln2022,\n         new_poor = new_buildings_poorAvpovln2022) |&gt; \n  group_by(old_poor, new_poor) |&gt;\n  summarize(no_poor = round(sum(weight_exposed*hhsize, na.rm = TRUE))) |&gt; \n  ungroup() |&gt; \n  gt() |&gt; \n  cols_label(\n    old_poor = \"Previous Poor = 1\",\n    new_poor = \"Poor after shock = 1\",\n    no_poor = \"Number of households\"\n  ) |&gt; \n  grand_summary_rows(\n    columns = c(no_poor),\n    fns= list(\n      Total = ~sum(., na.rm = TRUE)\n      ),\n    fmt = list(~ fmt_number(., decimals = 0))\n  ) \n\n\n\n\n\n\n\n\nPrevious Poor = 1\nPoor after shock = 1\nNumber of households\n\n\n\n\n\n0\n0\n58569\n\n\n\n1\n0\n1484\n\n\n\n1\n1\n18112\n\n\nTotal\n—\n—\n78,165\n\n\n\n\n\n\n\nThis shows that 471 out of 21,577 households with imputed rent affected by floods (2%) that were previously non-poor would fall into poverty in 2022.\nIf we do it the other way around 374 out of 21,577 households with imputed rent affected by floods (1.7%) that were previously non-poor would fall out of poverty in 2022 if appropriate measures were in place.\nA few measures.\n\n# Agriculture income share\nrent_imp &lt;- rent_poverty_impacts |&gt;\n  # Prevent Na divisions\n  filter(!is.na(imputed_rent)) |&gt;\n  mutate(rent_share_ca = imputed_rent / totc) |&gt; \n  mutate(loss_rent = imputed_rent - adjusted_rent_by_weight) |&gt; \n  mutate(loss_share_ca = loss_rent / totc) |&gt; \n  group_by(poor_Avpovln2022) |&gt; \n  #group_by(hh_02,NAM_1) |&gt;\n  summarize( avg_total_consumption = \n               round(weighted.mean(\n                 totc,weight_exposed, na.rm=TRUE)*er, digits=1),\n             avg_total_imputed_rent = \n               round(weighted.mean(\n                 imputed_rent, weight_exposed, na.rm=TRUE)*er, digits=1),\n             avg_rent_share_ca = \n               round(weighted.mean(\n                 rent_share_ca, weight_exposed ,na.rm=TRUE)*100, digits=1),\n             avg_loss = \n               round(weighted.mean(\n                 loss_rent, weight_exposed, na.rm=TRUE)*er, digits=1),\n             avg_loss_share_ca = \n               round(weighted.mean(\n                 loss_share_ca, weight_exposed ,na.rm=TRUE)*100, digits=1),\n             no_hh = round(sum(weight_exposed, na.rm = TRUE))\n             ) |&gt; \n  ungroup()\n\nrent_imp |&gt; \n  gt()\n\n\n\n\n\n\n\nPoor, Avpovln2022\navg_total_consumption\navg_total_imputed_rent\navg_rent_share_ca\navg_loss\navg_loss_share_ca\nno_hh\n\n\n\n\n0\n474.5\n113.1\n31.2\n10.7\n3.0\n17685\n\n\n1\n344.6\n103.0\n34.9\n9.6\n3.3\n3892\n\n\n\n\n\n\n#write.table(rent_imp, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n\n\n7.3 Combined impacts\nAssuming that the group impacted in agriculture have their dwelling close to their land, we can estimate the combined impacts of both agriculture and building losses and see its effects on the income distribution.\n\ncombined_impacts &lt;- ag_poverty_impacts |&gt;\n  left_join(rent_poverty_impacts, join_by(hhid)) |&gt; \n  select(hh_02.x, NAM_1.x,hhid, totc.x, imputed_rent, \n         adjusted_rent_by_weight, inc4, adjusted_inc4, \n         ae_r.x, aec_r.x,\n         PI.x, poor_Avpovln2022.x, weight_exposed.x,hhsize.x) |&gt;\n  # Prevent NAs from HHs without imputed rent\n  mutate(imputed_rent = if_else(is.na(imputed_rent), 0, imputed_rent),\n         adjusted_rent_by_weight = \n           if_else(is.na(adjusted_rent_by_weight), 0,\n                   adjusted_rent_by_weight))|&gt; \n  mutate(new_totc = totc.x + (imputed_rent -\n           adjusted_rent_by_weight) + (inc4 -\n           adjusted_inc4) ) |&gt;\n  # Equivalized consumption per person per month adjusted by\n  # prices and absentiism:\n  # Total hh consumption / adult equivalent (adj. absent) / price index\n  mutate(new_aec_r = new_totc / ae_r.x / PI.x) |&gt; \n  # Recalculate the poverty headcount\n  mutate(new_combined_poorAvpovln2022 = \n           if_else(new_aec_r &lt; 52883, 1, 0)) |&gt;  # Official poverty line\n  ungroup()\n\nAnd check impacts on poverty.\n\ncombined_impacts |&gt;\n  rename(old_poor = poor_Avpovln2022.x,\n         new_poor = new_combined_poorAvpovln2022) |&gt; \n  group_by(old_poor, new_poor) |&gt;\n  summarize(no_poor = round(sum(weight_exposed.x, na.rm = TRUE))) |&gt; \n  ungroup() |&gt; \n  gt() |&gt; \n  cols_label(\n    old_poor = \"Previous Poor = 1\",\n    new_poor = \"Poor after shock = 1\",\n    no_poor = \"Number of households\"\n  ) |&gt; \n  grand_summary_rows(\n    columns = c(no_poor),\n    fns= list(\n      Total = ~sum(., na.rm = TRUE)\n      ),\n    fmt = list(~ fmt_number(., decimals = 0))\n  ) \n\n\n\n\n\n\n\n\nPrevious Poor = 1\nPoor after shock = 1\nNumber of households\n\n\n\n\n\n0\n0\n6065\n\n\n\n1\n0\n280\n\n\n\n1\n1\n1366\n\n\nTotal\n—\n—\n7,711\n\n\n\n\n\n\n\nAnd we can see how the distribution changes.\n\n# Basic density plot comparing equivalized consumption per capita\nggplot(combined_impacts, aes(x = aec_r.x, fill = 'Consumption without measures')) + \n  geom_density(alpha = 0.5) + \n  geom_density(\n    data = combined_impacts, \n    aes(x = new_aec_r, fill = 'Consumption with measures'), \n    alpha = 0.5) +\n  labs(\n    fill = \"Distribution Type\", \n    #title = \"Comparison of Consumption Distributions\", \n    x = \"Equivalized consumption\", \n    y = \"Density\") +\n  theme_minimal()+\n  coord_cartesian(xlim = c(0, 250000)) + # Zoom in without removing data\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)+\n  geom_vline(xintercept = 55883, \n             color = \"red\", \n             linetype = \"dotted\", \n             size =0.8) +\n  annotate(\"text\", \n           x = 55883, \n           y = 0.0000025, \n           label = \"Poverty line\\nAMD 55,883\", \n           color = \"black\", \n           hjust = -0.1, \n           # vjust = -3.5,\n           #angle = 90, \n           size = 3)\n\n\n\n\n\n\n\n\n\n\n7.4 AAl Maps\n\n# Data for maps\n# Ag losses\nag_pov_aal &lt;- ag_poverty_impacts |&gt; \n  mutate(inc4 = if_else(is.na(inc4), 0, inc4)) |&gt;  # Prevent NA divisions\n  mutate(ag_share = inc4 / totalinc) |&gt;  # Ag share\n  mutate(loss_share_ic = (inc4 - adjusted_inc4)/totalinc) |&gt;\n  mutate(loss_share_ca = (inc4 - adjusted_inc4)/totc) |&gt; \n  group_by(NAM_1) |&gt; \n  summarize( avg_ag_share = \n               round(weighted.mean(ag_share,weight_exposed ,na.rm=TRUE)*100, digits=1),\n             avg_total_income = \n               round(weighted.mean(totalinc,weight_exposed, na.rm=TRUE), digits=1),\n             avg_loss_share_ic = \n               round(weighted.mean(loss_share_ic,weight_exposed, na.rm=TRUE)*100, digits=1),\n             avg_total_consumption = \n               round(weighted.mean(totc,weight_exposed, na.rm=TRUE), digits=1),\n             avg_loss_share_ca = \n               round(weighted.mean(loss_share_ic,weight_exposed, na.rm=TRUE)*100, digits=1),\n             ) |&gt; \n  ungroup()\n\n# Building losses\nbl_pov_aal &lt;- rent_poverty_impacts |&gt; \n  select(NAM_1, hh_02, totc, new_totc, imputed_rent,\n         new_buildings_poorAvpovln2022, poor_Avpovln2022,\n         adjusted_rent_by_weight, weight_exposed, is_dilapidated,\n         hhsize) |&gt; \n  filter(!is.na(imputed_rent)) |&gt; \n  mutate(shr_bl_loss_totc = (imputed_rent -\n           adjusted_rent_by_weight)/totc ) |&gt;\n  mutate(people_dilap = if_else(\n    is_dilapidated==TRUE, round(weight_exposed*hhsize), NA\n  )) |&gt; \n  group_by(hh_02,NAM_1) |&gt; \n  summarize(avg_shr_bl_loss_totc = \n             round(weighted.mean(\n               shr_bl_loss_totc,weight_exposed, \n               na.rm=TRUE)*100, digits=1),\n             people_dilap = sum(people_dilap, na.rm = TRUE)) |&gt; \n  ungroup()\n\n`summarise()` has grouped output by 'hh_02'. You can override using the\n`.groups` argument.\n\n# Combined losses\ncomb_pov_aal &lt;- combined_impacts |&gt; \n  select(NAM_1.x, hh_02.x, totc.x, new_totc, imputed_rent,\n         new_combined_poorAvpovln2022, poor_Avpovln2022.x,\n         weight_exposed.x, hhsize.x) |&gt; \n  #filter(!is.na()) |&gt; \n  mutate(shr_comb_loss_totc=\n           (new_totc-totc.x)/totc.x) |&gt;\n  mutate(no_poor_comb = if_else(\n    new_combined_poorAvpovln2022 == 0 & \n      poor_Avpovln2022.x == 1, \n    round(weight_exposed.x*hhsize.x), NA\n  )) |&gt;\n  group_by(hh_02.x, NAM_1.x) |&gt; \n  summarize(avg_shr_comb_loss_totc=\n              round(weighted.mean(\n                shr_comb_loss_totc,weight_exposed.x, \n                na.rm=TRUE)*100, digits=1),\n            no_more_poor_comb = sum(no_poor_comb, na.rm = TRUE)\n            ) |&gt; \n  ungroup()\n\n`summarise()` has grouped output by 'hh_02.x'. You can override using the\n`.groups` argument.\n\n# Maps\n\naal_map &lt;- adm1 |&gt; \n  left_join(ag_pov_aal, join_by(NAM_1 == NAM_1)) |&gt; \n  left_join(bl_pov_aal, join_by(NAM_1 == NAM_1)) |&gt; \n  left_join(comb_pov_aal, join_by(NAM_1== NAM_1.x))\n\n# Agriculture losses as a share of consumption\naal_map1 &lt;- tm_shape(aal_map)+ \n  tm_polygons(\"avg_loss_share_ca\", legend.show = FALSE) + \n  tm_text(\"avg_loss_share_ca\", size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"a. Agriculture losses as an\\naverage share of consumption,\\nn= 7,711 households\", \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\n# Building losses as a share of consumption\naal_map2 &lt;- tm_shape(aal_map)+ \n  tm_polygons(\"avg_shr_bl_loss_totc\", legend.show = FALSE) + \n  tm_text(\"avg_shr_bl_loss_totc\", size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"b. Building losses as an\\naverage share of consumption,\\nn=21,577 households\", \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\naal_map3 &lt;- tm_shape(aal_map)+ \n  tm_polygons(\"avg_shr_comb_loss_totc\", legend.show = FALSE) + \n  tm_text(\"avg_shr_comb_loss_totc\", size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"c. Combined losses as an\\naverage share of consumption\\n(agriculture + buildings),\\nn=7,711 households\", \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\naal_map4 &lt;- tm_shape(aal_map)+ \n  tm_polygons(\"no_more_poor_comb\", legend.show = FALSE) + \n  tm_text(\"no_more_poor_comb\", size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"Number of people that would leave poverty \", \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\naal_map5 &lt;- tm_shape(aal_map)+ \n  tm_polygons(\"people_dilap\", legend.show = FALSE) + \n  tm_text(\"people_dilap\", size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"Number of people exposed in dilapidated households \", \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\n\ntmap_arrange(aal_map1, aal_map2, aal_map3)\naal_map4\naal_map5\n\n\n\n\n\n\n\nFigure 3: Average Net Present Value of Imputed Rent and Agricultural Income (Year 2022, million AMD)\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Average Net Present Value of Imputed Rent and Agricultural Income (Year 2022, million AMD)\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Average Net Present Value of Imputed Rent and Agricultural Income (Year 2022, million AMD)",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "supporting-materials/vulnerability-hh-level-impacts.html#year-flood-events",
    "href": "supporting-materials/vulnerability-hh-level-impacts.html#year-flood-events",
    "title": "Vulnerability Analysis Calculations",
    "section": "8 100 Year flood events",
    "text": "8 100 Year flood events\n\n8.1 On agriculture\nWe now turn our attention to agricultural impacts, using the second method. This time, we also have new flood exposure data for a subset of households, based on a percentage given in the crops_1_in_100) dataset, which differs from crops_productivity, in that we only get one productivity loss value for any year. To annualize the monthly ag income variable, we apply a scaling factor of 65%, because with agriculture there is a seasonal component and not all months are equal.\n\nag_income_1_in_100 &lt;- ag_income |&gt; \n  left_join(ic, join_by(interview__key)) |&gt; \n  select( interview__key, hh_02,NAM_1,totalinc,\n          lvstk_onlyinc, inc4, annual_ag_income, \n          net_present_ag_value ,weight)\n\nAnd now we create two columns for weights, but we focus only on households with agricultural income.\n\nag_income_1_in_100 &lt;- ag_income_1_in_100 |&gt; \n  filter(!is.na(annual_ag_income)) |&gt;  # Filter by HHs w. ag inc\n  mutate(lvstk_onlyinc = \n           if_else(is.na(lvstk_onlyinc), 0, lvstk_onlyinc)) |&gt; \n  left_join(crops_1in100, join_by(NAM_1)) |&gt; \n  mutate(weight_exposed = weight * (pct_exp_100/100),\n         weight_unexposed = weight * (1- pct_exp_100/100))\n\nIn this case, we apply the shock to every household with agricultural income. Note that the variable inc4 includes sales of agricultural products and livestock (including imputed value of ag products for own consumption) and so we deduct the livestock component (lvstk_onlyinc) before applying the shock.\n\nag_income_1_in_100 &lt;- ag_income_1_in_100 |&gt;\n  # mutate(annual_ag_income = ((inc4 - lvstk_onlyinc) * 12 * ag_scaling))\n  mutate(ag_no_lvstk = (inc4 - lvstk_onlyinc)) |&gt;\n  mutate(adjusted_inc4 = \n           ag_no_lvstk * (1 - pct_1in100/100) +\n           lvstk_onlyinc) |&gt; \n  mutate(adjusted_ag_income_1_in_100 =\n           ag_no_lvstk * (1 - pct_1in100/100) ) |&gt; \n  mutate(adjusted_ag_income_1_in_100 = \n           adjusted_ag_income_1_in_100 + lvstk_onlyinc) |&gt; \n  mutate(adjusted_ag_income_1_in_100 =\n           adjusted_ag_income_1_in_100 * 12 * ag_scaling)\n\nAnd now we calculate impacts on the income distribution.\n\nag_poverty_impacts_1_in_100 &lt;- ca |&gt; \n  left_join(ag_income_1_in_100, join_by(hhid == interview__key)) |&gt;\n  filter(!is.na(inc4)) |&gt; \n  mutate(new_totc = totc - (inc4 -\n           adjusted_inc4) ) |&gt;\n  # Equivalized consumption per person per month adjusted by\n  # prices and absentiism:\n  # Total hh consumption / adult equivalent (adj. absent) / price index\n  mutate(new_aec_r = new_totc / ae_r / PI) |&gt; \n  # Recalculate the poverty headcount\n  mutate(new_ag_poorAvpovln2022 = \n           if_else(new_aec_r &lt; 52883, 1, 0)) |&gt;  # Official poverty line\n  ungroup()\n\nLet’s have a look at the two distributions.\n\n# Basic density plot comparing equivalized consumption per capita\nggplot(ag_poverty_impacts_1_in_100, aes(x = aec_r, fill = 'Total Consumption')) + \n  geom_density(alpha = 0.5) + \n  geom_density(\n    data = ag_poverty_impacts_1_in_100, \n    aes(x = new_aec_r, fill = 'Modified Total Consumption'), \n    alpha = 0.5) +\n  labs(\n    fill = \"Consumption Type\", \n    title = \"Comparison of Consumption Distributions\", \n    x = \"Equivalized consumption\", \n    y = \"Density\") +\n  theme_minimal()+\n  coord_cartesian(xlim = c(0, 250000)) + # Zoom in without removing data\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)+\n  geom_vline(xintercept = 55883, \n             color = \"red\", \n             linetype = \"dotted\", \n             size =0.8) +\n  annotate(\"text\", \n           x = 55883, \n           y = 0.0000025, \n           label = \"Poverty line\\nAMD 55,883\", \n           color = \"black\", \n           hjust = -0.1, \n           # vjust = -3.5,\n           #angle = 90, \n           size = 3)\n\n\n\n\n\n\n\n\nWe’ll try to understand the proportion of total income that agriculture (inc4) represents for this group and also, how much the loss represents out of total income. We will also evaluate the average differences between income and expenditure.\n\n# Agriculture income share\nag_pov &lt;- ag_poverty_impacts_1_in_100 |&gt; \n  filter(!is.na(inc4)) |&gt; \n  mutate(inc4 = if_else(is.na(inc4), 0, inc4)) |&gt;  # Prevent NA divisions\n  mutate(ag_share = inc4 / totalinc) |&gt;  # Ag share\n  mutate(loss = (inc4 - adjusted_inc4)) |&gt; \n  mutate(loss_share_ic = (inc4 - adjusted_inc4)/totalinc) |&gt;\n  mutate(loss_share_ca = (inc4 - adjusted_inc4)/totc) |&gt; \n  #group_by(hh_02,NAM_1) |&gt; \n  summarize( avg_ag_income = \n               round(weighted.mean(\n                 inc4,weight_exposed ,na.rm=TRUE)*er, digits=1),\n             avg_ag_share = \n               round(weighted.mean(\n                 ag_share,weight_exposed ,na.rm=TRUE)*100, digits=1),\n             avg_total_income = \n               round(weighted.mean(\n                 totalinc,weight_exposed, na.rm=TRUE)*er, digits=1),\n             avg_loss = \n               round(weighted.mean(\n                 loss,weight_exposed, na.rm=TRUE)*er, digits=1),\n             avg_loss_share_ic = \n               round(weighted.mean(\n                 loss_share_ic,weight_exposed, na.rm=TRUE)*100, digits=1),\n             avg_total_consumption = \n               round(weighted.mean(\n                 totc,weight_exposed, na.rm=TRUE)*er, digits=1),\n             avg_loss_share_ca = \n               round(weighted.mean(\n                 loss_share_ca,weight_exposed, na.rm=TRUE)*100, digits=1),\n             no_hh =\n               round(sum(weight_exposed,na.rm = TRUE))\n             ) |&gt; \n  ungroup()\n\nag_pov |&gt; \n  gt()\n\n\n\n\n\n\n\navg_ag_income\navg_ag_share\navg_total_income\navg_loss\navg_loss_share_ic\navg_total_consumption\navg_loss_share_ca\nno_hh\n\n\n\n\n161.3\n23.3\n705.2\n31.7\n4.6\n498.5\n6.5\n68969\n\n\n\n\n\n\n# write.table(ag_pov, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nWe fixed an earlier mistake in which the poverty calculation methodology differed from the official headcount. Now we can check is if any of the households that were not poor before became poor after the shock.\n\nag_imp &lt;- ag_poverty_impacts_1_in_100 |&gt;\n  rename(old_poor = poor_Avpovln2022,\n         new_poor = new_ag_poorAvpovln2022) |&gt; \n  group_by(old_poor, new_poor) |&gt;\n  summarize(no_poor = round(sum(weight_exposed, na.rm = TRUE))) |&gt; \n  ungroup()\n\nag_imp |&gt; \n  gt() |&gt; \n  cols_label(\n    old_poor = \"Previous Poor = 1\",\n    new_poor = \"Poor after shock = 1\",\n    no_poor = \"Number of households\"\n  ) |&gt; \n  grand_summary_rows(\n    columns = c(no_poor),\n    fns= list(\n      Total = ~sum(., na.rm = TRUE)\n      ),\n    fmt = list(~ fmt_number(., decimals = 0))\n  ) \n\n\n\n\n\n\n\n\nPrevious Poor = 1\nPoor after shock = 1\nNumber of households\n\n\n\n\n\n0\n0\n49156\n\n\n\n0\n1\n4873\n\n\n\n1\n1\n14940\n\n\nTotal\n—\n—\n68,969\n\n\n\n\n\n\n# write.table(ag_imp, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n4873 households that were not previously poor out of 68,969 (7%) would fall into poverty.\n\n\n8.2 On buildings\n\nrent_1_in_100 &lt;- hh |&gt;\n  select(hous_45__7, interview__key, NAM_1, weight, hh_02) |&gt; \n   mutate(\n         is_dilapidated = if_else(hous_45__7 == 1, TRUE, FALSE)\n         ) |&gt; \n  mutate(is_dilapidated = if_else(is.na(hous_45__7), FALSE, is_dilapidated)) |&gt; \n  left_join(imputed_rent, join_by(interview__key == interview__key )) |&gt; \n  rename(household_id = interview__key) |&gt;\n  left_join(buildings_1in100, by = \"NAM_1\") |&gt;\n  mutate(is_dilapidated = if_else(is.na(hous_45__7) | hous_45__7 == 0, FALSE, TRUE))\n\nWe now create two new weights columns weight_exposed and weight_unexposed.\n\nrent_1_in_100 &lt;- rent_1_in_100 |&gt; \n  mutate(weight_exposed = weight * (pct_100_exposed_buildings/100),\n         weight_unexposed = weight * (1- pct_100_exposed_buildings/100))\n\nAfter that, we apply the shock to ag income, depending on state of dilapidation.\n\nrent_1_in_100 &lt;- rent_1_in_100 |&gt; \n  mutate(adjusted_rent_1_in_100 = case_when(\n    is_dilapidated ~ imputed_rent * (1 - perc_dilapidated_100 / 100),\n    !is_dilapidated ~ imputed_rent * (1 - perc_normal_100 / 100),\n    TRUE ~ imputed_rent\n  ))\n\nAnd now we calculate impacts on the income distribution.\n\nrent_poverty_impacts_1_in_100 &lt;- ca |&gt; \n  left_join(rent_1_in_100, join_by(hhid == household_id)) |&gt;\n  #filter(!is.na(imputed_rent)) |&gt; \n  mutate(new_totc = totc - (imputed_rent -\n           adjusted_rent_1_in_100) ) |&gt;\n  # Equivalized consumption per person per month adjusted by\n  # prices and absentiism:\n  # Total hh consumption / adult equivalent (adj. absent) / price index\n  mutate(new_aec_r = new_totc / ae_r / PI) |&gt; \n  # Recalculate the poverty headcount\n  mutate(new_buildings_poorAvpovln2022 = \n           if_else(new_aec_r &lt; 52883, 1, 0)) |&gt;  # Official poverty line\n  ungroup()\n\nAnd now we can check impacts on poverty.\n\nrent1in100 &lt;- rent_poverty_impacts_1_in_100 |&gt;\n  filter(!is.na(imputed_rent)) |&gt; \n  rename(old_poor = poor_Avpovln2022,\n         new_poor = new_buildings_poorAvpovln2022) |&gt; \n  group_by(old_poor, new_poor) |&gt;\n  summarize(no_poor = round(sum(weight_exposed, na.rm = TRUE))) |&gt; \n  ungroup()\n\nrent1in100 |&gt; \n  gt() |&gt; \n  cols_label(\n    old_poor = \"Previous Poor = 1\",\n    new_poor = \"Poor after shock = 1\",\n    no_poor = \"Number of households\"\n  ) |&gt; \n  grand_summary_rows(\n    columns = c(no_poor),\n    fns= list(\n      Total = ~sum(., na.rm = TRUE)\n      ),\n    fmt = list(~ fmt_number(., decimals = 0))\n  ) \n\n\n\n\n\n\n\n\nPrevious Poor = 1\nPoor after shock = 1\nNumber of households\n\n\n\n\n\n0\n0\n163752\n\n\n\n0\n1\n5237\n\n\n\n1\n1\n37602\n\n\nTotal\n—\n—\n206,591\n\n\n\n\n\n\n# write.table(rent1in100, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n5,237 households out of 206,591 (2%) that were not previously poor would fall into poverty under a 100 year flood event.\n\n# Buildings\nrent_imp &lt;- rent_poverty_impacts_1_in_100 |&gt; \n  filter(!is.na(imputed_rent)) |&gt;\n  mutate(totc = if_else(is.na(totc), 0, totc)) |&gt;  # Prevent NA divisions\n  mutate(loss_share_ca = (new_totc - totc)/totc) |&gt; \n  group_by(hh_02,NAM_1) |&gt; \n  summarize( avg_total_expenditure = \n               round(weighted.mean(totc,weight_exposed, na.rm=TRUE), digits=1),\n             avg_new_total_expenditure = \n               round(weighted.mean(new_totc,weight_exposed, na.rm=TRUE), digits=1),\n             avg_loss_share_ca = \n               round(weighted.mean(loss_share_ca,weight_exposed, na.rm=TRUE)*100, digits=1),\n             no_hh = \n               round(sum(weight_exposed, na.rm = TRUE))\n             ) |&gt; \n  ungroup()\n  \n\nrent_imp |&gt; \n  gt()\n\n\n\n\n\n\n\n02. Marz\nNAM_1\navg_total_expenditure\navg_new_total_expenditure\navg_loss_share_ca\nno_hh\n\n\n\n\n1\nYerevan\n196427.7\n188406.0\n-5.4\n50578\n\n\n2\nAragatsotn\n216219.6\n212852.5\n-2.0\n9082\n\n\n3\nArarat\n194454.2\n190728.1\n-2.3\n33136\n\n\n4\nArmavir\n191307.9\n187495.6\n-2.7\n19191\n\n\n5\nGegharkunik\n191801.6\n188828.6\n-2.1\n17764\n\n\n6\nLori\n189554.3\n184421.8\n-3.5\n19186\n\n\n7\nKotayk\n216616.7\n211771.1\n-2.8\n18376\n\n\n8\nShirak\n162371.9\n159457.5\n-2.4\n16419\n\n\n9\nSyunik\n195658.6\n187870.2\n-4.8\n12301\n\n\n10\nVayots Dzor\n230628.3\n224606.0\n-3.6\n4513\n\n\n11\nTavush\n186668.4\n181039.5\n-3.9\n6047\n\n\n\n\n\n\n# write.table(rent_imp, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n\n\n8.3 Combined 1 in 100 year flood event impacts\nAssuming that the group impacted in agriculture have their dwelling close to their land, we can estimate the combined impacts of both agriculture and building losses and see its effects on the income distribution.\n\ncombined_impacts_1_in_100 &lt;- ag_poverty_impacts_1_in_100 |&gt;\n  left_join(rent_poverty_impacts_1_in_100, join_by(hhid)) |&gt; \n  select(hh_02.x, NAM_1.x,hhid, totc.x, imputed_rent, adjusted_rent_1_in_100, \n         inc4, adjusted_inc4, ae_r.x, aec_r.x, totalinc,\n         PI.x, poor_Avpovln2022.x, weight_exposed.x) |&gt;\n  # Prevent NAs from HHs without imputed rent\n  mutate(imputed_rent = if_else(is.na(imputed_rent), 0, imputed_rent),\n         adjusted_rent_1_in_100 = \n           if_else(is.na(adjusted_rent_1_in_100), 0,\n                   adjusted_rent_1_in_100))|&gt; \n  mutate(new_totc = totc.x - (imputed_rent -\n           adjusted_rent_1_in_100) - (inc4 -\n           adjusted_inc4) ) |&gt;\n  # Equivalized consumption per person per month adjusted by\n  # prices and absentiism:\n  # Total hh consumption / adult equivalent (adj. absent) / price index\n  mutate(new_aec_r = new_totc / ae_r.x / PI.x) |&gt; \n  # Recalculate the poverty headcount\n  mutate(new_combined_poorAvpovln2022 = \n           if_else(new_aec_r &lt; 52883, 1, 0)) |&gt;  # Official poverty line\n  ungroup()\n\n\n# Agriculture income share\ncombined_pov &lt;- combined_impacts_1_in_100 |&gt; \n  filter(!is.na(inc4)) |&gt; \n  mutate(inc4 = if_else(is.na(inc4), 0, inc4)) |&gt;  # Prevent NA divisions\n  mutate(loss = (totc.x - new_totc)) |&gt; \n  mutate(loss_share_ic = loss/totalinc) |&gt;\n  mutate(loss_share_ca = loss/totc.x) |&gt; \n  #group_by(hh_02.x,NAM_1.x) |&gt; \n  summarize( avg_total_income = \n               round(weighted.mean(\n                 totalinc,weight_exposed.x, na.rm=TRUE)*er, digits=1),\n             avg_loss = \n               round(weighted.mean(\n                 loss,weight_exposed.x, na.rm=TRUE)*er, digits=1),\n             avg_loss_share_ic = \n               round(weighted.mean(\n                 loss_share_ic,weight_exposed.x, na.rm=TRUE)*100, digits=1),\n             avg_total_consumption = \n               round(weighted.mean(\n                 totc.x,weight_exposed.x, na.rm=TRUE)*er, digits=1),\n             avg_loss_share_ca = \n               round(weighted.mean(\n                 loss_share_ca,weight_exposed.x, na.rm=TRUE)*100, digits=1),\n             no_hh =\n               round(sum(weight_exposed.x,na.rm = TRUE))\n             ) |&gt; \n  ungroup()\n\ncombined_pov |&gt; \n  gt()\n\n\n\n\n\n\n\navg_total_income\navg_loss\navg_loss_share_ic\navg_total_consumption\navg_loss_share_ca\nno_hh\n\n\n\n\n705.2\n38.2\n6.3\n498.5\n8.1\n68969\n\n\n\n\n\n\n# write.table(combined_pov, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nAnd check impacts on poverty.\n\ncombined_imp &lt;- combined_impacts_1_in_100 |&gt;\n  rename(old_poor = poor_Avpovln2022.x,\n         new_poor = new_combined_poorAvpovln2022) |&gt; \n  group_by(old_poor, new_poor) |&gt;\n  summarize(no_poor = round(sum(weight_exposed.x, na.rm = TRUE))) |&gt; \n  ungroup()\n\ncombined_imp |&gt; \n  gt() |&gt; \n  cols_label(\n    old_poor = \"Previous Poor = 1\",\n    new_poor = \"Poor after shock = 1\",\n    no_poor = \"Number of households\"\n  ) |&gt; \n  grand_summary_rows(\n    columns = c(no_poor),\n    fns= list(\n      Total = ~sum(., na.rm = TRUE)\n      ),\n    fmt = list(~ fmt_number(., decimals = 0))\n  ) \n\n\n\n\n\n\n\n\nPrevious Poor = 1\nPoor after shock = 1\nNumber of households\n\n\n\n\n\n0\n0\n47611\n\n\n\n0\n1\n6418\n\n\n\n1\n1\n14940\n\n\nTotal\n—\n—\n68,969\n\n\n\n\n\n\n# write.table(combined_imp, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n6,418 out of 68,969 households (9.3%) would fall into poverty under this combined shock.\nAnd we can see how the distribution changes.\n\n# Basic density plot comparing equivalized consumption per capita\nggplot(combined_impacts_1_in_100, aes(x = aec_r.x, fill = 'Current consumption')) + \n  geom_density(alpha = 0.5) + \n  geom_density(\n    data = combined_impacts_1_in_100, \n    aes(x = new_aec_r, fill = 'Consumption after shock'), \n    alpha = 0.5) +\n  labs(\n    fill = \"Distribution Type\", \n    #title = \"Comparison of Consumption Distributions Under Vulnerability\", \n    x = \"Equivalized consumption\", \n    y = \"Density\") +\n  theme_minimal()+\n  coord_cartesian(xlim = c(0, 250000)) + # Zoom in without removing data\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)+\n  geom_vline(xintercept = 55883, \n             color = \"red\", \n             linetype = \"dotted\", \n             size =0.8) +\n  annotate(\"text\", \n           x = 55883, \n           y = 0.0000025, \n           label = \"Poverty line\\nAMD 55,883\", \n           color = \"black\", \n           hjust = -0.1, \n           # vjust = -3.5,\n           #angle = 90, \n           size = 3)\n\n\n\n\n\n\n\n\nWe save everything to a vulnerability.R file\n\n#save.image(file='outputs/vulnerability.RData')",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Vulnerability Analysis Calculations"
    ]
  },
  {
    "objectID": "presentations/ARM_poverty.html#content",
    "href": "presentations/ARM_poverty.html#content",
    "title": "Global Decarbonization and Impacts on Poverty in Armenia",
    "section": "Content",
    "text": "Content\n\n\n\nArmenia CCDR - The World Bank"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html",
    "href": "drafts/CCDR-poverty-code-explanation.html",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "",
    "text": "Filename: 01_Master.do\nThe master file explains the structured workflow:\n\nSetup and Preprocessing: The initial segments define the working environment, set the necessary paths, and import necessary libraries or tools. The globals defined will help in driving the subsequent operations.\nMacro Inputs & Creation of Globals: Fetches macro inputs from an Excel file and dynamically define some global variables. There are placeholder comments suggesting an alternate approach to fetch scenarios and years, which might have been used in earlier iterations or for debugging purposes.\nMFmod Microsimulation: Run a microsimulation model to generate simulated weights and welfare aggregate measures.\nIndicator Generation: Using the simulated data, compute various indicators. These are poverty rates, income measures, and other socio-economic indicators.\nOpening the Excel Scenario File: Lastly, the main workflow ends by opening an Excel file to inspect or use the scenarios and results interactively."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#program-setup",
    "href": "drafts/CCDR-poverty-code-explanation.html#program-setup",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.1 Program setup",
    "text": "2.1 Program setup\nThis sets the minimum required Stata version to run the code to 15.1. We also perform some housekeeping:\n\ndrop _all removes all variables from the dataset in memory.\nclear all removes all data from memory.\nset more off turns off the ‘more’ feature, which pauses output at the screen every time it’s filled.\n\nversion 15.1\ndrop _all\nclear all\nset more off\nChecks if the current Stata user’s name is \"wb527706\". If it is, it sets the global macro path to the specified directory. This tailors the code to a specific user’s directory structure.\nif \"`c(username)'\"==\"wb527706\" {\n    global path \"C:\\Users\\WB527706\\OneDrive - WBG\\Madagascar\"\n}\nThe next lines set up a series of global macros with directory paths based on the earlier-defined path:\nglobal HHsurvey    = \"$path\\data\"\nglobal inputs      = \"$path/inputs\"    \nglobal outputs     = \"$path/outputs\"\nglobal MAGclimsim    \"$path\\MagClimSim.xlsx\""
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#hh-consumption-aggregates-and-characteristics",
    "href": "drafts/CCDR-poverty-code-explanation.html#hh-consumption-aggregates-and-characteristics",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.2 HH consumption aggregates and characteristics",
    "text": "2.2 HH consumption aggregates and characteristics\nThis loads a dataset from the specified directory, which was previously defined in the global macro HHsurvey. The clear option ensures that any current data in memory is replaced. Note that we don’t have access to that survey.\nuse \"$HHsurvey\\outputdata\\ca_real_pl.dta\", clear\nHere, a new variable rural is created, which is equal to 1 if the area is “Rural” and 0 if the area is “Urbain”.\ngen     rural   = 1     if  area==\"Rural\"\nreplace rural   = 0     if  area==\"Urbain\"\nThe variable pcer is renamed to pcc (indicating per capita expenditure).\nren pcer pcc\nA new variable yhh is created by multiplying pcc (per capita expenditure) by hhsize (household size).\ngen yhh=pcc*hhsize\nThese lines rename the variables: wgt becomes wgt0 and wgt_adj becomes wgt.\nren wgt wgt0 \nren wgt_adj wgt\nThis line keeps only the listed variables in the dataset, dropping all others.\nkeep  hhid rural hhsize region pcc yhh wgt yhh // \n      pline215 pline365 pline685 ubpl lbpl\nA temporary file named HHcha is specified, and the current dataset is saved to that file.\ntempfile HHcha\nsave `HHcha'\nSo far:\n\nSets up paths tailored to the user’s directory structure.\nLoads a specific dataset related to household consumption aggregates and characteristics.\nManipulates and keeps only select variables of interest.\nSaves the modified data to a temporary file for later use."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#demographic-characteristics",
    "href": "drafts/CCDR-poverty-code-explanation.html#demographic-characteristics",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.3 Demographic characteristics",
    "text": "2.3 Demographic characteristics\nThis line loads a dataset related to demographic characteristics from the directory specified by the global macro HHsurvey, replacing data in memory.\nuse \"$HHsurvey\\outputdata\\S01_DEMO_01.dta\", clear\nHere, a new variable hhgrap is created, identical to the cluster variable. The dataset in memory is then merged with the DéfinitionsQuatreMilieux.dta dataset based on the hhgrap variable. The resulting _merge variable (indicating merge success for each observation) is dropped as customary.\ngen hhgrap = cluster \nmerge n:1 hhgrap using \"$HHsurvey\\outputdata\\DéfinitionsQuatreMilieux.dta\"\ndrop _merge\nThe variable MILIEU4 is renamed to zone and the variable sex is renamed to gender. A new binary variable head is created, which takes a value of 1 when relhead equals 1 (presumably indicating that the individual is the head of the household) and 0 otherwise. The variable relhead is renamed to rel.\nren MILIEU4 zone\nren sex gender\ngen head = relhead == 1\nren relhead rel\nOnly the specified variables are retained, and all other variables are dropped.\nkeep hhid pid gender age head rel zone\nThe dataset in memory is saved to a temporary file named demo.\ntempfile demo\nsave `demo'"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#years-of-education",
    "href": "drafts/CCDR-poverty-code-explanation.html#years-of-education",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.4 Years of education",
    "text": "2.4 Years of education\nA new dataset related to education is loaded, replacing the current dataset in memory. Note that we don’t have access to that one.\nuse \"$HHsurvey\\inputdata\\EPM21\\S02_EDUC.dta\", clear\nA new variable yos (probably standing for the availability of “years of schooling”) is created based on the variable q2_26. However, its value is set to 0 wherever the variable q2_01 is not equal to 1. Note that the consultant made a code annotation explicitly sayin that it is NOT “years of schooling”. Then the variable yos is renamed to educy.\ngen yos = q2_26\nreplace yos=0 if q2_01 != 1\nrename yos educy\nThe data is sorted by hhgrap and hhnum. Then, a new variable hhid is created by concatenating hhgrap and hhnum, presumably to create a unique household ID. The variables hhgrap and q2_0x are then renamed to cluster and pid, respectively.\nsort hhgrap hhnum\negen hhid = concat(hhgrap hhnum), format(%9.0g) punct(\",\")\nrename hhgrap cluster\nrename q2_0x pid\nOnly the specified variables are retained, and all others are dropped. The dataset in memory is saved to a temporary file named edu.\nkeep hhid pid educy\ntempfile edu\nsave `edu'\nTo summarize, this code segment:\n\nLoads a dataset related to demographic characteristics.\nMerges it with another dataset to enhance the information.\nMakes some modifications to the variable names and keeps relevant variables.\nSaves the modified dataset to a temporary file.\nLoads a new dataset related to years of education.\nMakes transformations related to educational years and constructs a unique household ID.\nKeeps the necessary variables and saves the modified dataset to another temporary file."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#non-labor-income",
    "href": "drafts/CCDR-poverty-code-explanation.html#non-labor-income",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.5 Non labor income",
    "text": "2.5 Non labor income\n\n2.5.1 Pensions, rents, and dividends\nThis section of code is focused on calculating and aggregating non-labor income. First, we have pensions, rents, dividends, and lotteries. The script loads a dataset related to various revenues.\nuse \"$HHsurvey\\inputdata\\EPM21\\S05_REVE.dta\", clear\nThe next series of lines, as before, creates a unique household ID by concatenating hhgrap and hhnum, and then renames some variables.\nsort    hhgrap hhnum\negen    hhid =  concat(hhgrap hhnum), format(%9.0g) punct(\",\")\nrename  hhgrap  cluster\nrename  q5_0x   pid \nFor each type of income (pensions, rents, dividends, and occasional income like lottery winnings), the script creates a new variable only if another corresponding variable is set to 1 (probably indicating that the income source is relevant for that observation).\nSurvey: Pension annual basis: retrate/veuvage/dinvalidité/alimentaire.\ngen nli_pen1=q5_02 if q5_01==1\ngen nli_pen2=q5_04 if q5_03==1 \ngen nli_pen3=q5_06 if q5_05==1 \ngen nli_pen4=q5_08 if q5_07==1 \nSurvey: Rents.\ngen nli_rent=q5_10 if q5_09==1\nSurvey: Dividends.\ngen nli_div=q5_12 if q5_11==1\nSurvey: Lottery winnings, inheritance, sale of property, etc.\ngen nli_occ=q5_14 if q5_13==1\nThis line creates a new variable, nli_prdo (presumably non-labor income pensions, rents, dividents, other), that sums up the previously created income variables for each observation. The data is then collapsed (aggregated) by hhid, and saved into a temporary file.\negen nli_prdo=rsum(nli_pen1 nli_pen2 nli_pen3 nli_pen4 nli_rent nli_div nli_occ)\ncollapse (sum) nli_prdo ,by(hhid)\ntempfile nli_prdo\nsave `nli_prdo'\n\n\n2.5.2 Individual transfers\nThe next section focuses on individual transfers. It follows a similar structure: load data, create a unique household ID, create variables for different income sources (in this case, transfers), aggregate by household, and save to a temporary file.\nThe new variables (nli_hhs1 and nli_hhs2) are generated based on conditions related to the frequency of the transfer (monthly, quarterly, etc.), and then aggregated into nli_itrans.\nuse \"$HHsurvey\\inputdata\\EPM21\\S13_TRAN_A.dta\", clear\n\n//ID at HH and IND definition \n   \nsort    hhgrap hhnum\negen    hhid =  concat(hhgrap hhnum), format(%9.0g) punct(\",\")\nrename  hhgrap  cluster\n    \ngen      nli_hhs1=q13_18a*12     if q13_18b==1 & (q13_01==1 | q13_02==1) \nreplace  nli_hhs1=q13_18a*4      if q13_18b==2 & (q13_01==1 | q13_02==1)\nreplace  nli_hhs1=q13_18a*2      if q13_18b==3 & (q13_01==1 | q13_02==1)\nreplace  nli_hhs1=q13_18a        if q13_18b==4 & (q13_01==1 | q13_02==1)\nreplace  nli_hhs1=q13_18a        if q13_18b==5 & (q13_01==1 | q13_02==1)\n    \ngen     nli_hhs2=q13_21a*12      if q13_21b==1 & (q13_01==1 | q13_02==1)\nreplace  nli_hhs2=q13_21a*4      if q13_21b==2 & (q13_01==1 | q13_02==1)\nreplace  nli_hhs2=q13_21a*2      if q13_21b==3 & (q13_01==1 | q13_02==1)\nreplace  nli_hhs2=q13_21a        if q13_21b==4 & (q13_01==1 | q13_02==1)\nreplace  nli_hhs2=q13_21a        if q13_21b==5 & (q13_01==1 | q13_02==1)\n    \negen nli_itrans=rsum(nli_hhs1 nli_hhs2)\ncollapse (sum) nli_itrans ,by(hhid)\nSaved temporarily in:\ntempfile nli_itrans\nsave `nli_itrans'\n\n\n2.5.3 Public Transfers\nThis section deals with public transfers. The data handling is quite similar to the previous sections. The income from public transfers is calculated based on the frequency, aggregated by household, and saved into a temporary file nli_gov.\nu \"$HHsurvey\\inputdata\\EPM21\\S15_FILE.dta\", clear\n\n//ID at HH and IND definition \nsort    hhgrap hhnum\negen    hhid =  concat(hhgrap hhnum), format(%9.0g) punct(\",\")\nrename  hhgrap  cluster\n    \ng        nli_gov=q15_10a*12     if q15_10b==1 & (q15_02==1 ) \nreplace  nli_gov=q15_10a*4      if q15_10b==2 & (q15_02==1 )  \nreplace  nli_gov=q15_10a*2      if q15_10b==3 & (q15_02==1 ) \nreplace  nli_gov=q15_10a        if q15_10b==4 & (q15_02==1 )\nreplace  nli_gov=q15_10a        if q15_10b==5 & (q15_02==1 )    \n\ncollapse (sum) nli_gov ,by(hhid)\ntempfile nli_gov\nsave `nli_gov'\nThe last piece of code merges the aggregated data from the three previous sections\nu  `nli_prdo', clear\nmerge 1:1 hhid using `nli_itrans' , nogen\nmerge 1:1 hhid using `nli_gov' , nogen\nHere, the script loads the data related to pensions, rents, dividends, etc., and then merges it with the data from individual and public transfers using hhid as the key variable.\nA new variable, nli_hh, is created, which is the sum of all non-labor income sources for each household.\negen nli_hh=rsum(nli_prdo nli_gov nli_itrans)\nFinally, this aggregated data is saved into another temporary file.\ntempfile nli\nsave `nli'\nTo summarize, this segment of the code:\n\nLoads datasets related to various non-labor income sources: pensions, rents, dividends, individual transfers, and public transfers.\nProcesses each data source to calculate the total income for each type.\nMerges the data from the various sources and aggregates the total non-labor income by household.\nSaves the resulting dataset for future use."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#labor-market-variables",
    "href": "drafts/CCDR-poverty-code-explanation.html#labor-market-variables",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.6 Labor market variables",
    "text": "2.6 Labor market variables\nA dataset related to employment is loaded into Stata.\nu \"$HHsurvey\\inputdata\\EPM21\\S04_EMPL_AI.dta\", clear\nThe subsequent lines create a unique household ID by concatenating hhgrap and hhnum, and rename some variables. This pattern has been repeated from the previous chunks of code. The data is then merged with the non-labor income dataset (created in the previous section):\nsort    hhgrap hhnum\negen    hhid =  concat(hhgrap hhnum), format(%9.0g) punct(\",\")\nrename  hhgrap  cluster\nrename  q4a_xx  pid\n    \n// Merge non-labor income part\nmerge n:1 hhid using `nli' , nogen\n\n2.6.1 Labor Force Status:\nThe variable lstatus is created based on different responses to the employment-related questions. This variable categorizes individuals into employed, unemployed and seeking work, unemployed but not seeking work, or outside the labor force (OLF).\n//employed\ng lstatus = 1 if  q4a_01==1 | q4a_02==1 | q4a_03==1 | //\n                  q4a_04==1 | q4a_09&lt;4\n\n// Unemployed - available and searching \nreplace lstatus = 2  if q4a_96==1 \n\n// Unemployed - available, but not searching\nreplace lstatus = 3  if q4a_96==2   \n\n//OLF\nreplace lstatus = 4  if missing(lstatus) \n\nlab var lstatus \"Labor force status\"\nlab def lstatus_lab 1 \"employed\" 2 \"unemployed - seeking\" //\n                    3 \"unemployed - not-seeking\" 4 \"OLF\"\nlab val lstatus lstatus_lab\n\n\n2.6.2 Employed Population and Salaried Status:\nThe code identifies which individuals are employed (employed variable) and then aggregates this at the household level. It then determines if employed individuals are salaried or self-employed.\ngen employed = lstatus==1\nbysort hhid : egen employed_hh=max(employed)\n\ngen salaried = .\nreplace salaried = 1 if q4a_24 ==1\n\n// self-employed if employed and salary is missing.\nreplace salaried = 0 if mi(salaried) & employed==1 \nlab var salaried \"Employment status\"\nlab def salaried 1 \"paid employee\" 0 \"self-employed\"\nlab val salaried salaried\n\n\n2.6.3 Labor Income Computations:\nThe subsequent part of the code calculates labor income (lab_pri) based on different frequencies of payment (daily, weekly, monthly, etc.). Similar calculations are done for bonuses (lab_ikb), other in-kind incomes (lab_iko, lab_ikf), and incomes from a second job (lab_sec, lab_sik, lab_sit, lab_sif). The total labor income is then aggregated into lab_tot.\nSurvey: What was [NAME]’s salary for this job (for the time period considered)?\ng        lab_pri= q4a_46a*365               if q4a_46b==1  \nreplace  lab_pri= q4a_46a*(365/7)*(1)       if q4a_46b==2\nreplace  lab_pri= q4a_46a*12                if q4a_46b==3\nreplace  lab_pri= q4a_46a                   if q4a_46b==4\nSurvey: In-kind bonus. A combien évaluez-vous les primes ( uniquement ceux qui ne sont pas inclus dans le salaire)?\ng        lab_ikb= q4a_48a*365               if q4a_48b==1  \nreplace  lab_ikb= q4a_48a*(365/7)*(1)       if q4a_48b==2\nreplace  lab_ikb= q4a_48a*12                if q4a_48b==3\nreplace  lab_ikb= q4a_48a                   if q4a_48b==4\nSurvey: Other inkind. A combien évaluez-vous ces avantages ( uniquement ceux qui ne sont pas inclus dans le salaire)?\ng        lab_iko= q4a_50a*365               if q4a_50b==1  \nreplace  lab_iko= q4a_50a*(365/7)*(1)       if q4a_50b==2\nreplace  lab_iko= q4a_50a*12                if q4a_50b==3\nreplace  lab_iko= q4a_50a                   if q4a_50b==4\nSurvey: Food from the job.\ng        lab_ikf= q4a_52a*365               if q4a_52b==1  \nreplace  lab_ikf= q4a_52a*(365/7)*(1)       if q4a_52b==2\nreplace  lab_ikf= q4a_52a*12                      if q4a_52b==3\nreplace  lab_ikf= q4a_52a                   if q4a_52b==4\nSurvey: Second activity labor income.\ng        lab_sec= q4a_62a*365               if q4a_62b==1  \nreplace  lab_sec= q4a_62a*(365/7)*(1)       if q4a_62b==2\nreplace  lab_sec= q4a_62a*12                if q4a_62b==3\nreplace  lab_sec= q4a_62a                   if q4a_62b==4   \nSurvey: In-kind second bonus.\ng        lab_sik= q4a_64a*365               if q4a_64b==1  \nreplace  lab_sik= q4a_64a*(365/7)*(1)       if q4a_64b==2\nreplace  lab_sik= q4a_64a*12                if q4a_64b==3\nreplace  lab_sik= q4a_64a                   if q4a_64b==4\nSurvey: Other in-kind, such as transport.\ng        lab_sit= q4a_66a*365               if q4a_66b==1  \nreplace  lab_sit= q4a_66a*(365/7)*(1)       if q4a_66b==2\nreplace  lab_sit= q4a_66a*12                if q4a_66b==3\nreplace  lab_sit= q4a_66a                   if q4a_66b==4\nSurvey: In-kind food.\ng        lab_sif= q4a_68a*365               if q4a_68b==1  \nreplace  lab_sif= q4a_68a*(365/7)*(1)       if q4a_68b==2\nreplace  lab_sif= q4a_68a*12                if q4a_68b==3\nreplace  lab_sif= q4a_68a                   if q4a_68b==4\nAll labor aggregated into lab_tot.\negen lab_tot=rsum(lab_pri lab_ikb lab_iko lab_ikf //\n                  lab_sec lab_sik lab_sit lab_sif)\n\n// Code turned off, presumably used with already processed surveys.\n*egen lab_tot=rsum(lab_pri  lab_sec)\n\n// All labor income expressed in thousands.\nreplace lab_tot=lab_tot*1000\n\n\n2.6.4 Treatment of Missing Data and Outliers:\nThe script examines missing labor income values among employed individuals and identifies outliers. Observations with labor incomes more than 5 standard deviations away from the mean are marked as outliers. There’s an annotation that explains that missings represent 10% of those employed. Mainly primary sector and household workers.\n//Outliers sd&gt;5\nsum lab_tot if employed==1 & lab_tot&gt;0\ngen d = lab_tot/r(sd)\ngen outlier = 1 if d&gt;5\nreplace outlier = 1 if employed==1 & lab_tot==0\nreplace outlier = 0 if outlier==.\n\n// Missing\ngen missings = 1     if employed==1 & lab_tot==0\nreplace missings = 0 if missings==.\n\n\n2.6.5 Merge with Demographic Characteristics:\nThe dataset is merged with household characteristics (HHcha), demographics (demo), and education (edu) datasets.\nmerge n:1 hhid using `HHcha' \nkeep if _merge==3\nmerge 1:1 hhid pid using `demo' , nogen\nmerge 1:1 hhid pid using `edu' , gen(edu)\n\n\n2.6.6 Sector Categorization:\nThe sector variable categorizes individuals into primary, secondary, or tertiary sectors based on their reported economic activity (q4a_230).\n***Sector\ngen sector=1 if q4a_230==\"A\" | q4a_230==\"B\" | q4a_230==\"C\" | q4a_230==\"0\" | //\n                q4a_230==\"1\" | q4a_230==\"2\" | q4a_230==\"3\" | q4a_230==\"4\" | //\n                q4a_230==\"5\" | q4a_230==\"6\" | q4a_230==\"7\" | q4a_230==\"8\" | //\n                q4a_230==\"9\" \n\nreplace sector=2 if  q4a_230==\"D\"| q4a_230==\"E\"| q4a_230==\"F\"\n\nreplace sector=3 if  q4a_230==\"G\" | q4a_230==\"H\" | q4a_230==\"I\" |   //\n                     q4a_230==\"J\" |q4a_230==\"K\"  | q4a_230==\"L\" |   //\n                     q4a_230==\"M\" | q4a_230==\"N\" | q4a_230==\"O\" |   //\n                     q4a_230==\"P\" | q4a_230==\"Q\" | q4a_230==\"R\" |   //\n                     q4a_230==\"S\" | q4a_230==\"T\" | q4a_230==\"U\"\nThere’s also imputation logic in place for missing sector information, where an employed individual without a specified sector is assigned the sector of the household head.\ngen aux = sector if head==1\nbysort hhid : egen sectorhh=max(aux)\nreplace sector=sectorhh if sector==. & employed==1\n\nreplace sector=2 if sector==. & employed==1 //to check \nreplace sector=. if lstatus==2 | lstatus==3\nlab var sector \"Economic activity sector\"\n\nlab def sector_lab 0 \"unemployed\" 1 \"primary (Agr)\" //\n                   2 \"secondary (Ind)\" 3 \"tertiary (Ser)\" \nlab val sector sector_lab\nreplace sector = . if lstatus==4 //No sector for OLF"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#model-for-predicting-labor-income",
    "href": "drafts/CCDR-poverty-code-explanation.html#model-for-predicting-labor-income",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.7 Model for Predicting Labor Income:",
    "text": "2.7 Model for Predicting Labor Income:\nThe code calculates squared values for education (educy2) and age (age2) and creates a binary variable for males (male). A natural logarithm of total labor income is then calculated (lnlab).\nclonevar industry = sector\n\ngen educy2   = educy * educy\ngen age2     = age*age\ngen male     = 1 if gender==1\nreplace male = 0 if gender==2\nSubsequently, a regression model (reg) predicts the logarithm of labor income (lnlab) based on various factors, but only for employed individuals who are neither outliers nor missing income values.\ngen lnlab    = ln(lab_tot)\nreg lnlab age gender i.educy age2 i.region i.q4a_24 i.sector [iw=wgt]  //\n          if employed==1 & outlier==0 & missings==0\nUsing this model, predicted income values (remp2 and temp2) are generated for employed outliers or those missing labor income data.\npredict remp2 if employed==1 & outlier==1 | missings==1 , xb\npredict temp2 if employed==1 & outlier==1 | missings==1 \nFinally, these predicted values are transformed to actual income scale (simuli) using the exponential function, and negative predictions are set to zero.\ngen simuli = .\nreplace simuli = exp(temp2)\nreplace simuli = 0 if simuli&lt;0\nIn summary, this section of the code:\n\nLoads and processes labor market data.\nCalculates labor force status and categorizes people based on their employment and income details.\nComputes and aggregates labor income for different sources.\nTreats missing data and outliers for labor income.\nMerges the processed labor data with demographic and education datasets.\nCategorizes individuals based on their sector of economic activity.\nPredicts labor income for those missing data using regression, and adjusts the predictions as needed."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#shares-and-total-income-to-the-model",
    "href": "drafts/CCDR-poverty-code-explanation.html#shares-and-total-income-to-the-model",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.8 Shares and total income to the model",
    "text": "2.8 Shares and total income to the model\nThis section is focused on computing shares and total income in preparation for a model.\nHere, for those individuals who are employed and are either considered outliers or have missing labor income data (lab_tot), their labor income is replaced with the simulated value (simuli) generated in the previous section.\nreplace lab_tot=simuli if  employed==1 & (outlier==1 | missings==1 )\nThis line calculates the total labor income at the household level, aggregating all individual incomes within each household.\nbysort hhid : egen lab_hh=sum(lab_tot)\nThe total income for each household is computed by summing the total labor income (lab_hh) and non-labor income (nli_hh).\negen    income_hh=rsum(lab_hh nli_hh)\nThen we calculate the share of labor and non-labor income out of the total income for each household.\ngen       s_lab    = lab_hh/income_hh\ngen       s_nli    = nli_hh/income_hh\nAnd compute the natural logarithms of total income (income_hh) and household consumption (yhh).\ngen     lny      =ln(income_hh)\ngen     lnc      =ln(yhh)\nThe marginal propensity to consume (mpc) is calculated as the ratio of household consumption (yhh) to total income (income_hh).\ng       mpc      = yhh/income_hh\nThe subsequent lines compute shares and derive other variables related to labor income and consumption:\n\nshare captures the individual’s share of labor income in the household’s total labor income, but only for those employed.\nylb gives the portion of household consumption funded by labor income, while ynl represents the portion of household consumption sourced from non-labor income.\nylbi represents the individual’s portion of household consumption funded by their labor income, again limited to the employed.\n\ngen share = lab_tot/lab_hh  if employed==1\n\n// HH Consumption from labor\ngen ylb = yhh*s_lab\nlab var ylb   //\n    \"Household consumption from labor income -nominal (Ariary/hh/year)\"\n\n// HH Consumption from non-labor\ngen ynl = yhh*(1-s_lab)\nlab var ynl  //\n    \"Household consumption from non-labor income - nominal (Ariary/hh/year)\"\n\ng ylbi=ylb*share if employed==1\nThe dataset is then pruned to keep only select variables, and saved to a temporary file.\nkeep hhid pid industry yhh ylb ynl ylbi salaried  \ntempfile labor\nsave `labor'"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#final-datasets",
    "href": "drafts/CCDR-poverty-code-explanation.html#final-datasets",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "2.9 Final datasets",
    "text": "2.9 Final datasets\nThe main household characteristics dataset (HHcha) is loaded again. It then gets merged sequentially with the demographic (demo), education (edu), and labor (labor) datasets.\nOnce all necessary merges are done, the generation variables (dem, edu, labo) are dropped. The resulting final dataset, which combines household characteristics, demographics, education, and labor variables, is then saved as MAG_assigned.dta.\nuse `HHcha' , clear\nmerge 1:n hhid using `demo' , gen(dem)\nkeep if dem==3\nmerge 1:1 hhid pid using `edu' , gen(edu)\nmerge 1:1 hhid pid using `labor' , gen(labo)\ndrop dem edu labo\nsave  \"$HHsurvey\\MAG_assigned.dta\", replace\nIn summary, this segment:\n\nAdjusts individual labor income values based on simulations for outliers and missing data.\nComputes various shares related to labor and non-labor incomes.\nCreates the logarithm of incomes and computes the marginal propensity to consume.\nDerives individual and household-level variables for consumption funded by labor and non-labor income.\nMerges several datasets to compile a comprehensive dataset containing household characteristics, demographics, education, and labor information.\nSaves the final dataset for subsequent analysis."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#import-population-data-to-be-used-in-the-creation-of-globals",
    "href": "drafts/CCDR-poverty-code-explanation.html#import-population-data-to-be-used-in-the-creation-of-globals",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "3.1 Import population data to be used in the creation of globals",
    "text": "3.1 Import population data to be used in the creation of globals\nWe first load a dataset specified by the global macro popdata, but only for the observations where the variable country matches the value specified by the global macro country. Here, we are aggregating the data by Variant and country using gcollapse (an enhanced version of Stata’s collapse command). The (sum) function sums up all the variables starting with yf and ym. And we reshape the dataset from wide to long format based on the yf and ym variables. The combined identifiers are country and Variant, while the variable year is created to represent the time dimension.\nuse \"$popdata\" if country==\"$country\", clear\nqui gcollapse (sum) yf* ym*, by(Variant country) \nqui reshape long yf ym, i(country Variant) j(year)\nHere, we generate a new variable named pop, which is the sum of the yf (likely female population) and ym (likely male population) variables, essentially getting the total population. The individual gender-based population variables (yf and ym) are then dropped, as we now have the combined pop variable. The variable country is also dropped from the dataset. The dataset is then saved into a temporary file named pop.\nqui g pop = yf+ym \nqui drop yf ym\ndrop country\ntempfile pop\nsave `pop'\nThe year variable is summarized to get its basic statistics. Two local macros are defined: minyear, which is assigned the value of the global macro surveyyear, and maxyear, which is assigned the maximum value of the year variable.\nqui sum year\nlocal minyear = $surveyyear\nlocal maxyear = r(max)\nA local macro v is initialized to 1. The subsequent loop then runs over several population projection variants. For each variant and for every year between minyear and maxyear, the total population (pop) for that year (pop_t) and the base year (pop_base) is calculated. The population growth rate is then computed by dividing pop_t by pop_base and saved into a global macro with a naming convention based on the year and variant. The counter v increments for each variant.\nlocal v = 1\nforeach variant in \"Medium\" \"Low\" \"High\" \"Constant-fertility\" //\n                   \"Instant-replacement\" \"Momentum\"  //\n                   \"Instant-replacement-zero-migration\" //\n                   \"Constant-mortality\" \"No-change\" {\n  forvalues t = `minyear'/`maxyear' {  \n        /* Population growth */\n        qui sum pop if year==`t' & Variant==\"`variant'\"\n        local pop_t = r(sum) \n        qui sum pop  if year==`minyear' & Variant==\"`variant'\"\n        local pop_base = r(sum) \n        global pop_growth_t`t'_v`v' = `pop_t'/`pop_base'\n    }\n    local v = `v'+1\n}\nLastly, the code imports an Excel file specified by the macro scenario_file from a sheet named elas_rep, clearing the existing data in memory. These are three cells with the elasticities for Agriculture, Manufacturing (Industry), and Services. We put these in a matrix E from the dataset, variable A (first column).\nimport excel \"${scenario_file}\", sheet(elas_rep) clear \nmkmat A  , mat(E)\nIn summary, this chunk of code:\n\nImports a dataset and reshapes it to long format based on population variables.\nAggregates the data to get total populations by year and variant.\nComputes population growth rates for different scenarios across years.\nImports another dataset from Excel related to some scenarios.\nTransforms part of the dataset into a matrix for further analysis."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#import-data-from-the-excel-file-received-from-macroeconomic-modelers",
    "href": "drafts/CCDR-poverty-code-explanation.html#import-data-from-the-excel-file-received-from-macroeconomic-modelers",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "3.2 Import data from the excel file received from macroeconomic modelers",
    "text": "3.2 Import data from the excel file received from macroeconomic modelers\nA local macro i is initialized to 0. A loop is then started, iterating over a global macro called scenarios, which presumably contains a list of sheet names from the Excel file to import. For each sheet name in scenarios, the code imports data from the Excel file specified by the scenario_file macro. The data is imported from the cell range A3:N63. This command renames the variables in the dataset using the variable names stored in the global macro $vars. The counter i is incremented by 1, and a new variable scenid is created in the dataset, which is given the current value of i. This acts as an identifier for each scenario. The dataset is saved into a temporary file named based on the current value of i (like scen1, scen2, etc.). The loop then moves on to the next sheet name in scenarios.\nforeach x of global scenarios {\n    qui import excel \"${scenario_file}\", sheet(\"`x'\") cellrange(A3:N63) clear\n    rename (*) ($vars)\n    local i = `i'+1\n    g scenid = `i'\n    tempfile scen`i'\n    qui save `scen`i''\n}\nThe dataset from the first scenario (scen1) is loaded into memory. A new local macro num is defined, which calculates the number of items in the scenarios global macro. Then, a loop starts from 2 up to the value of num. For each iteration, the dataset for the scenario corresponding to the current value of i is appended to the dataset in memory.\nuse `scen1', clear\nlocal num : list sizeof global(scenarios)\nforvalues i = 2/`num' {\n    append using `scen`i''\n}\nAny observations with a missing value for the year variable are dropped. A new variable rwage is created, which represents the real wage by dividing the wage variable by the cpi (Consumer Price Index) variable. This adjusts the wage for inflation.\ncap drop if year==.\nqui g rwage = wage/cpi\nThe variable Variant is initialized with empty strings. Then, a loop iterates over another global macro named variants. For each iteration, the value of Variant is replaced with the current item from variants where the scenid matches the current value of i. This assigns the appropriate variant name to each observation based on its scenid.\ng Variant = \"\"\nlocal i = 1\n    foreach variant of global variants {\n        replace Variant = \"`variant'\" if scenid==`i'\n        local i = `i' + 1\n    }\nThe dataset is then merged with the one saved in the temporary file pop based on the year and Variant variables. Observations that didn’t find a match in the pop dataset are dropped, and the _merge variable (created by the merge command) is also dropped. The dataset is sorted by scenid and then year.\nmerge m:1 year Variant using `pop'\ndrop if _merge==2\ndrop _merge\nsort scenid year\nThen a new variable consumption_pc is created, which calculates the per capita consumption. This is done by dividing the consumption variable by 100 times the pop variable.\ng consumption_pc = consumption/(pop*10e2)  \n}\nIn summary:\n\nWe import various scenarios from an Excel file and save each as a temporary Stata dataset.\nCombines all the scenarios into one dataset.\nProcesses the dataset by calculating real wages, assigning scenario names, merging with population data, and calculating per capita consumption values."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#creating-globals-by-yearscenario-to-be-used-later-in-the-microsimulation",
    "href": "drafts/CCDR-poverty-code-explanation.html#creating-globals-by-yearscenario-to-be-used-later-in-the-microsimulation",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "3.3 Creating globals by year/scenario to be used later in the microsimulation",
    "text": "3.3 Creating globals by year/scenario to be used later in the microsimulation\nThese lines determine the range of years in the dataset. The minimum year is set to a predefined global macro ($surveyyear), and the maximum year is the maximum year observed in the dataset.\nqui sum year\nlocal minyear = $surveyyear\nlocal maxyear = r(max)\nThis line identifies all the unique values of scenid in the dataset and stores them in a local macro called scenarios.\nlevelsof scenid, loc(scenarios)\nThe code then starts a loop, iterating over each unique scenario in the dataset. Inside the loop for each scenario, another loop starts which iterates over the years from the minimum to the maximum year. Within these nested loops, a series of commands calculate global growth rates and other indicators for various economic variables (GDP, consumption, employment, value added in different sectors, wages, etc.). The calculations are typically in the form:\nqui sum [variable] if year==`t' & scenid==`sc'\nlocal [variable]_t = r(sum)\nqui sum [variable] if year==`minyear' & scenid==`sc'\nlocal [variable]_base = r(sum)\nglobal [variable]_growth_t`t'_sc`sc' = `variable_t'/`variable_base'\nFor each combination of year and scenario, these commands:\n\nSum up the variable’s value for the current year (t) and scenario (sc).\nSave this sum in a local macro.\nSum up the variable’s value for the base year and the current scenario.\nSave this sum in another local macro.\nCalculate the growth rate by dividing the current year’s sum by the base year’s sum.\nSave the growth rate in a global macro.\n\nThese steps are repeated for several economic indicators. At the end of the nested loops, the Stata dataset will contain a series of global macros capturing the growth rates and other economic indicators for each year and scenario.\nforeach sc of numlist `scenarios' {  \n    forvalues t = `minyear'/`maxyear' {  \n        \n        /* GDP */\n        \n        qui sum gdp if year==`t' & scenid==`sc'\n        local gdp_t = r(sum) \n        qui sum gdp  if year==`minyear' & scenid==`sc'\n        local gdp_base = r(sum) \n        global gdp_growth_t`t'_sc`sc' = `gdp_t'/`gdp_base'\n        \n        /* Consumption */\n        \n        qui sum consumption if year==`t' & scenid==`sc'\n        local consumption_t = r(sum) \n        qui sum consumption  if year==`minyear' & scenid==`sc'\n        local consumption_base = r(sum) \n        global c_growth_t`t'_sc`sc' = `consumption_t'/`consumption_base' \n        \n        /* Consumption per capita */\n\n        qui sum consumption_pc if year==`t' & scenid==`sc'\n        local consumption_t = r(sum) \n        qui sum consumption_pc   if year==`minyear' & scenid==`sc'\n        local consumption_base = r(sum) \n        global pcc_growth_t`t'_sc`sc' = `consumption_t'/`consumption_base'\n\n        /* Remittances */\n        \n        qui sum remittances if year==`t' & scenid==`sc'\n        local remittances_t = r(sum) \n        qui sum remittances  if year==`minyear' & scenid==`sc'\n        local remittances_base = r(sum) \n        global remitt_growth_t`t'_sc`sc' = `remittances_t'/`remittances_base'       \n        \n        /* Employment */\n        \n        qui sum employment if year==`t' & scenid==`sc'\n        local employment_t = r(sum) \n        qui sum employment   if year==`minyear' & scenid==`sc'\n        local employment_base = r(sum) \n        global emp_growth_t`t'_sc`sc' = `employment_t'/`employment_base'        \n        \n        /* Employment rate */\n        \n        qui sum employment    if year==`t' & scenid==`sc'\n        local employment_t_n = r(sum) \n        qui sum wa_population if year==`t' & scenid==`sc'\n        local employment_t_d = r(sum)   \n        global emp_rate_t`t'_sc`sc' = `employment_t_n'/`employment_t_d' \n        \n        /* Working age population */\n        \n        qui sum wa_population if year==`t' & scenid==`sc'\n        local employment_t = r(sum) \n        qui sum wa_population    if year==`minyear' & scenid==`sc'\n        local employment_base = r(sum) \n        global wapop_growth_t`t'_sc`sc' = `employment_t'/`employment_base'\n        \n        /* Value added */\n        \n        qui sum agriculture_va if year==`t' & scenid==`sc'\n        local N_i = r(sum) \n        qui sum agriculture_va if year==`minyear' & scenid==`sc'\n        local N_base = r(sum) \n        global growth_agriculture_t`t'_sc`sc' = `N_i'/`N_base'\n        \n        qui sum industry_va if year==`t' & scenid==`sc'\n        local N_i = r(sum) \n        qui sum industry_va if year==`minyear' & scenid==`sc'\n        local N_base = r(sum) \n        global growth_industry_t`t'_sc`sc' = `N_i'/`N_base'\n\n        qui sum services_va if year==`t' & scenid==`sc'\n        local N_i = r(sum) \n        qui sum services_va if year==`minyear' & scenid==`sc'\n        local N_base = r(sum) \n        global growth_services_t`t'_sc`sc' = `N_i'/`N_base'\n            \n        /* Nominal Wages */\n        \n        qui sum wage        if year==`t' & scenid==`sc'\n        local N_i = r(sum) \n        qui sum wage        if year==`minyear' & scenid==`sc'\n        local N_base = r(sum) \n        global growth_wage_t`t'_sc`sc' = `N_i'/`N_base'\n        \n        /* Real Wages */\n        \n        qui sum rwage       if year==`t' & scenid==`sc'\n        local N_i = r(sum) \n        qui sum rwage       if year==`minyear' & scenid==`sc'\n        local N_base = r(sum) \n        global growth_rwage_t`t'_sc`sc' = `N_i'/`N_base'\n        \n    }\n}\n}\nAfter finishing the calculations for all scenarios and years, the code then lists all global macros, giving you an overview of the generated variables. The block of Stata commands then ends.\nIn summary, this code processes the dataset to generate a comprehensive set of global macros containing values for various economic indicators, broken down by year and scenario. These global macros are then used in subsequent analyses or microsimulations.\nglobal c:all globals\nmacro list c"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#define-key-variables-and-globalsrates",
    "href": "drafts/CCDR-poverty-code-explanation.html#define-key-variables-and-globalsrates",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "4.1 Define key variables and globals/rates",
    "text": "4.1 Define key variables and globals/rates\nThis command recodes the educy variable, which represents years of education. It bins the years into three categories: “0-3”, “3-7”, and “8+”. The newly created categorical variable is named calif.\nrecode educy (0/3 = 1 \"0-3\") (3/7 = 2 \"3-7\") (7/11 = 3 \"8+\"), g(calif)\nA matrix industry_growth is created, which is based on a combination of previously defined global variables and the matrix E, which you can recall we created before with the elasticities. The industry_growth matrix represents growth in different industry sectors.\nmat industry_growth = /// \n    1+(${growth_agriculture_t`t'_sc`i'}-1)*E[1,1] \\ ///\n    1+(   ${growth_industry_t`t'_sc`i'}-1)*E[2,1] \\ ///\n    1+(   ${growth_services_t`t'_sc`i'}-1)*E[3,1]\nmatlist industry_growth\nA global variable employment is created based on employment rates for a particular year and scenario. This variable captures the growth in employment rate from the survey year to the target year.\nglobal employment = (${emp_rate_t`t'_sc`i'})  / (${emp_rate_t${surveyyear}_sc`i'})"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#main-reweighting-command",
    "href": "drafts/CCDR-poverty-code-explanation.html#main-reweighting-command",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "4.2 Main reweighting command",
    "text": "4.2 Main reweighting command\nThis is the most important part of the code. The reweighting of the dataset.\nThe command mfmodms_reweight appears to be a custom command (possibly from a user-written package or an in-house module) that performs the reweighting based on several constraints. The variables and parameters to be considered in the reweighting process are specified.\nmfmodms_reweight, age(age) edu(calif) gender(gender) hhsize(hhsize)       // \n                  id(hhid) iw(wgt) country(\"$country\") iyear($surveyyear) //\n                  tyear(`t') generate(wgtsim) industry(industry)          //\n                  growth(industry_growth) match(HH) popdata(\"$popdata\")   //\n                  employment($employment) variant(`variant')"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#wage-bill-at-baseline",
    "href": "drafts/CCDR-poverty-code-explanation.html#wage-bill-at-baseline",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "4.3 Wage bill at baseline",
    "text": "4.3 Wage bill at baseline\nThe command quietly calculates the weighted sum of the yflab variable, which represents the family labor income. The result is stored as a scalar named Y0.\nqui sum yflab [aw=wgt]\nscalar Y0 = r(mean)"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#wage-bill-at-simulated-yearscenario",
    "href": "drafts/CCDR-poverty-code-explanation.html#wage-bill-at-simulated-yearscenario",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "4.4 Wage bill at simulated year/scenario",
    "text": "4.4 Wage bill at simulated year/scenario\nThis portion of the code takes the user through calculating the simulated wage bill for different scenarios and years and then modifies the distribution of labor income accordingly.\nThe comments suggest adjusting the yflab (probably representing labor income or wage) in such a way that it grows consistently with Value Added (VA), ensuring the share of the wage bill on VA remains constant. This is done for three sectors: agriculture, industry, and services.\nHere, the wage bill for the simulated year and scenario is computed and stored in a scalar Y1.\n/* \"wage\" by sector grow in a way consistent with VA, \n    such that the share of the wage bill on VA is constant */\nqui replace yflab = yflab*(${growth_agriculture_t`t'_sc`i'} /    //\n                    industry_growth[1,1]) if industry==1\nqui replace yflab = yflab*(${growth_industry_t`t'_sc`i'}    /    //\n                    industry_growth[2,1]) if industry==2\nqui replace yflab = yflab*(${growth_services_t`t'_sc`i'}    /    //\n                    industry_growth[3,1]) if industry==3\n\n// Labor income (wage bill) at simulated year and scenario      \n    qui sum yflab [aw=wgtsim`t']\n    scalar Y1 = r(mean)"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#re-center-and-simulated-welfare-aggregate",
    "href": "drafts/CCDR-poverty-code-explanation.html#re-center-and-simulated-welfare-aggregate",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "4.5 Re-center and simulated welfare aggregate",
    "text": "4.5 Re-center and simulated welfare aggregate\nThis section aims to adjust the labor income, aggregate it to the household level, then adjust it further with non-labor income and consumption projections. The comment suggests re-centering the labor income distribution, ensuring that only the distribution of labor income is modified. Labor income is aggregated at the household level, producing a new variable named ysim_t’_sci', which seems to represent simulated labor income for a specific year (t) and scenario (i). The simulated labor income is adjusted to become per capita values. To the simulated labor income per capita, non-labor income (denoted by pcnli) is added.\n/* Re-center, so we only modify the distribution of \"labor income\" */   \nqui replace yflab = yflab * (Y0/Y1) \n    \n/* Aggregate to household again */\nbys hhid: egen ysim_`t'_sc`i' = sum(yflab) \n\n/* Divide by hhsize to make per capita again */\nqui replace ysim_`t'_sc`i'=(ysim_`t'_sc`i')/hhsize  \n    \n/* Adding back \"non-labor\" income */\nqui replace ysim_`t'_sc`i'=ysim_`t'_sc`i' + pcnli"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#adjusting-the-consumption-according-to-the-projection",
    "href": "drafts/CCDR-poverty-code-explanation.html#adjusting-the-consumption-according-to-the-projection",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "4.6 Adjusting the consumption according to the projection",
    "text": "4.6 Adjusting the consumption according to the projection\nThis portion adjusts the simulated labor income based on the growth in per capita consumption from projections. The conditional statement checks if it’s the base year (2021) and the first scenario. If it is, the dataset is saved anew. If not, the dataset is appended with new simulated values for the other years and scenarios.\nqui sum pcc [aw=wgt]\nscalar pccY0 = r(mean)\nqui sum ysim_`t'_sc`i' [aw=wgtsim`t']\nscalar pccrwY1 = r(mean)\n    \nqui replace ysim_`t'_sc`i' = ysim_`t'_sc`i' *   //\n                             (pccY0*${pcc_growth_t`t'_sc`i'}) / pccrwY1\nlabel var ysim_`t'_sc`i'  //\n   \"Simulated pcc - After reweighting and hh consumption growth, Year: `t'\"\n\nren wgtsim`t' wgtsim`t'_sc`i' \n\nif (`i'==1 & `t'==2021) save \"$path/data/MAG_simulated.dta\",replace \nelse {\n    qui merge 1:1 hhid pid using  //\n              \"$path/data/MAG_simulated.dta\", keepusing(ysim_*_sc* wgtsim*_sc* )\n    drop _merge\n    qui save \"$path/data/MAG_simulated.dta\", replace \n} // This closes the loop.\nAfter processing each scenario and year, a message is displayed in red, indicating the completion of the scenario-year pair.\ndis in red \"DONE WITH SCENARIO `i' AND YEAR `t'\"\nThese closing brackets represent the end of loops and conditions. The code runs through various scenarios and years, updating the variable i (scenario counter) for each iteration.\n} /* Conditional to skip scenarios for 2021 */\n} /* Loop over years */\nlocal i = `i'+1\n} /* Loop over scenarios */\nFinally, the timer that was started at the beginning of the code is turned off, and the time taken is displayed.\ntimer off 1\ntimer list 1\nIn summary: Overall, this code simulates labor income under different scenarios and years, adjusts it based on various factors, and then saves or appends the results to the file named MAG_simulated.dta."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#poverty-statistics-by-regions-urbanrural-and-gender",
    "href": "drafts/CCDR-poverty-code-explanation.html#poverty-statistics-by-regions-urbanrural-and-gender",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "5.1 Poverty statistics by regions, urban/rural, and gender",
    "text": "5.1 Poverty statistics by regions, urban/rural, and gender\nThis code is primarily involved in generating poverty and inequality indicators for different scenarios and years. Load the simulated data created before for further processing. Merge the data with another dataset that likely contains poverty lines or thresholds (based on variable names).\nuse \"$path/data/MAG_simulated\",clear    \nmerge 1:1 hhid pid using \"$path/data/MAG_assigned\", keepusing(pline* lbpl ubpl)\nDefine global macros for scenarios and years. Note that we only analyze every ten years.\nglobal scenssim 1 2 3 4 5 6\nglobal nscsteps 2030(10)2050\nCreate a binary indicator for urban areas based on binary “rural”. If not present.\n*gen urban = rural!=1\nKeep only the specified variables in the dataset.\nkeep hhid pid wgt wgtsim* ysim* pcc region gender zone pline* lbpl ubpl\nDefine different poverty thresholds.\ngen pov19 = 1.9\ngen pov32 = 3.2\ngen pov55 = 5.5\ngen pov10 = 10\nCreate a variable called ‘country’ with value 1 for all observations. Also, define a new variable Ysim_baseyear equal to the pcc variable, which seems to represent per capita consumption.\ng country=1\ng Ysim_baseyear = pcc\nThe next set of commands uses the sp_groupfunction command multiple times. This command seems to be a user-defined or package-specific function that calculates some group-specific statistics, likely poverty rates, and Gini coefficients. For example, This block calculates statistics for the base year at the country level:\npreserve\n    sp_groupfunction [aw=wgt], poverty(Ysim_baseyear)  //\n    povertyline(pline215 pline365 pline685 lbpl ubpl)  //\n                gini(Ysim_baseyear) mean(Ysim_baseyear) by(country) \n    g year=$surveyyear\n    tempfile resultsbase\n    qui save `resultsbase'\nrestore \nSimilar blocks are provided for calculating statistics at different levels: region, urban vs. rural (zone), and gender.\npreserve\n    sp_groupfunction [aw=wgt], poverty(Ysim_baseyear) //\n    povertyline(pline215 pline365 pline685 lbpl ubpl) //\n                mean(Ysim_baseyear) by(country region) \n    g year=$surveyyear\n    tempfile resultsbase_reg\n    qui save `resultsbase_reg'\nrestore \n\npreserve\n    sp_groupfunction [aw=wgt], poverty(Ysim_baseyear)  //\n    povertyline(pline215 pline365 pline685 lbpl ubpl)  //\n                mean(Ysim_baseyear) by(country zone) \n    g year=$surveyyear\n    tempfile resultsbase_urb\n    qui save `resultsbase_urb'\nrestore \n\npreserve\n    sp_groupfunction [aw=wgt], poverty(Ysim_baseyear)  //\n    povertyline(pline215 pline365 pline685 lbpl ubpl)  //\n                mean(Ysim_baseyear) by(country gender) \n    g year=$surveyyear\n    tempfile resultsbase_gender\n    qui save `resultsbase_gender'\nrestore \nAnd then we drop the previously created variable Ysim_baseyear.\ndrop Ysim_baseyear \nThe following loop iterates over scenarios and years, calculating the same set of statistics for each combination. For each scenario-year combination, the results are saved in a temporary file.\nforeach sc in $scenssim {  \n    forvalues i=$nscsteps { \n      preserve\n      noisily sp_groupfunction [aw=wgtsim`i'_sc`sc'], poverty(ysim_`i'_sc`sc')..\n      g year=`i'\n      tempfile results`i'_`sc'\n      qui save `results`i'_`sc''\n      restore\n    }\n}\nThe following nested loops iterating over various scenarios ($scenssim) and years ($nscsteps) do the same. Within each loop, a function named sp_groupfunction is applied to the data, which likely computes poverty-related statistics. The results are then saved to separate temporary files for different combinations of scenarios, years, and groupings (region, urban/rural zone, and gender). The results are saved in temporary files named with the pattern results[year]_[grouping]_[scenario]. In summary, the code generates poverty statistics based on simulated data, breaking down the results by different groupings (regions, urban/rural zones, and gender) for various scenarios and years.\nThen the code focuses on aggregating results from various files, preparing data for output, and then performing additional calculations on income percentiles. Here’s the step-by-step breakdown:\nThe script first loads the results from the base file (resultsbase) and then appends (or stacks) results from the different scenarios, regions, zones (urban/rural), and gender groupings created before.\nuse `resultsbase',clear\nappend using `resultsbase_reg'\nappend using `resultsbase_urb'\nappend using `resultsbase_gender'\nforeach sc in $scenssim {\n    forvalues i=$nscsteps {\n        append using `results`i'_`sc''\n        append using `results`i'_reg_`sc''\n        append using `results`i'_urb_`sc''\n        append using `results`i'_gender_`sc''\n    }\n}\nWe then create a Unique Identifier (concat) and Reorder.\ngen concat = measure +\"_\"+ variable+\"_\" +reference+\"_\"+string(region)+ //\n                      \"_\"+string(zone)+\"_\"+string(gender)\norder concat, first\ngen regionref = region\nThe aggregated data is then exported to an Excel file and then saved in a Stata format.\nexport excel using \"$scenario_file\", sheet(\"Indicators\",modify) firstrow(varlabels)\ndrop concat \nsave \"$path/outputs\\MAG_indicators\",replace\nThe script switches to another dataset (MAG_simulated) and prepares to compute income percentiles.\nuse \"$path\\data\\MAG_simulated\",clear\nmerge 1:1 hhid pid using \"$path/data/MAG_assigned\", keepusing(pline* lbpl ubpl)\nkeep hhid pid wgt wgtsim* ysim* pcc pline* lbpl ubpl\ng Ysim_baseyear = pcc\ng country = 1\ndrop pcc\nIt first calculates the percentiles for the base year income.\ngquantiles Ysim_baseyear [aw=wgt], _pctile nq(100)\nmat quantiles = r(quantiles_used)\nmat colnames quantiles = Ysim_baseyear\nIn essence, this code is primarily focused on organizing and summarizing the simulated results from different scenarios, then performing statistical analyses on the income distribution by quantiles.\nforeach sc in $scenssim {\n    forval simyear = $nscsteps {    \n        gquantiles ysim_`simyear'_sc`sc' [aw=wgtsim`simyear'_sc`sc'], _pctile nq(100)\n        mat mat0 = r(quantiles_used)\n        mat colnames mat0 = ysim_`simyear'_sc`sc'\n        mat quantiles = quantiles,mat0\n    }\n}\nWe export income by percentiles to our Excel file, modifying it (not replace). First, the data is cleared out of the current memory. The quantiles matrix is converted to a dataset format using svmat. Each column from the matrix becomes a variable in the dataset. A new variable, percentil, is created to indicate the row number (or the percentile). The dataset is exported to an Excel file under the sheet “Income_by_percentile”.\nclear\nsvmat quantiles, names(col)\ng percentil = _n\nexport excel using \"$scenario_file\", sheet(\"Income_by_percentile\",modify) //\n                   firstrow(varlabels)"
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#compute-deviations",
    "href": "drafts/CCDR-poverty-code-explanation.html#compute-deviations",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "5.2 Compute deviations",
    "text": "5.2 Compute deviations\nThe dataset MAG_indicators is loaded into memory. Several variables are modified to replace missing or specific values:\nuse \"$path/outputs\\MAG_indicators\", clear\ndrop country\nreplace region = 99 if region==.\nreplace zone = 99 if zone==.\nreplace gender = 99 if gender==.\nreplace reference = \"NA\" if reference==\"\"\nreplace _population=_population*10e-7\npreserve\nThe dataset is then subsetted to keep observations where measure is “fgt0” (Foster-Greer-Thorbecke poverty measure with parameter 0). Some transformations are applied to the value variable based on the measure variable. A temporary file (number_poor) is saved. The original data is then restored and appended with this modified subset.\nkeep if inlist(measure,\"fgt0\")\nreplace value=value*_population\nreplace measure=\"np_fgt0\" if measure==\"fgt0\"\ntempfile number_poor \nsave `number_poor'\nrestore\nappend using `number_poor'\n        ```\n\nFurther transformations are performed:\n\n```stata\nreplace value=100*value if inlist(measure,\"fgt0\",\"fgt1\",\"fgt2\",\"gini\")\nreplace variable = \"ysim_base_sc0\" if variable==\"Ysim_baseyear\"\nThe code then proceeds to extract scenid (scenario ID) and creates labels for it. It also creates a variable, Ysim, by extracting a part of the variable column. Some cleanup operations are also performed.\ng scenid = real(substr(variable,13,2))\nlabel define scenid 0 \"base year\" $scenid ,replace\nlabel values scenid scenid\ng Ysim = substr(variable,1,ustrpos(variable,\"_sc\")-5)\nreplace Ysim = \"base\" if year==$surveyyear\ndrop variable \nThe final version of the dataset is exported to an Excel file under the sheet “Results_long”.\nexport excel using \"$scenario_file\", sheet(\"Results_long\",modify) firstrow(varlabels)\nOverall, this code focuses on data manipulation tasks, including reshaping the data, generating new variables, making adjustments to existing variables, and exporting the processed data to Excel for further use."
  },
  {
    "objectID": "drafts/CCDR-poverty-code-explanation.html#generating-deviations-from-baseline",
    "href": "drafts/CCDR-poverty-code-explanation.html#generating-deviations-from-baseline",
    "title": "A guide to CCDR Poverty Analysis code",
    "section": "5.3 Generating Deviations from Baseline:",
    "text": "5.3 Generating Deviations from Baseline:\nRemove observations where the variable Ysim is equal to “base”. Keep only those observations where the gender is coded as 99. Drop the variable _population as it is not needed. Reshape the dataset from long format to wide format using the value variable, with new columns created for each scenid. This will generate variables like value1, value2, etc., for each scenario.\ndrop if Ysim==\"base\"\nkeep if gender==99\ndrop _population\nreshape wide value, i(measure reference region zone year) j(scenid)\nFor each scenario, calculate the deviation as a percentage from the baseline (value1). This deviation indicates how much a value in a given scenario has changed relative to the baseline scenario. Similarly, compute the deviation in levels (absolute difference from the baseline).\nlocal num : list sizeof global(scenarios)\nforvalues i=2(1)`num' {\n  g des_percent_`i' = 100*(value`i'-value1)/value1\n}\nforvalues i=2(1)`num' {\n  g des_level_`i' = value`i'-value1\n}\nDrop all the value* columns. Reshape the dataset back to long format based on the deviation columns. Rename the columns for better clarity.\ndrop value*\nreshape long des_percent_ des_level_, i(Ysim measure reference region zone year) j(scenid)\nrename des_*_ des_*\nCreate a new variable, concat, that combines several variables into a single string. This is for uniquely identifying or categorizing each observation. Reorder the dataset so that concat appears first.\ngen concat = measure +\"_\"+ Ysim+string(year)+\"_\" //\n             +string(scenid)+\"_\"+reference+\"_\"+  //\n             string(region)+\"_\"+string(zone)+\"_\"+ //\n             string(gender) \norder concat, first\nFinally, export the current dataset to an Excel file under the sheet “Deviations_long”.\nexport excel using \"$scenario_file\", sheet(\"Deviations_long\",modify) //\n                   firstrow(varlabels)\nThis code is mainly concerned with deriving deviations from a baseline across different scenarios and organizing the resulting dataset for further analysis and presentation. The computed deviations provide insights into how different scenarios compare to a reference or baseline scenario in the given dataset."
  },
  {
    "objectID": "bg-notes/hh-assets-and-fuelwood-use.html",
    "href": "bg-notes/hh-assets-and-fuelwood-use.html",
    "title": "Household Assets and Fuelwood Use",
    "section": "",
    "text": "Homes are perhaps the most important asset owned by households. In 2022, most of the 800,604 Armenian households owned their dwelling (about 89.4%), as shown in Figure 1, with only about 5.9% of homes renting and 4.7% with other forms of tenure (ARMSTAT, 2023). Also, about 97% of households lived in houses or apartments, as opposed to hostel; railcar / container; other temporary lodging; or “other” (3% combined). Houses averaged 112.0 m2, while apartments averaged 68.3 m2. As expected, 96.2% of apartments were located in urban areas, with about half of them in Yerevan (55.3%). Houses, on the other hand, were located mostly in rural areas (65.8%), with about 13.4% of them located in Yerevan.\nFigure 1. Dwelling ownership by Marz\n\nSource: Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).\nThe rental market is small and an urban phenomenon, with 92% of rentals occurring in that area. The average rent for a house was AMD 56,613.6 (about USD 143.1) with an average price of AMD 882.4 (USD 2.23) per square meter. Conversely, apartments were rented at a more expensive mean of AMD 82,124.2 (USD 207.6) with an average price of AMD 1,447.8 (USD 3.7) per square meter.\nOwned dwellings are an asset from which households derive welfare. Non renters derived an average of AMD 54,338.1 (USD 137.33) in monthly imputed rent. The emergent rental market information was used to impute rent to non-renters using a log linear modeling approach described by Ceriani et al. (2019), in which imputed rent was predicted using a combination of household characteristics (urban/rural, Marz, number of rooms, presence of an indoor toilet, number of household, square meters, type of dwelling, members) and head of household characteristics (sex, highest completed schooling level, age group). These values are shown in Table 1 by decile and for the whole country. Net present value of that monthly imputed rent (for a 2050 horizon) was also estimated at AMD 17.3 million (USD 43,881.5), using a 5% annual discount rate and a 5% average inflation rate.\nTable 1. Imputed rent and average net present value (2050 horizon) for non-renters\n\n\n\n\n\n\n\n\n\nDecile\nAverage dwelling area (m2)\nAverage imputed rent (Dram per month)\nAverage net present value of rent (2050 horizon)\n\n\n\n\n1\n89.4\n45,632.3\n14,565,638.7\n\n\n2\n91.1\n51,149.9\n16,326,813.0\n\n\n3\n91.1\n52,635.7\n16,801,095.0\n\n\n4\n89.5\n55,010.5\n17,559,108.4\n\n\n5\n88.3\n53,896.9\n17,203,666.3\n\n\n6\n87.2\n54,230.5\n17,310,143.3\n\n\n7\n93.0\n54,532.6\n17,406,560.2\n\n\n8\n89.1\n52,558.3\n16,776,366.9\n\n\n9\n86.7\n56,791.0\n18,127,439.9\n\n\n10\n87.5\n59,735.1\n19,067,188.6\n\n\nCountry\n89.0\n54,338.1\n17,344,482.6\n\n\n\nSource: author based on log linear imputed rent approach (Ceriani et al., 2019)\nA little over a third (288,718 or 36.1%) of households owned a car in 2022 and used it in the month prior to the survey. However, a higher percentage of homes in rural areas (47.5%) own a vehicle. Given the characteristics of rural areas and the availability of public transportation, it is a particularly important asset for household mobility. This also means that these households are exposed to energy transition risks derived from changes to prices of fuels and technological changes in car technologies (TNFD, 2023).\nArmenians access water mainly through centralized water supply (95.6% or 765,728 households), 2.7% of households have their own system of water supply, and the remaining 1.6% access water through spring water or well; delivered water; bought water; or “other”. Urban households spend an average of about AMD 2,506.5 (USD 6.3) on water, while rural households spend about AMD 1,790.1 (USD 4.5).\nMost homes (99.8% or 798,835 households) access electricity through the national grid, with only a small share 0.2% using solar panels. Both urban and rural homes spend about the same average expenditure on electricity of about AMD 7,948.3 (USD 20.1).",
    "crumbs": [
      "Home",
      "Background notes",
      "Household Assets and Fuelwood Use"
    ]
  },
  {
    "objectID": "bg-notes/hh-assets-and-fuelwood-use.html#household-assets",
    "href": "bg-notes/hh-assets-and-fuelwood-use.html#household-assets",
    "title": "Household Assets and Fuelwood Use",
    "section": "",
    "text": "Homes are perhaps the most important asset owned by households. In 2022, most of the 800,604 Armenian households owned their dwelling (about 89.4%), as shown in Figure 1, with only about 5.9% of homes renting and 4.7% with other forms of tenure (ARMSTAT, 2023). Also, about 97% of households lived in houses or apartments, as opposed to hostel; railcar / container; other temporary lodging; or “other” (3% combined). Houses averaged 112.0 m2, while apartments averaged 68.3 m2. As expected, 96.2% of apartments were located in urban areas, with about half of them in Yerevan (55.3%). Houses, on the other hand, were located mostly in rural areas (65.8%), with about 13.4% of them located in Yerevan.\nFigure 1. Dwelling ownership by Marz\n\nSource: Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).\nThe rental market is small and an urban phenomenon, with 92% of rentals occurring in that area. The average rent for a house was AMD 56,613.6 (about USD 143.1) with an average price of AMD 882.4 (USD 2.23) per square meter. Conversely, apartments were rented at a more expensive mean of AMD 82,124.2 (USD 207.6) with an average price of AMD 1,447.8 (USD 3.7) per square meter.\nOwned dwellings are an asset from which households derive welfare. Non renters derived an average of AMD 54,338.1 (USD 137.33) in monthly imputed rent. The emergent rental market information was used to impute rent to non-renters using a log linear modeling approach described by Ceriani et al. (2019), in which imputed rent was predicted using a combination of household characteristics (urban/rural, Marz, number of rooms, presence of an indoor toilet, number of household, square meters, type of dwelling, members) and head of household characteristics (sex, highest completed schooling level, age group). These values are shown in Table 1 by decile and for the whole country. Net present value of that monthly imputed rent (for a 2050 horizon) was also estimated at AMD 17.3 million (USD 43,881.5), using a 5% annual discount rate and a 5% average inflation rate.\nTable 1. Imputed rent and average net present value (2050 horizon) for non-renters\n\n\n\n\n\n\n\n\n\nDecile\nAverage dwelling area (m2)\nAverage imputed rent (Dram per month)\nAverage net present value of rent (2050 horizon)\n\n\n\n\n1\n89.4\n45,632.3\n14,565,638.7\n\n\n2\n91.1\n51,149.9\n16,326,813.0\n\n\n3\n91.1\n52,635.7\n16,801,095.0\n\n\n4\n89.5\n55,010.5\n17,559,108.4\n\n\n5\n88.3\n53,896.9\n17,203,666.3\n\n\n6\n87.2\n54,230.5\n17,310,143.3\n\n\n7\n93.0\n54,532.6\n17,406,560.2\n\n\n8\n89.1\n52,558.3\n16,776,366.9\n\n\n9\n86.7\n56,791.0\n18,127,439.9\n\n\n10\n87.5\n59,735.1\n19,067,188.6\n\n\nCountry\n89.0\n54,338.1\n17,344,482.6\n\n\n\nSource: author based on log linear imputed rent approach (Ceriani et al., 2019)\nA little over a third (288,718 or 36.1%) of households owned a car in 2022 and used it in the month prior to the survey. However, a higher percentage of homes in rural areas (47.5%) own a vehicle. Given the characteristics of rural areas and the availability of public transportation, it is a particularly important asset for household mobility. This also means that these households are exposed to energy transition risks derived from changes to prices of fuels and technological changes in car technologies (TNFD, 2023).\nArmenians access water mainly through centralized water supply (95.6% or 765,728 households), 2.7% of households have their own system of water supply, and the remaining 1.6% access water through spring water or well; delivered water; bought water; or “other”. Urban households spend an average of about AMD 2,506.5 (USD 6.3) on water, while rural households spend about AMD 1,790.1 (USD 4.5).\nMost homes (99.8% or 798,835 households) access electricity through the national grid, with only a small share 0.2% using solar panels. Both urban and rural homes spend about the same average expenditure on electricity of about AMD 7,948.3 (USD 20.1).",
    "crumbs": [
      "Home",
      "Background notes",
      "Household Assets and Fuelwood Use"
    ]
  },
  {
    "objectID": "bg-notes/hh-assets-and-fuelwood-use.html#household-exposure-to-the-agricultural-sector",
    "href": "bg-notes/hh-assets-and-fuelwood-use.html#household-exposure-to-the-agricultural-sector",
    "title": "Household Assets and Fuelwood Use",
    "section": "Household exposure to the agricultural sector",
    "text": "Household exposure to the agricultural sector\nAbout a fourth of all Armenian households (23.4%) had a monthly agricultural income component; a number that rises to 61% when discussing rural households, with that income representing an average 22.6% of total income. Overall, 91.3% of those 187,176 households that derived an agricultural income were located in rural areas. This income averaged AMD 72,275.2 (USD 182.67) and represented an average of 23.7% of total income for rural homes. These same figures averaged AMD 47,853.1 (USD 120.94) and 12.0% in urban areas, respectively. When it comes to deciles, perhaps unintuitively, the average share of total income that comes from agriculture rises from 20.7% for the first decile to 26.3% for the tenth, as shown in Table 2.\nTable 2. Average agricultural income\n\n\n\n\n\n\n\n\n\nDecile\nAverage total\nincome\nAverage agricultural\nIncome\nAverage agricultural income\nshare of total income\n\n\n\n\n1\n272,739.6\n54,013.3\n20.7%\n\n\n2\n277,204.7\n47,470.2\n19.3%\n\n\n3\n308,608.9\n75,184.5\n22.0%\n\n\n4\n280,508.0\n51,349.7\n18.3%\n\n\n5\n306,497.2\n68,414.4\n24.0%\n\n\n6\n301,499.4\n69,378.7\n22.8%\n\n\n7\n315,422.2\n72,267.4\n21.5%\n\n\n8\n337,909.6\n78,487.8\n25.5%\n\n\n9\n295,644.2\n78,357.8\n23.5%\n\n\n10\n315,725.2\n92,134.7\n26.3%\n\n\nCountry\n303,234.5\n70,156.4\n22.6%\n\n\n\nSource: author based on Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).\nNote: column three is the average of the share calculated at the household level and weighted by population weights, not column two divided by column one.\nWhile only 23.4% of households, derive income from the agricultural sector, exposure to agriculture is larger, since 41% of households use land for agricultural purposes (owned and/or rented). A total of 98% out of the 328,438 households that use agricultural land own their plots, and 5.3% of those (also) rented. The average area used by households for agricultural purposes is 7,329.9 m2, of which an average 70.6%, or 5,551.2 m2, is used for crops. This suggests that own consumption of agricultural output plays a role in Armenian incomes. Table 3 shows that for households without agricultural sales, 4.3% of income can be attributed to imputed use of agricultural products for own consumption. More generally, this table shows the shares of household income from different sources, for households with agricultural land, with or without deriving income from that land, compared with households without agricultural land. It is evident that public pensions and benefits plays a much bigger role (29.7%) for all households, along with hired employment (41.3%).\nTable 3. Average shares of sources of income for households with and without agricultural land\n\n\n\n\n\n\n\n\n\n\n\nHouseholds with agricultural land (owned or not) with agricultural income\nHouseholds with agricultural land (owned or not) with no agricultural income\nHouseholds without agricultural land (owned or not) with or without agricultural income\nAll households\n\n\n\n\nNumber of households\n184,738\n143,700\n472,166\n800,604\n\n\nAverage total income (Dram)\n302,290.3\n223,874.8\n247,434.9\n255,863.9\n\n\nAverage share of income coming from:\n\n\n\n\n\n\nSale of agricultural products\n22.8%\n0.0%\n0.1%\n5.3%\n\n\nImputed use of agricultural products for own consumption\n8.7%\n4.3%\n0.6%\n3.2%\n\n\nHired employment\n28.5%\n40.5%\n46.6%\n41.3%\n\n\nSelf-employment\n10.2%\n9.2%\n8.0%\n8.7%\n\n\nProperty (rent, interest, equity gain)\n0.1%\n0.1%\n0.2%\n0.2%\n\n\nPublic pensions and benefits\n22.2%\n33.8%\n31.5%\n29.7%\n\n\nTransfers\n7.2%\n10.5%\n11.6%\n10.4%\n\n\nOther\n0.7%\n1.3%\n1.4%\n1.2%\n\n\nAll income shares\n100%\n100%\n100%\n100%\n\n\n\nSource: author based on Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).",
    "crumbs": [
      "Home",
      "Background notes",
      "Household Assets and Fuelwood Use"
    ]
  },
  {
    "objectID": "bg-notes/hh-assets-and-fuelwood-use.html#household-reliance-on-firewood-for-heating",
    "href": "bg-notes/hh-assets-and-fuelwood-use.html#household-reliance-on-firewood-for-heating",
    "title": "Household Assets and Fuelwood Use",
    "section": "Household reliance on firewood for heating",
    "text": "Household reliance on firewood for heating\n\nExpenditure elasticities for fuelwood\nMost homes in Armenia use natural gas for heating (61.9% or 495,203 households), but an important 23.8% (190,884 households) use wood for heating, followed by 21.7% that use electricity and 8.2% pressed dung. Negligible percentages of households use liquefied gas or coal (0.2% and 0.9% respectively). In rural areas, 51.5% of rural homes (143,724 households) use wood for heating, which correlates with the 54.8% of rural households that have a self-made heater as main technology. Not only is wood used as heating source, but it is also used as an “energy carrier” (mainly cooking) by 11.3% of all households. In rural areas, 25.8% of homes use it for this purpose.\nWhile there is a market for fuelwood, many homes do not pay for fuelwood annually. Figure 2 shows the distribution for this concept, showing a clear component of households paying zero Dram for their fuelwood consumption annually. This is possibly related to those households that collect wood for free. However, as shown in Table 4, homes that do pay for fuelwood spend an average of AMD 128,209.2 (about USD 317.8) annually, which is roughly 5.5% of total annual expenditure for the average household.\nFigure 2. Annual wood expenditure distribution\n\nSource: Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).\nTable 4. Average fuelwood expenditure and quantity used annually and in the month prior to the survey\n\n\n\n\n\n\n\n\n\n\n\n\nArea\nAverage monthly household total expenditure (Dram)\nAverage monthly household energy expenditure (Dram)\nFuelwood expenditure in month prior to survey (Dram)\nFuelwood quantity used in month prior to survey (m3)\nFuelwood expenditure annually\n(Dram)\nFuelwood quantity used annually\n(m3)\n\n\n\n\nURBAN\n187,752.5\n12,700.4\n2,510.2\n0.6\n116,428.6\n6.4\n\n\nRURAL\n201,881.6\n15,652.8\n1,821.9\n0.6\n131,653.5\n6.8\n\n\nCountry\n192,673.8\n13,728.7\n1,977.6\n0.6\n128,209.2\n6.7\n\n\n\nSource: author based on Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).\nHowever, an expenditure elasticity of annual wood expenditure across income deciles1 (when moving from decile to decile) points to a complex relationship between income and fuelwood expenditure. The higher elasticities in the middle deciles might reflect an increased ability and desire to spend on fuelwood, possibly for heating during colder months. The negative figures in the higher deciles could suggest a shift towards more modern heating solutions, or that beyond a certain income level, the relative importance of fuelwood decreases. The variations between deciles underscore the diverse factors influencing energy choices, including affordability, accessibility, and preferences. (see Table 5). The higher expenditure elasticity for fuelwood in decile 5 could be indicative of larger household sizes or larger homes that require more wood for heating and cooking. This would naturally lead to a higher quantity of fuelwood being used, which would increase expenditure on fuelwood without necessarily implying luxury consumption that theory would suggest with high elasticities.\nThis interpretation would be consistent with a scenario where these households, perhaps due to their size or location, have not yet transitioned to other energy sources, which are often more accessible in urban settings or to wealthier households that can afford the initial investment in more modern heating systems. This considers the socio-economic context that might prevent a switch to alternative fuels, even when households have slightly more income. Issues such as availability of infrastructure, initial costs of switching to gas or electric heating, and cultural preferences for wood.\nIn rural areas for example, the availability of fuelwood and the lack of infrastructure for alternative energy sources can lead to a situation where households spend more on wood simply because it’s one of the few options available to them, and not because they particularly prefer wood over other fuels. Indeed, when conducting this analysis across urban to rural, the elasticity value reaches 3.25 (growth of wood expenditure is three times the growth of total consumption across urban/rural), confirming that living in rural areas might be a big determinant of fuelwood use, simply because it’s the technology to which those homes have access and other infrastructure is lacking.\nTable 5. Expenditure elasticity of annual wood expenditure across deciles\n\n\n\nDecile\nElasticity of Annual Wood Expenditure\n\n\n\n\n1\n-\n\n\n2\n0.5\n\n\n3\n0.2\n\n\n4\n1.6\n\n\n5\n3.3\n\n\n6\n2.1\n\n\n7\n-0.3\n\n\n8\n0.4\n\n\n9\n-0.1\n\n\n10\n-0.2\n\n\n\nSource: author based on Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).\nFor comparison, the expenditure elasticity of natural gas expenditure (which has a more traditionally priced market) across income deciles suggest that natural gas is not consistently treated as a normal or inferior good across the income spectrum. The negative elasticities in higher deciles may indicate a trend of higher-income households either becoming more energy-efficient, having better-insulated homes, or switching to alternative fuels that are perceived as cleaner or more convenient. This might be consistent with the higher percentage of urban homes that use electricity for cooking and heating. The very high negative elasticity in decile 9 is particularly notable and could warrant a closer investigation to understand the underlying causes.\nTable 6. Expenditure elasticity of natural gas expenditure across deciles\n\n\n\nDecile\nElasticity\n\n\n\n\n1\n-\n\n\n2\n0.62\n\n\n3\n0.10\n\n\n4\n-0.88\n\n\n5\n-0.44\n\n\n6\n0.22\n\n\n7\n-0.29\n\n\n8\n-0.48\n\n\n9\n-6.59\n\n\n10\n-0.55\n\n\n\nSource: author based on Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).\nWhen comparing pressed dung, the elasticity behavior shows a component that responds much more to income levels. Table 7 shows the household expenditure elasticity of press dung expenditure across deciles, suggesting big leaps to lower levels of expenditure on pressed dung as households move from decile to decile, especially moving from decile 3 to 4, from decile 5 to 6 and then from 6 to 7.\nTable 7. Expenditure elasticity of pressed dung expenditure across deciles\n\n\n\nDecile\nElasticity of Pressed Dung Expenditure\n\n\n\n\n1\n0.00\n\n\n2\n0.43\n\n\n3\n-0.56\n\n\n4\n-6.41\n\n\n5\n-3.35\n\n\n6\n-25.71\n\n\n7\n-12.18\n\n\n8\n1.02\n\n\n9\n0.48\n\n\n10\n-4.81\n\n\n\nSource: author based on Integrated Living Conditions Survey, 2022 (ARMSTAT, 2023).\n\n\nDeterminants of fuelwood consumption\nThere are various factors that influence fuelwood consumption by households. While income is important, it’s not the only determinant, and there are other factors, which include affordability and availability of fuels, scarcity of fuelwood supply, fuel preferences, and cost and performance of end-use equipment. Moreover, all these factors also on whether the household is in an urban or rural setting (Lefevre et al., 1997).\nTo better understand this, a linear model was conducted on household data for Armenia (to explain annual wood quantity used by household as determined by Marz, urban/rural, total household income, dwelling size in square meters, hectares of forests per thousand inhabitants by Marz2 (see Table 8), average price of natural gas in Marz and area (urban/rural), computed as the average of the division of total spent on natural gas by amount of natural gas used by households in that Marz and area (urban/rural); and average electricity bill in that Marz and Area.\nThe explanatory power of the model is moderate3 (Adjusted R-squared: 0.2668), but the results are interesting as shown in Table 9. The baseline level of annual wood consumption is high (with an intercept 26.51 cubic meters of wood). The effect of administrative division is small and statistically insignificant (ADM1 administrative division -0.0271). The effect of being in a rural area, compared to an urban area, is not significant. Results indicate that for each unit increase in total income, wood consumption increases by approximately 0.0000029 cubic meters per year. This effect is highly significant (p &lt; 2.26e-16), suggesting a positive relationship between income and wood consumption, which is counterintuitive. More in line with expectations is dwelling size, which is also highly significant, suggesting that higher dwelling sizes lead to higher fuelwood use. The number of hectares of forest by thousand inhabitants is a significant predictor. The coefficient indicates that more forest availability leads to higher wood consumption. Both the price of natural gas and average expenditure on electricity in Marz and area are non-significant.\nTable 8. Hectares of forest per thousand inhabitants\n\n\n\n\n\n\n\n\n\n\n\nMarz\nForested areas (Ha)\nNot Forested areas (Ha)\nTotal area (Ha)\nPopulation 2020 (thousand inhabitants)\nHectares of forest per thousand inhabitants\n\n\n\n\nYerevan\n-\n23,246.8\n23,246.8\n1,084.0\n-\n\n\nAragatsotn\n1,948.2\n270,744.9\n272,693.1\n26.8\n72.7\n\n\nArarat\n3,417.5\n207,130.8\n210,548.3\n72.1\n47.4\n\n\nArmavir\n196.5\n125,429.7\n125,626.2\n82.4\n2.4\n\n\nGegharkunik\n13,525.9\n383,155.5\n396,681.4\n66.6\n203.1\n\n\nLori\n81,492.8\n293,208.2\n374,701.0\n126.1\n646.3\n\n\nKotayk\n11,571.6\n199,289.4\n210,861.0\n136.8\n84.6\n\n\nShirak\n882.2\n269,527.6\n270,409.8\n135.6\n6.5\n\n\nSyunik\n44,390.9\n402,134.9\n446,525.9\n93.2\n476.3\n\n\nVayots Dzor\n2,500.0\n226,086.5\n228,586.5\n17.1\n146.2\n\n\nTavush\n121,962.8\n149,079.3\n271,042.2\n93.2\n1,308.6\n\n\n\nSource: author land use land cover information from ESA CCI Land Cover time-series v.2.0.7, 1992-2020 (ESA, 2023) and population data (ARMSTAT, 2020).\nTable 9. Regression results on the determinants of fuelwood use\n\n\n\n\nMin\n1Q\nMedian\n3Q\nMax\n\n\n\n\nResiduals:\n-13.625\n-1.716\n-0.501\n1.35\n13.445\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficients:\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\nSignif.\n\n\n\n\n(Intercept)\n6.19E+00\n1.58E+01\n0.391\n0.696\n\n\n\nADM1 administrative divisions\n-2.71E-02\n3.33E-02\n-0.815\n0.415\n\n\n\nUrban/Rural\n7.22E-02\n2.02E-01\n0.358\n0.72\n\n\n\nDwelling size (m2)\n2.08E-02\n2.22E-03\n9.4\n&lt; 2e-16\n***\n\n\nTotal income (Dram)\n2.22E-06\n3.14E-07\n7.081\n2.26E-12\n***\n\n\nHa. of forest / 1000 inhabitants\n3.28E-03\n3.23E-04\n10.145\n&lt; 2e-16\n***\n\n\nAvg. price of natural gas (Dram/m3)\n-2.47E-02\n1.14E-01\n-0.217\n0.828\n\n\n\nAvg. electricity expenditure (Dram)\n-2.88E-06\n9.76E-05\n-0.03\n0.976\n\n\n\n\nSignificance codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ’ ’ 1\nResidual standard error: 2.808 on 1392 degrees of freedom (3784 observations deleted due to missingness)\nMultiple R-squared: 0.2704, Adjusted R-squared: 0.2668.\nF-statistic: 73.71 on 7 and 1392 DF, p-value: &lt; 2.2e-16.\nOverall, it can be concluded that there is an income effect that is counter intuitive. Higher income is associated with increased wood consumption. Wealthier households probably consume more wood due to larger homes as captured in the effect of dwelling size, and a strong cultural preference for wood as a fuel source with probably limited alternatives due to infrastructure or other reasons. Forest availability is also a strong predictor, suggesting that more available forest resources lead to higher wood consumption. There’s a tendency for wood consumption to decrease as natural gas prices increase, though this is almost insignificant and general electricity expenditure does not seem to be a significant determinant of wood consumption at all.",
    "crumbs": [
      "Home",
      "Background notes",
      "Household Assets and Fuelwood Use"
    ]
  },
  {
    "objectID": "bg-notes/hh-assets-and-fuelwood-use.html#references",
    "href": "bg-notes/hh-assets-and-fuelwood-use.html#references",
    "title": "Household Assets and Fuelwood Use",
    "section": "References",
    "text": "References\nARMSTAT. (2020). Marzes of the Republic of Armenia and Yerevan city in figures, 2020.\nARMSTAT. (2023). Integrated Living Conditions Survey 2022 [dataset].\nCeriani, L., Olivieri, S., & Ranzani, M. (2019). Housing, imputed rent, and households’ welfare. Poverty & Equity Global Practice Working Papers, 213. https://documents1.worldbank.org/curated/pt/336451565194643402/pdf/Housing-Imputed-Rent-and-Households-Welfare.pdf\nESA. (2023). ESA CCI Land Cover time-series v.2.0.7 (1992-2020). European Space Agency.\nLefevre, T., Todoc, J., & Raj Timilsina, G. (1997). The Role of Wood Energy in Asia. Food and Agriculture Organization of the United Nations.\nTNFD. (2023). Recommendations of the Taskforce on Nature-related Financial Disclosures. Green Finance Institute.",
    "crumbs": [
      "Home",
      "Background notes",
      "Household Assets and Fuelwood Use"
    ]
  },
  {
    "objectID": "bg-notes/hh-assets-and-fuelwood-use.html#footnotes",
    "href": "bg-notes/hh-assets-and-fuelwood-use.html#footnotes",
    "title": "Household Assets and Fuelwood Use",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is calculated as: Decile Expenditure Elasticity of Demand = % Change in average expenditure from decile to decile / % Change in average Fuelwood Expenditure Demanded by (expenditure) decile.↩︎\nAuthor calculated this variable using land use land cover information from ESA CCI Land Cover time-series v.2.0.7, 1992-2020 (ESA, 2023) and population data (ARMSTAT, 2020).↩︎\nModerate but robust. The model has been probed using the Variance Inflation Factor, showing that the model does not appear to suffer from sever multicollinearity. Correlations among variables are moderate or low as well. A concern about interaction between urban/rural and dwelling size, which removed the slight significance of a previous model without dwelling size was ruled out, testing a model with the interaction of these variables, which was not significant.↩︎",
    "crumbs": [
      "Home",
      "Background notes",
      "Household Assets and Fuelwood Use"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "bg-notes/vulnerability.html",
    "href": "bg-notes/vulnerability.html",
    "title": "Vulnerability Background Note",
    "section": "",
    "text": "Armenians face exposure to a variety of natural hazards and climate shocks, including air pollution, floods, earthquakes, forest loss, air quality, variation in precipitation, and variation in temperature. A vulnerability index analysis conducted alongside this study (World Bank, 2023b) reveals that the northern areas of the country are most exposed to these hazards. People’s ability to handle the impacts of these hazards and shocks depends on their economic resources and institutions. That means that exposure alone is not a problem and it is the combination of exposure with economic concentration and living standards what creates sticking points in the face of these hazards and also offers mitigating factors when economic conditions and living standards are better. When considering this interaction, the most vulnerable locations are Ghukasyan (Ararat), Gugark, Kalinino, Spitak (Lori), Aragats (Aragatsotn), Amasya, and Ani (Shirak) and the least vulnerable are all located in or near major cities, including Yerevan (Capital), Gyumri (Shirak), (Kotayk), Dildijan (Tavush), and Vagharshapat (Armavir).\nArmenians are convinced that climate change is real and, according to the Life in Transition (LiTS) and Deep Dive Surveys (DDS) (World Bank, 2023a), and 84 percent of Armenian respondents in those surveys believed that climate change will seriously affect them during their lifetime (World Bank, 2023). Moreover, 92 percent of them believed that climate change would affect today’s children during their lifetime, making Armenia the seventh most aware country out of the 37 surveyed from both, Europe and Central Asia. The concerns are such, that 51 percent of respondents believe that protecting the environment is more important than economic growth. Half of respondents see regulations and public investments as important to address climate change, but remain concerned, as only 8 percent of individuals believe hypothetical extra taxes would be spent on climate change by the government.\nAlmost a quarter of Armenian households (187,176 out of 800,604 households or 23.4 percent) derive income from the sales of agricultural products. However, exposure to the climate risks of agriculture is larger, since 41 percent of households use land for agricultural purposes (owned and/or rented), regardless of agricultural sales, and for almost a fifth of the households (135,360 households), 26.4 percent of their total income also comes from hired employment in the agricultural sector, 20.2 percent from the sale of agricultural products, and 9.5 percent from imputed use of agricultural products for own consumption. For this group, the total exposure to the agricultural sector represents 56 percent of their total income under climate risks faced there.\nFlood risks threaten regional growth in a differentiated manner. An analysis conducted for this report estimates combined agricultural, buildings and roads losses ranging between 0.1 and 15.4 percent of regional GDP that would result from a 1 in a 100-year flood event in Armenia’s regions for a country total of between USD 167.1 and USD 462 million (Lenoble & Rozenberg, 2024). The worst hit regions would be Vayots Dzor (up to 15.4 percent), Lori (up to 12.0 percent), Tavush (up to 11.1 percent), and Aragatsotn (up to 9.4 percent), while the Yerevan metropolitan region would be the most resilient (up to 1.2 percent). These disparities in vulnerability suggest that different policies and investment public and private strategies are needed at the regional level for flood proofing production zones, infrastructure, and dwellings.\nHomes are perhaps the most important asset owned by households. In 2022, most of the 800,604 Armenian households owned their dwelling (about 89.4 percent), as shown in Figure 1, with only about 5.9 percent of homes renting and 4.7 percent with other forms of tenure . Also, about 97 percent of households lived in houses or apartments, as opposed to less formal structures. Houses averaged 112.0 m2, while apartments averaged 68.3 m2. As expected, 96.2 percent of apartments were located in urban areas, with about half of them in Yerevan (55.3 percent). Houses, on the other hand, were located mostly in rural areas (65.8 percent), with about 13.4 percent of them located in Yerevan (ARMSTAT, 2023).\nOwned dwellings are an asset from which households derive welfare. By not having to spend on housing, more is available as disposable income. The emergent rental market information of Armenia was used to impute rent to non-renters using a log linear modeling approach described by Ceriani, Olivieri, & Ranzani (2019) and it was established that owners derived an average of AMD 53,145.27 (USD 122.8) in monthly imputed rent, with an average dwelling of 89m2.\nWhen this imputed rent is considered a long-term asset of income flows until 2050, its Net Present Value is established at an average of AMD 15.6 million (USD 36,057.7). This varies by region, with the highest values in Yerevan, Kotayk, Ararat and Armavir and the lowest values in Vayots Dzor, Shirak, and Tavush.\nHeavy rainfall and floods exacerbated by climate variability already have an impact on between 2.1 and 5.4 percent of Armenian structures every year, depending on the region of the country. If proper adaptation measures were in place, 21,577 Armenian homes (3 percent) would not lose an average of USD 3,226.7 from the imputed rent asset value of their homes each year.\nAgriculture from owned land also provides an income flow that is considered an asset with an average Net Present Value of AMD 13.4 million (USD 30,939.5 ) for households that sell agricultural products. However, depending on the region, between 1.4 and 6.8 percent of these are impacted by heavy rainfall and floods annually. If proper climate adaptation measures were in place, as many as 7,711 households every year would avoid average asset losses of Net Present Value of agricultural income of AMD 2.1 million (USD 4,874.5).\nFor households with agricultural income, the combined welfare losses of impacts to their dwellings and crops brought about by exposure to climate related heavy rains and floods represents losses of about 6 percent of their monthly total consumption expenditure. This means that for these households, appropriate climate change adaptation measures could mean an increase in average monthly income from USD 498.5 to USD 529.6 pushing the entire distribution away from the poverty line. For about 280 of the 7,711 impacted households (about 4 percent), this would mean escaping poverty.\nDepending on the region, between 19 and 57 percent of buildings would be exposed to a 1 in 100 years flood event. This would mean that a total of 206,591 households (26% of all households) would suffer monthly average losses in consumption expenditure of about 3.5 percent and 5,237 or 2% of them would fall into poverty under such an eventuality.\nSimilarly, between 12 and 60 percent of households with agricultural production could be impacted by a 1 in 100 year flood event. A total of 68,969 households could see an average 6.5 percent of their monthly consumption expenditure lost by this situation. For 7 percent of households impacted (4,873 households) this would push them below the poverty line.\nFor this last group, impacts on both buildings and agricultural production would result in average losses of 8.1 percent of monthly consumption. This would result in 6,418 households (9 percent of those impacted) falling below the poverty line.\n\n\n\n\nReferences\n\nARMSTAT. (2023). Integrated Living Conditions Survey 2022.\n\n\nCeriani, L., Olivieri, S., & Ranzani, M. (2019). Housing, imputed rent, and households’ welfare. Poverty & Equity Global Practice Working Papers, (213). Retrieved from https://documents1.worldbank.org/curated/pt/336451565194643402/pdf/Housing-Imputed-Rent-and-Households-Welfare.pdf\n\n\nLenoble, C., & Rozenberg, J. (2024). Armenia flood risk analysis for the CCDR [Draft for Discussion]. Washington, D.C.\n\n\nWorld Bank. (2023a). Perspectives on Climate Change: Beliefs, Challenges, and Policy Recommendations: Evidence from Five Countries in Europe and Central Asia. Washington, D.C.: The World Bank.\n\n\nWorld Bank. (2023b). Vulnerability Map for Armenia. Washington, D.C.: The World Bank.",
    "crumbs": [
      "Home",
      "Background notes",
      "Vulnerability Background Note"
    ]
  },
  {
    "objectID": "data/ARM-Geodata/ARM-ADM1.html",
    "href": "data/ARM-Geodata/ARM-ADM1.html",
    "title": "Armenia CCDR",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     \n\n\n        0 0     false"
  },
  {
    "objectID": "drafts/tasks.html",
    "href": "drafts/tasks.html",
    "title": "Tasks",
    "section": "",
    "text": "You will re-do the impact of flooding on imputed rent with the admin1 data attached (Data_AAL_AAE.xlsx) and with a duplication of households instead of random selection. We cannot calculate a poverty impact or income distribution impact, however showing who is affected more (poor vs non poor for example) is already a very useful story for the CCDR.\nYou will do the analysis of flooding on agriculture income using the average annual losses from the attached data (Data_AAL_AAE.xlsx) and with a duplication of households instead of random selection. For this one we can calculate an impact on poverty or income distribution. For example, are there households that fall into poverty because of the shocks?\nFor both these impacts it would be interesting to show the total impact for poor vs non poor (or other groups, for example urban vs rural) but also the impact relative to the households’ income for these same groups."
  },
  {
    "objectID": "drafts/tasks.html#flood-analysis",
    "href": "drafts/tasks.html#flood-analysis",
    "title": "Tasks",
    "section": "",
    "text": "You will re-do the impact of flooding on imputed rent with the admin1 data attached (Data_AAL_AAE.xlsx) and with a duplication of households instead of random selection. We cannot calculate a poverty impact or income distribution impact, however showing who is affected more (poor vs non poor for example) is already a very useful story for the CCDR.\nYou will do the analysis of flooding on agriculture income using the average annual losses from the attached data (Data_AAL_AAE.xlsx) and with a duplication of households instead of random selection. For this one we can calculate an impact on poverty or income distribution. For example, are there households that fall into poverty because of the shocks?\nFor both these impacts it would be interesting to show the total impact for poor vs non poor (or other groups, for example urban vs rural) but also the impact relative to the households’ income for these same groups."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Armenia Country Climate and Development Report",
    "section": "",
    "text": "This website contains background documents and guides created for Armenia’s Country Climate and Development Report. Authorship is indicated in each document."
  },
  {
    "objectID": "supporting-materials/microsimulation.html",
    "href": "supporting-materials/microsimulation.html",
    "title": "Armenia CCDR Microsimulation",
    "section": "",
    "text": "In this calculation file, we “age” the household survey according to demographic projections and different macroeconomic scenarios to explore the impact of climate-related risks and policy measures on the consumption expenditure distribution. It is part of a larger project with all contributions to Armenia’s CCDR, which can be downloaded from GitHub in the form of an Rstudio project.\nAs a convention, code is presented in the following format in this guide:\n\n# Some comment that is not evaluated by R\nsome_variable &lt;- some_function(some_object, some_parameter = TRUE)\n\nWe assume that the reader has created an Rstudio project and is familiar with basic R functions. Within that project we recommend the following file structure:\nroot/\n├── scripts\n│   └── my_script.R\n├── data/\n|   ├── my_data.sav\n|   ├── my_data.dta\n|   └── my_data.csv\n└── output\n    ├── my_output1.csv\n    └── my_output2.xlsx\nUsing RStudio project makes it possible to not use setwd() to establish the root directory and refer to subdirectories in a relative manner, making interoperability easier within teams and not hard coding a particular computer’s file structure into the code. If you are not using RStudio, just add setwd(r'(C:\\My\\path\\to\\project\\root)') at the beginning of your coding session.",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  },
  {
    "objectID": "supporting-materials/microsimulation.html#introduction",
    "href": "supporting-materials/microsimulation.html#introduction",
    "title": "Armenia CCDR Microsimulation",
    "section": "",
    "text": "In this calculation file, we “age” the household survey according to demographic projections and different macroeconomic scenarios to explore the impact of climate-related risks and policy measures on the consumption expenditure distribution. It is part of a larger project with all contributions to Armenia’s CCDR, which can be downloaded from GitHub in the form of an Rstudio project.\nAs a convention, code is presented in the following format in this guide:\n\n# Some comment that is not evaluated by R\nsome_variable &lt;- some_function(some_object, some_parameter = TRUE)\n\nWe assume that the reader has created an Rstudio project and is familiar with basic R functions. Within that project we recommend the following file structure:\nroot/\n├── scripts\n│   └── my_script.R\n├── data/\n|   ├── my_data.sav\n|   ├── my_data.dta\n|   └── my_data.csv\n└── output\n    ├── my_output1.csv\n    └── my_output2.xlsx\nUsing RStudio project makes it possible to not use setwd() to establish the root directory and refer to subdirectories in a relative manner, making interoperability easier within teams and not hard coding a particular computer’s file structure into the code. If you are not using RStudio, just add setwd(r'(C:\\My\\path\\to\\project\\root)') at the beginning of your coding session.",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  },
  {
    "objectID": "supporting-materials/microsimulation.html#preamble",
    "href": "supporting-materials/microsimulation.html#preamble",
    "title": "Armenia CCDR Microsimulation",
    "section": "2 Preamble",
    "text": "2 Preamble\nWe start with a clean environment, making sure that any objects from a previous session are not present. We take this opportunity to keep our country ISO code in a variable iso in case we need it later.\n\n# Clean workspace\nrm(list = ls())\n\n# Armenia country ISO code\niso &lt;- \"ARM\"\n\n# Survey year\nsurvey_year &lt;- 2022\n\n# Exchange rate USD per dram\ner &lt;- 0.002310\n\n# Years of interest for our macroeconomic scenario analysis\nanalysis_years &lt;- c(2030)\n\nWe call the appropriate libraries.\nRather than calling our libraries as we go, we will make sure we have everything we need from the beginning.\n\nlibrary(tidyverse) # includes dplyr, ggplot2, purr...\nlibrary(haven)     # to read SPSS and Stata datasets\nlibrary(readxl)    # to read from MS-Excel\nlibrary(openxlsx)  # to write to MS-Excel.\nlibrary(gt)        # pretty tables\nlibrary(car)       # companion to applied regression\nlibrary(modelr)    # regression models\n#library(anesrake)  \n# Raking reweighting but we don't load it, because \n# it changes the meaning of summarize from dplyr, \n# so we use the form anesrake::anesrake() when using it.\n#library(ebal)      # Entropy reweighting (not used)\nlibrary(janitor)   # pretty subtotals\nlibrary(broom)     # More regressions\nlibrary(zoo)       # Calculate moving window average and max value\n\n# Geopackages\nlibrary(sf)        # to read and write shapefile maps\nlibrary(terra)     # to perform geocalculations\nlibrary(tmap)      # for static and interactive maps",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  },
  {
    "objectID": "supporting-materials/microsimulation.html#datasets",
    "href": "supporting-materials/microsimulation.html#datasets",
    "title": "Armenia CCDR Microsimulation",
    "section": "3 Datasets",
    "text": "3 Datasets\nWe then load the datasets that we need for this study. The World Bank has processed some of these already for poverty analysis and so we have the original SPSS datasets with all variables for Households hh and for Individuals pp, as well as a consumption aggregate ca and a household income ic dataset, which are Stata datasets. This is for the year 2022. These are imported using the haven package. These are based on Armenia Integrated Living Conditions Survey 2022 (ARMSTAT, 2023). We take this oportunity to standardize the household identification variable to household_id.\n\n# Households (hh)\nhh &lt;- read_sav(\n  \"data/ARM-HH-survey/original-spss-files/ILCS-ARM-2022-Households.sav\") %&gt;% \n  rename(household_id = interview__key)\n# Persons (pp)\npp &lt;- read_sav(\n  \"data/ARM-HH-survey/original-spss-files/ILCS-ARM-2022-Persons.sav\") %&gt;% \n  rename(household_id = interview__key)\n# Consumption aggregate at household level (ca)\nca  &lt;- read_dta(\"data/ARM-HH-survey/CONSAGG2022.dta\") %&gt;% \n  rename(household_id = hhid)\n# Processed income at household level (ic)\nic  &lt;- read_dta(\"data/ARM-HH-survey/totinc.dta\") %&gt;% \n  rename(household_id = interview__key)\n# Food diary\nfood_with_prices &lt;- read_dta(\"data/ARM-HH-survey/FOOD_with_prices_short.dta\")%&gt;% \n  rename(household_id = hhid)\n\nWe will work non-destructively, meaning we will not rewrite these data sets and we will only create intermediate data frame objects from them to perform transformations, selections and other data management tasks. For example, we will keep household assignment to poverty status and consumption deciles handy by creating a subset of our ca data with only our household identifiers, deciles, and poverty.\n\n# From the WB processed dataset, we extract deciles and poverty\ndeciles &lt;- ca %&gt;% \n  select( household_id, decile, poor_Avpovln2022, \n          poor_Foodpovln2022, poor_Lpovln2022, poor_Upovln2022)\n\nOur population data comes from UN’s projections.\n\npopulation_projections &lt;- read_dta(\"data/UN2022_population.dta\") %&gt;% \n  filter(country == iso) # we filter for Armenia\n\nThe macro scenario dataset is an input provided by the Macroeconomic CGE simulation team, with yearly information on GDP, working age population, employment by economic activity (for an aggregation of three sectors: agriculture, manufacturing, and services), wages by economic activity, value added by economic activity, remittances, consumer price index, food price index and energy price index (for a bundle of gas, oil, coal, electricity) by decile (10 representative households in the macro model), and carbon tax revenue transfers to household deciles.\n\nscenario_file &lt;- \"data/ARM-Microsimulation/ARM_MacroScenarioInformation.xlsx\"\nscenario_varlist &lt;- read_xlsx(\n  \"data/ARM-Microsimulation/ARM_Macro_varlist.xlsx\")\nprices_2030 &lt;- \n  read.csv(\"data/ARM-Microsimulation/prices2030.csv\")\n\nEconomic Activities in the Survey is in Armenian. The following dataset is a lookup table with the English names.\n\nsectors &lt;- read_xlsx(\"data/ARM-HH-survey/economic_activity_codes.xlsx\")\n\nWe also have geographical information for level 1 in Shapefile format, which we import with the sf package. We rename the column with the name of the administrative region to match our household survey data set conventions to ease mergers. The dplyr package from the tidyverse meta package allows us to “pipe” or link processing steps using the %&gt;% pipe. Although there is no geoprocessing in this analysis, this will come in handy for graphical presentations.\n\n# Armenia marzes or administrative level 1 shapefile\nadm1 &lt;- read_sf(\"data/ARM-Geodata/ARM-ADM1.shp\") %&gt;% \n  select(COD_HH_SVY, NAM_1, geometry) %&gt;% \n  # Make sure that names match the rest of datasets\n  mutate(NAM_1 = if_else(NAM_1 == \"Gergharkunik\", \"Gegharkunik\", NAM_1)) %&gt;% \n  # Sort by number\n  arrange(COD_HH_SVY)\n# We rename with the survey designation\nnames(adm1) &lt;- c(\"marz_no\",\"marz\", \"geometry\") \n\nAnd we plot it for reference (see Figure 1). This is done with the tmap R package and the code shown in Listing 1.\n\n\n\n\nListing 1: Plotting a map with with the tmap package\n\n\ntm_shape(adm1)+\n  tm_polygons(\"marz\", legend.show = FALSE) +\n  tm_text(\"marz\", size = 3/4)\n\n\n\n\n\n\n\n\n\n\nFigure 1: Map of Armenia at administrative level 1 (ADM1)\n\n\n\n\n\nMarzes names are more accurate in the shapefile than in the survey. We will use them from here on instead of the survey factor labels.\n\n\n\n\nListing 2: Marz name from geodata\n\n\nadm1_names &lt;- adm1 %&gt;% \n  select(-geometry)\n\nhh &lt;- hh %&gt;% \n  left_join(adm1_names, join_by(hh_02 == marz_no))\n\nic &lt;- ic %&gt;% \n  left_join(adm1_names, join_by(hh_02 == marz_no))\n\nrm(adm1_names)\n\n\n\n\nWe also have an Excel file with changes to labor productivity due to climate variability. We bind together the datasets found in each Excel sheet.\n\n\n\n\nListing 3: Import labor productivity data\n\n\nfile &lt;- r\"(data/ARM-Microsimulation/LaborProductivityChanges.xlsx)\"\nsheets &lt;- excel_sheets(file)\n\n# Use lapply to read and process each sheet\nlabor_productivity &lt;- lapply(sheets, function(sheet) {\n  info &lt;- read_excel(\n    file,\n    sheet = sheet,\n    col_names = TRUE,\n    col_types = c(\"text\", \"text\", \"numeric\", \"text\", \"numeric\")\n  )\n  info$sector &lt;- sheet\n  return(info)\n})\n\n# Bind all data frames in the list into a single data frame\nlabor_productivity &lt;- bind_rows(labor_productivity)\n\n\n\n\nFinally, we have our climate vulnerability information. For this analysis we only use the crops_productivity and livestock_productivity that comes from Estimating the Economic Impacts of Climate Change in Armenia (Strzepek, Boehlert, Castillo, & Smet, 2024).\n\n\n\n\nListing 4: Import crops and livestock yield loss data\n\n\ncrops_productivity &lt;- \n  read.csv(\"data/ARM-Vulnerability-Analysis/ARM_crops_combined_REF_shock_admin1.csv\") %&gt;% \n  rename(marz = Province)\nlivestock_productivity &lt;-\n  read.csv(\n    \"data/ARM-Vulnerability-Analysis/ARM_livestock_REF_shock_admin1.csv\"\n    ) %&gt;% \n  rename(marz = Province)",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  },
  {
    "objectID": "supporting-materials/microsimulation.html#data-preparation-income-outliers-and-missings",
    "href": "supporting-materials/microsimulation.html#data-preparation-income-outliers-and-missings",
    "title": "Armenia CCDR Microsimulation",
    "section": "4 Data preparation income outliers and missings",
    "text": "4 Data preparation income outliers and missings\nWe start with various renames for standardization. Naming conventions in the guidance code use traditional abbreviations like nli for non-lablor income. We are opting for more descriptive variable names like non_labor_income, labor_income, etc. to have more easily readable code. We make an exception for total consumption, because it’s a variable that we use in every scenario and it supersedes lenght limits when adding scenario identifiers.\n\nca &lt;- ca %&gt;% \n  rename(urban_rural = urb_rur,\n         tc = totc)\n\n\n4.1 Household consumption aggregates and characteristics\nInitial necessary variables.\n\npoverty_designations &lt;- ca %&gt;%\n  mutate(rural_dummy = ifelse(urban_rural == 2, 1, 0)) %&gt;%\n  select(household_id, rural_dummy, hhsize,hhsize_R, tc, marz, aepc, weight, \n         Foodpovln2022, Lpovln2022, Upovln2022, Avpovln2022, \n         poor_Foodpovln2022, poor_Lpovln2022, poor_Upovln2022, \n         poor_Avpovln2022, decile )  # Keep only necessary columns\n\n\n\n4.2 Demographic characteristics, education, labor force\nHere the original code calls for Zone data, which is not present in our dataset, due to the different administrative structure of Armenia. However, we use hh_01_code (settlement) for this purpose. In the end, this variable was never used.\n\nzone_data &lt;- hh %&gt;% \n  select(household_id, hh_01_code, hh_02, hh_03, marz) %&gt;% \n  rename(\n    household_id  = household_id, # Household id\n    settlement    = hh_01_code,   # Settlement\n    marz_no       = hh_02,        # Marz\n    urban_rural   = hh_03         # 1 = urban, 2 = rural\n  )\n\nDemographic data, merge with zone data Note that ed_03 (educy) below is not years of education, but education level (primary, general, secondary, etc.) However, it is ordered in a way that higher levels imply more years of education. We perform several steps within the first pipe call. The variable lstatus (Labor Force Status) here is very important for the reweigthing of the dataset later on. Note that from here onwards we will be creating _microsim versions of our datasets with the transformations needed for calculations. That way we avoid changing our original data and can refer to it later without fearing we’ve left things behind.\n\npp_microsim &lt;- pp %&gt;%\n  rename(household_id = household_id) %&gt;%\n  left_join(zone_data, join_by(household_id))  %&gt;%\n  mutate(\n    # Demographic characteristics\n    # Unique person id\n    person_id = paste0(household_id, \"-\", str_pad(mem_001__id, 2, pad = \"0\")),\n    head = ifelse(mem_03 == 1, 1, 0),\n    # Education level\n    educy = ifelse(is.na(ed_03) | ed_03 == 8, 0, ed_03),\n    # Labor Force Status\n    lstatus = case_when(\n      # 1. Employed\n      est_03 == 1 | est_04 == 1 | est_05 == 1 |\n        est_06 == 1 | est_08 == 1 ~ 1L,\n      # 2. Unemployed (available, and searching)\n      est_10 == 1 ~ 2L,\n      # 3. Inactive (available, not searching)\n      est_10 == 2 ~ 3L,\n      # Out of the labor force\n      .default = 4L # Default to OLF\n    ),\n    employed = (lstatus == 1),\n    # Salaried status (1. paid employee; 2 self-employed)\n    salaried = ifelse(\n      !is.na(emp_11a),\n      1L,\n      ifelse(is.na(emp_11a) &\n               employed == TRUE, 0L, NA_integer_)\n    )\n  ) %&gt;%\n  rename(rel = mem_03, # relationship to HH head\n         gender = mem_02,\n         age = mem_05)\n\nLater, when we conduct the reweighting of the dataset, we need to summarize into three levels of education.\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(calif = case_when(\n    educy &gt;= 0 & educy &lt;= 2 ~ \"None - General\",\n    educy &gt; 3 & educy &lt;= 7 ~ \"Secondary - Vocational\",\n    educy &gt; 7 & educy &lt;= 11 ~ \"Higher +\",\n    TRUE ~ NA_character_  # Values outside the specified ranges\n  ))\n\nCount the number of employed persons by household. Note that it is necessary to explicitly tell R to ignore missing values(NA). This is different from Stata where 1 + .= 1 (where . is “missing”). In R 1 + NA = NA (where NA means “not available”). Not adding na.rm = TRUE to aggregation functions such as sum() in Listing 5 below will not throw an error and only provide a column with NA for households where at least one individidual has an employed status of NA.\n\n\n\n\nListing 5: Employed in household\n\n\npp_microsim &lt;- pp_microsim %&gt;% \n  mutate(employed = (lstatus == 1)) %&gt;% \n  group_by(household_id) %&gt;% \n  # Count within each household\n  mutate(employed_hh = sum(employed, na.rm = TRUE)) %&gt;%   \n  ungroup() \n\n\n\n\nHere the original Stata code calculates income variables and aggregates them by household. We skip that because the dataset ic already has these elements calculated by the WB poverty team. We’ll add them later as we need them.\nHowever, as we’ll see later labor income information is heavily non-reported in the dataset. Labor income is a crucial step in merging the dataset with macroeconomic information and so we will predict income for those that do not report it below. These variables are related to labor income, amount and frequency, which we have to standardized to a monthly or yearly value.\nPrimary and Secondary Job income:\n\nemp_11 How much was %rostertitle%’s payment for wages/salary/income for last month?\nemp_12 What period of time was the wage/income for?\nemp_25 How much was %rostertitle%’s payment for wages/salary/income for last month?\nemp_26 What period of time was the wage/income for?\n\nBonus, In-Kind, and food from job was not asked in Armenia, If it were, you should add a mutate() statement like the ones below for each subcategory in Listing 6. We use coalesce(colname, 0) when adding the annual_labor_total again to prevent sums of NA’s. This function replaces a value with 0 within the calculation if it’s missing, but doesn’t change its value permanently.\n\n\n\n\nListing 6: Annualized labor income\n\n\npp_microsim &lt;- pp_microsim %&gt;% \n  # Labor income primary job\n  mutate(annual_labor_income_primary = case_when(\n    emp_12 == 1 ~ emp_11 * 365,\n    emp_12 == 2 ~ (emp_11/7) * 365,  # Assuming weekly rate \n    emp_12 == 3 ~ (emp_11/14) * 365,\n    emp_12 == 4 ~ emp_11 * 12,\n    emp_12 == 5 ~ emp_11 * 2,\n    emp_12 == 6 ~ emp_11,\n    emp_12 == 7 ~ NA\n  ))   %&gt;% \n  # Labor income secondary job\n  mutate(annual_labor_income_secondary = case_when(\n    emp_26 == 1 ~ emp_25 * 365,\n    emp_26 == 2 ~ (emp_25/7) * 365,  # Assuming weekly rate \n    emp_26 == 3 ~ (emp_25/14) * 365,\n    emp_26 == 4 ~ emp_25 * 12,\n    emp_26 == 5 ~ emp_25 * 2,\n    emp_26 == 6 ~ emp_25,\n    emp_26 == 7 ~ NA\n  )) %&gt;% \n  # Annual labor total in thousands of dram\n  mutate(annual_labor_total = \n           (coalesce(annual_labor_income_primary, 0) + \n           coalesce(annual_labor_income_secondary, 0))/1000)\n\n# Restore annual_labor_total to NA if both NA\npp_microsim &lt;- pp_microsim %&gt;% \n  mutate(annual_labor_total =\n           if_else(\n             is.na(annual_labor_income_primary)\n             & is.na(annual_labor_income_secondary),\n         NA, \n         annual_labor_total))\n\n\n\n\nNow we need to check the share of individuals that are employed, but did not report income. This is done in Listing 7 below.\n\n\n\n\nListing 7: Employed with no income reported\n\n\ntotal_employed_no_income &lt;- pp_microsim %&gt;%\n  filter(employed == TRUE & is.na(annual_labor_total)) %&gt;% \n  nrow()\n\ntotal_employed &lt;- pp_microsim %&gt;%\n  filter(employed == TRUE) %&gt;%\n  nrow()\n\npercent_employed_no_income &lt;- \n  (total_employed_no_income / total_employed) * 100\n\nprint(\n  paste0(\n    \"There is \",\n    format(\n      percent_employed_no_income,digits = 2, nsmall=2\n      ),\n    \"% of the employed population that reports no income.\")\n  )\n\n\n\n\n[1] \"There is 28.57% of the employed population that reports no income.\"\n\n\nWe also need to mark income outliers as those with incomes outside 5 standard deviations.\n\npp_microsim &lt;- pp_microsim  %&gt;% \n  mutate(\n    # Calculate standard deviation\n    sd = sd(annual_labor_total, na.rm = TRUE), \n    d = annual_labor_total / sd,                \n    # Combined outlier condition\n    outlier = (d &gt; 5) | (employed == TRUE & annual_labor_total == 0), \n    # Mark potential missings\n    missings = if_else(employed == TRUE, is.na(annual_labor_total), NA) \n  ) \n\nEconomic sector. The economic sectors dataset contains a lookup table for sector aggregation which we add to the pp_microsim database in Listing 8.\n\n\n\n\nListing 8: Sector aggregation\n\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(emp_04 = as.integer(emp_04)) %&gt;% \n  left_join(sectors, join_by(\"emp_04\" == \"economic_activity_code\") ) %&gt;% \n  rename(sector = ea_shortcode)\n\n\n\n\nSome individuals report no sector for either their primary or secondary job. In Listing 9 we find out the sector of other family members in their home and assign the sector of whoever is closest using fill( other_sector, .direction = \"downup\").\n\n\n\n\nListing 9: Assign sector to those who don’t report one\n\n\npp_microsim &lt;- pp_microsim %&gt;%\n  group_by(household_id) %&gt;%\n  mutate(\n    # Create a temporary variable 'other_sector'\n    # which captures the sector of any employed \n    # individual in the household\n    other_sector = \n      if_else(employed == TRUE & !is.na(sector), sector, NA_real_)\n  ) %&gt;%\n  # Use 'fill' to propagate 'other_sector' values within the household\n  fill(other_sector, .direction = \"downup\") %&gt;%\n  mutate(\n    # Impute missing 'sector' values based on the 'other_sector'\n    sector = \n      if_else(is.na(sector) & employed == TRUE, other_sector, sector)\n  ) %&gt;%\n  # Drop the temporary 'other_sector' variable\n  select(-other_sector) %&gt;%\n  ungroup()\n\n\n\n\nWe then assign a specific value for missing sectors for those employed with no one else in the hh to assign value. We select services as it’s the heaviest sector in the dataset (we do it like this, instead of say, any matching technique, because it’s only 2 observations).\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(sector = if_else(is.na(sector) & employed == TRUE, 3, sector))\n\nWe provide value labels for sector factors.\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(sector_name = factor(sector, levels = c(1, 2, 3),\n                         labels = c(\"Agriculture\", \n                                    \"Manufacturing\", \n                                    \"Services\")))\n\nWe make sure that those outside the labor force (OLF) do not report a sector, which we replace with NA for those who meet the condition.\n\n\n\n\nListing 10: No sector for OLF\n\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(lstatus = as.numeric(lstatus),\n         sector = \n           if_else(lstatus == 4, \n                   as.character(NA), \n                   as.character(sector)),\n         industry = as.factor(sector)) %&gt;%\n  # We need this for reweighting and \n  # not messing up the regression below.\n  mutate(sector_w = sector)\n\n\n\n\n\n\n4.3 The regression\nSince labor income was a key variable, which we needed to match with the future wage bill by economic activity, we first checked for missing values among employed individuals. We found that almost a third of respondents (28.6%) did not report income for either their primary or secondary job. To overcome this limitation, we used the available information from the remaining respondents to estimate an extended Mincer equation, as shown in Equation 1, and implemented in Listing 11. For the respondents with available information, we also identified outliers as those outside of five standard deviations from the mean labor income.\n\\[\n\\ln(lab_i) = \\beta_0 + \\beta_1 \\text{age}_i + \\beta_2 \\text{gender}_i + \\beta_3 \\text{educy}_i + \\beta_4 \\text{age}^2_i + \\beta_5 \\text{marz}_i + \\beta_6 \\text{sector}_i + \\epsilon_i\n\\tag{1}\\]\nWhere:\n\n\\(\\ln(lab_i)\\) is the natural logarithm of labor income for individual \\(i\\).\n\\(\\beta_0\\) is the intercept term.\n\\(\\beta_1, \\beta_2, \\beta_3, \\beta_4, \\beta_5, \\beta_6\\) are the coefficients for the respective independent variables.\n\\(\\text{age}_i\\) is the age of individual \\(i\\).\n\\(\\text{gender}_i\\) is a binary variable indicating the gender of individual \\(i\\) (1 for male, 2 for female).\n\\(\\text{educy}_i\\) represents the level of education for individual \\(i\\) (ordered: 1) None to General, 2) Secondary to Vocational, 3) Higher education).\n\\(\\text{age}^2_i\\) is the square of the age of individual \\(i\\), included to capture non-linear effects of age on labor income.\n\\(\\text{marz}_i\\) represents the region where individual \\(i\\) resides.\n\\(\\text{sector}_i\\) represents the sector of employment for individual \\(i\\) (i.e., agriculture, manufacturing or services).\n\\(\\epsilon_i\\) is the error term for individual \\(i\\).\n\nWe first prepare our variables for the regression.\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(\n    educy2 = educy^2,\n    age2 = age^2,\n    male = case_when(\n      gender == 1 ~ 1,\n      gender == 2 ~ 0\n    ),\n    lnlab = log(annual_labor_total),\n    simuli = NA_real_ # Initialize simuli\n  )\n\nFilter the data for regression conditions.\n\nregression_data &lt;- pp_microsim %&gt;%\n  filter(employed == TRUE & outlier == FALSE & missings == FALSE)\n\nRegression model.\n\n\n\n\nListing 11: Income regression model\n\n\nmodel &lt;- lm(lnlab ~ age + gender + educy + age2 + marz + sector, \n            data = regression_data)\n\n\n\n\nPredict for specific conditions\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(\n    condition = (lstatus == 1 & (outlier == TRUE | missings == TRUE))\n  )\n\nApplying predictions.\nNote: The ‘predict’ function in R does not directly support conditions within the function call, so we handle this by filtering or subsetting the data as needed.\ntemp2 equivalent - Note: ‘type = “response”’ might be needed depending on model type.\n\npp_microsim$simuli[pp_microsim$condition==TRUE] &lt;- exp(\n  predict(model, pp_microsim[pp_microsim$condition==TRUE, ], type = \"response\"))\n\nHandling negative values in ‘simuli’.\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(\n    simuli = if_else(simuli &lt; 0, 0, simuli)\n  )\n\nThere were 8 observations that met the criteria:\nWe will replace annual_labor_total with this value for those observations.\n\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(annual_labor_total = if_else(\n    employed == TRUE & (outlier == TRUE | missings == TRUE),\n    simuli, annual_labor_total))\n\n# And get monthly incomes for everyone\npp_microsim &lt;- pp_microsim %&gt;% \n  mutate(monthly_labor_income = annual_labor_total / 12)\n\nMerging datasets.\n\npp_microsim &lt;- pp_microsim %&gt;%\n  left_join(poverty_designations, by = \"household_id\")\n\n\n\n4.4 Total income and shares\nTotal labor income at HH level.\n\npp_microsim &lt;- pp_microsim %&gt;%\n  group_by(household_id) %&gt;%\n  mutate(lab_hh = sum(annual_labor_total, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nMonthly incomes come from the ic data set.\n\nincomes &lt;- ic %&gt;% \n  select(household_id, inc1, inc2, inc3, inc4, inc5, inc6, inc7, inc8)\n\nTotal income at HH level (the commented out portion was a less efficient way of accomplishing the same result of coalescing NAs to 0 so that the sum can be performed). Note that here we need to use the magittr pipe %&gt;% instead of the newer Native Pipe %&gt;% , because we need to reference the correct scope with the dot ..\n\npp_microsim &lt;- pp_microsim %&gt;%\n  left_join(incomes, by = c(\"household_id\" = \"household_id\")) %&gt;%\n  mutate(across(inc5:inc8, ~replace_na(., 0))) %&gt;%\n  mutate(nli_hh = 12 * rowSums(select(., inc5:inc8), na.rm = TRUE)) %&gt;%\n  mutate(income_hh = lab_hh + nli_hh)\n\n# pp_microsim &lt;- pp_microsim %&gt;%\n#   left_join(incomes, join_by(household_id == household_id)) %&gt;% \n#   mutate(nli_hh = (  coalesce(inc5) + \n#                      coalesce(inc6) +\n#                      coalesce(inc7) +\n#                      coalesce(inc8)) * 12) %&gt;% \n#   mutate(income_hh = lab_hh + nli_hh)\n\nFinal subset of data.\n\npp_microsim &lt;- pp_microsim %&gt;%\n  select(household_id, person_id, industry, salaried,\n         rural_dummy, hhsize,hhsize_R, marz_no, aepc, weight, \n         Foodpovln2022, Lpovln2022, Upovln2022, Avpovln2022, \n         poor_Foodpovln2022, poor_Lpovln2022, poor_Upovln2022, \n         poor_Avpovln2022, decile, settlement, urban_rural,\n         gender, age, head, rel, educy, calif, sector, sector_name,\n         annual_labor_total,annual_labor_income_primary,\n         annual_labor_income_secondary,monthly_labor_income,\n         lstatus, sector_w, marz.x ) %&gt;%\n  rename(marz = marz.x)\n\n# Exporting to Stata (might be necessary for reweigthing with wentropy)\n# write_dta(pp_microsim, path = \"outputs/pp_microsim.dta\", version = 10)",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  },
  {
    "objectID": "supporting-materials/microsimulation.html#un-population-projections",
    "href": "supporting-materials/microsimulation.html#un-population-projections",
    "title": "Armenia CCDR Microsimulation",
    "section": "5 UN Population Projections",
    "text": "5 UN Population Projections\nNow we are ready to move to our demographic projections and macroeconomic model information.\nFirst, filtering based on country (our iso variable).\n\npopulation_projections &lt;- population_projections  %&gt;%  \n  filter(country == iso)\n\nCollapsing data by summing up variables starting with “yf” and “ym” and reshaping data to long format.\n\npopulation_projections &lt;- population_projections %&gt;%\n  group_by(Variant, country, cohort) %&gt;%\n  summarize(across(starts_with(c(\"yf\", \"ym\")), sum)) %&gt;%\n  ungroup()\n\npopulation_projections &lt;- pivot_longer(population_projections,\n                              cols = starts_with(c(\"yf\", \"ym\")),\n                              names_to = c(\".value\", \"year\"),\n                              names_pattern = \"(yf|ym)(.*)\")\n\nCreating new variable total_population as the sum of yf and ym. Dropping country variables.\n\npopulation_projections &lt;- population_projections %&gt;%\n  mutate(total_population = yf + ym) %&gt;%\n  select( -country) %&gt;% \n  mutate(year = as.numeric(year))\n\nSummarizing the year to find the range.\n\nminyear &lt;- survey_year # Make sure `survey_year` is correctly defined\nmaxyear &lt;- max(as.numeric(population_projections$year))\n\nWe have that the “Min Year” is 2022 and the “Max Year” is 2100. Now we create a population growth rate by demographic variant dataset. We initialize an empty list to store our data by variant and we loop over variants to create this list.\n\n# With minyear and maxyear defined above\n# Initialize a list to store growth data\npop_growth &lt;- list()\n\n# Loop over variants\nvariants &lt;- unique(population_projections$Variant)\nfor (variant in variants) {\n  for (t in minyear:maxyear) {\n    \n    # Calculate population for year t\n    pop_t &lt;- population_projections %&gt;%\n      filter(year == t, Variant == variant) %&gt;%\n      summarize(sum_pop = sum(total_population)) %&gt;%\n      pull(sum_pop)\n    \n    # Calculate population for base year\n    pop_base &lt;- population_projections %&gt;%\n      filter(year == minyear, Variant == variant) %&gt;%\n      summarize(sum_pop = sum(total_population)) %&gt;%\n      pull(sum_pop)\n    \n    # Calculate growth rate and store in list with dynamic naming\n    growth_rate &lt;- pop_t / pop_base\n    pop_growth[[paste0(t, \"_\", variant)]] &lt;- list(\n      growth_rate = growth_rate, pop_t = pop_t\n      )\n  }\n}\n\nWith the list ready, we convert back to dataframe by stitching the list elements one on top of the other.\n\n# Convert list to dataframe\npop_growth &lt;- do.call(rbind, lapply(names(pop_growth), function(x) {\n  # Extract year and variant from the name\n  parts &lt;- unlist(strsplit(x, \"_\"))\n  year &lt;- as.integer(parts[1])\n  variant &lt;- parts[2]\n  \n  # Create a tibble for each entry\n  tibble(year = year, \n         variant = variant, \n         total_population = pop_growth[[x]]$pop_t,\n         pop_growth_rate = pop_growth[[x]]$growth_rate)\n}))\n\n# Arrange the dataframe for better readability\npop_growth &lt;- arrange(pop_growth, variant, year)\n\n# Display the first few rows of the dataframe\npop_growth[c(1:09),]\n\n# A tibble: 9 × 4\n   year variant            total_population pop_growth_rate\n  &lt;int&gt; &lt;chr&gt;                         &lt;dbl&gt;           &lt;dbl&gt;\n1  2022 Constant-fertility            2780.           1    \n2  2023 Constant-fertility            2778.           0.999\n3  2024 Constant-fertility            2778.           0.999\n4  2025 Constant-fertility            2776.           0.998\n5  2026 Constant-fertility            2774.           0.998\n6  2027 Constant-fertility            2770.           0.996\n7  2028 Constant-fertility            2766.           0.995\n8  2029 Constant-fertility            2761.           0.993\n9  2030 Constant-fertility            2755.           0.991",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  },
  {
    "objectID": "supporting-materials/microsimulation.html#macro-scenarios",
    "href": "supporting-materials/microsimulation.html#macro-scenarios",
    "title": "Armenia CCDR Microsimulation",
    "section": "6 Macro Scenarios",
    "text": "6 Macro Scenarios\nThe following code snippets accomplish the following:\n\nImport data from Excel sheets corresponding to each scenario and combine them into one data frame.\nRename columns, create a ‘scenid’ to identify scenarios, and merge with population projections.\nCalculate real wages and consumption per capita.\n\nHere we use the Excel tab names to create the names of the scenarios going forward, with a previous cleaning in which we convert names to lower case, replace spaces and special characters with underscores, we remove the word scenario from the name, and remove leading or trailing spaces or underscores.\n\n# Macro Scenario File imported in \"Datasets\" section (scenario_file) \nsheets &lt;- excel_sheets(scenario_file)\nscenario_sheets &lt;- sheets[c(1,2,3)]\n\n# Define the names of the scenarios and the variants\n# modify list with the tab numbers in the Excel file\nscenarios &lt;- scenario_sheets %&gt;%\n  # Convert all text to lowercase\n  str_to_lower() %&gt;%  \n  # Replace all spaces and hyphens with underscores\n  str_replace_all(\"[ -]\", \"_\") %&gt;%\n  # Remove the word 'scenario' or 'scenarios'\n  str_remove_all(\"scenario?s?\") %&gt;%\n  # Remove leading and trailing underscores\n  str_replace_all(\"^_+|_+$\", \"\")  \n\nOur scenarios are: Our scenarios are: baseline, dry_hot, and nzs..\n\n# Create an empty list to store data frames for each scenario\nscen_data_list &lt;- list()\n\n# Import data for each scenario and store it in the list\nfor (i in seq_along(scenarios)) {\n  sheet_data &lt;- read_excel(scenario_file, \n                           sheet = scenario_sheets[i], \n                           range = \"B3:AT31\",\n                           col_names = FALSE)\n  sheet_data$scenario_id &lt;- scenarios[i]\n  colnames(sheet_data) &lt;- scenario_varlist$var_short_name\n  scen_data_list[[i]] &lt;- sheet_data\n}\n\n# Combine all data frames into one\ncombined_data &lt;- bind_rows(scen_data_list)\n\n# Rename population_m from the data set because we will use \n# UN pop projections from the other data set.\ncombined_data &lt;- combined_data %&gt;% \n  rename(population_m_macrodata = population_m)\n\n# Calculate real wages\ncombined_data &lt;- combined_data %&gt;%\n  mutate(rwage_agr_m_amd = wage_agr_m_amd / cpi,\n         rwage_man_m_amd = wage_man_m_amd / cpi,\n         rwage_ser_m_amd = wage_ser_m_amd / cpi)\n\npop_data &lt;- population_projections %&gt;% \n  group_by(Variant, year) %&gt;% \n  summarize(female = sum(yf),\n            male = sum(ym),\n            total_population = sum(total_population) ) %&gt;% \n  ungroup()\n\n# Filter population data to macro model years\npop_data &lt;- pop_data %&gt;% \n  filter(year &lt;= max(combined_data$year),\n         Variant == variants[7])\n# Merge the combined data with population projections\nmacro_data &lt;- combined_data %&gt;%\n  left_join(pop_data, by = c(\"year\"))\n\n# Calculate consumption per capita and other totals\nmacro_data &lt;- macro_data %&gt;%\n  mutate(\n    consumption_pc = consumption_b_amd / (total_population),\n    total_employment = lab_agr_1000p + lab_man_1000p + lab_ser_1000p,\n    employment_rate = working_age_pop_m / total_population\n    )\n\n# Function to add growth rate columns directly in the dataframe\ncalculate_growth &lt;- function(data, value_column) {\n  growth_col_name &lt;- paste0(value_column, \"_growth\") # dynamic name for growth column\n  data %&gt;%\n    arrange(year) %&gt;%\n    group_by(Variant, scenario_id) %&gt;%\n    mutate(\n      base_value = first(!!sym(value_column)),\n      !!sym(growth_col_name) := !!sym(value_column) / base_value\n    ) %&gt;%\n    select(-base_value) %&gt;% # optionally remove base_value column if not needed\n    ungroup()\n}\n\n# Columns to calculate growth for\nvalue_columns &lt;- c(\n  \"gdp_b_amd\",           # GDP\n  \"consumption_b_amd\",   # Consumption\n  \"consumption_pc\",      # Consumption PC\n  \"remittances_b_amd\",   # Remittances\n  \"total_employment\",    # Employment\n  \"employment_rate\",     # Employment rate\n  \"working_age_pop_m\",   # Working age population\n  \"va_agr_b_amd\",        # Value added agriculture\n  \"va_man_b_amd\",        # Value added manufacturing\n  \"va_ser_b_amd\",        # Value added services\n  \"wage_agr_m_amd\",      # Nominal wage agriculture\n  \"wage_man_m_amd\",      # Nominal wage manufacturing\n  \"wage_ser_m_amd\",      # Nominal wage services\n  \"rwage_agr_m_amd\",     # Real wage agriculture\n  \"rwage_man_m_amd\",     # Real wage manufacturing\n  \"rwage_ser_m_amd\"      # Real wage services\n  )\n\n# Applying the growth calculation to the macro_data for each column\nfor (col in value_columns) {\n  macro_data &lt;- calculate_growth(macro_data, col)\n}\n\n# Now `macro_data` will have growth rate columns for each of the variables listed\n# We rearrange the dataset for clarity\nmacro_data &lt;- macro_data %&gt;% \n  relocate(scenario_id, Variant, .before = year) %&gt;% \n  arrange(scenario_id, Variant, year)\n\n# write.table(macro_data, \"clipboard\", sep=\"\\t\", row.names=FALSE)",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  },
  {
    "objectID": "supporting-materials/microsimulation.html#reweighting-of-the-dataset",
    "href": "supporting-materials/microsimulation.html#reweighting-of-the-dataset",
    "title": "Armenia CCDR Microsimulation",
    "section": "7 Reweighting of the dataset",
    "text": "7 Reweighting of the dataset\n\n7.1 Aggregation of population data\nThis is based on a custom command to reweight the survey according to macroeconomic data for every possible combination of variant, year, and country. In the macro data we know they only used the “medium” variant and we only need to reweight for a specific year (2030) for Armenia (ARM), so we will conduct the reweighting directly with these parameters.\n\n# We join several cohorts from 0 to 29 years old and from\n# 60 onwards, because the reweighting procedure works\n# best if each category is at least 5% of the population\n# The solution here works best for Armenia.\n\npopulation_projections &lt;- population_projections %&gt;%\n  # filter(Variant == \"Medium\") %&gt;%\n  # Recoding cohorts into ordered factors\n    mutate(cohort_short = factor(case_when(\n    cohort %in% c(\"P0004\", \"P0509\",\"P1014\",\n                  \"P1519\",\"P2024\", \"P2529\") ~ \"P0029\",\n    cohort %in% c(\"P3034\", \"P3539\") ~ \"P3039\",\n    cohort %in% c(\"P4044\", \"P4549\") ~ \"P4049\",\n    cohort %in% c(\"P5054\", \"P5559\") ~ \"P5059\",\n    cohort %in% c(\"P6064\", \"P6569\",\"P7074\", \"P7579\",\n                  \"P8084\", \"P8589\", \"P9094\", \"P9599\",\n                  \"P100up\") ~ \"P60up\"\n  ), levels = c(\"P0029\", \"P3039\",\n                \"P4049\", \"P5059\", \"P60up\"))) %&gt;%\n\n  # Convert factor 'cohort' to numeric codes\n  mutate(cohort_code = as.integer(cohort_short))\n\n# Checking the resulting dataset\nprint(pop_data)\n\n# A tibble: 60 × 5\n   Variant  year female  male total_population\n   &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;            &lt;dbl&gt;\n 1 Medium   1991  1867. 1750.            3618.\n 2 Medium   1992  1850. 1724.            3575.\n 3 Medium   1993  1799. 1658.            3457.\n 4 Medium   1994  1763. 1610.            3374.\n 5 Medium   1995  1741. 1581.            3323.\n 6 Medium   1996  1731. 1568.            3299.\n 7 Medium   1997  1719. 1552.            3271.\n 8 Medium   1998  1705. 1535.            3241.\n 9 Medium   1999  1689. 1517.            3206.\n10 Medium   2000  1672. 1496.            3169.\n# ℹ 50 more rows\n\n\nLet’s now create cohorts in our pp_microsim data to match our population projection data.\n\n# Convert 'age' into 'cohort' factor with levels ordered as specified\npp_microsim &lt;- pp_microsim %&gt;%\n    mutate(cohort = factor(case_when(\n    age &gt;= 0  & age &lt;= 29 ~ \"P0029\",\n    age &gt;= 30 & age &lt;= 39 ~ \"P3039\",\n    age &gt;= 40 & age &lt;= 49 ~ \"P4049\",\n    age &gt;= 50 & age &lt;= 59 ~ \"P5059\",\n    age &gt;= 60  ~ \"P60up\"\n  ), levels = c(\"P0029\", \"P3039\", \"P4049\", \"P5059\", \"P60up\")))\n\n# Convert the 'cohort' and 'gender' factor to numeric codes\npp_microsim &lt;- pp_microsim %&gt;%\n  mutate(cohort_code = as.integer(cohort)) %&gt;% \n  mutate(gender_code = as.integer(gender))\n\nWe also need demographic targets for 2030\n\n# Ensure pop_targets_2030 is correctly prepared\n# We use the \"Medium\" variant = variants[7]\npop_targets_2030 &lt;- population_projections  %&gt;% \n  filter(year == 2030, Variant == variants[7])  %&gt;% \n  group_by(cohort_code, cohort_short) %&gt;% \n    summarize(female = sum(yf),\n              male   = sum(ym), \n              total = sum(total_population),\n              ) %&gt;%\n  ungroup()\n\npop_total &lt;- sum(pop_targets_2030$total)\n\npop_targets_2030 &lt;- pop_targets_2030 %&gt;% \n  mutate(pct_total = total / pop_total)\n\n#writeClipboard(pop_targets_2030)\n# write.table(pop_targets_2030, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nAnd economic targets from our macroeconomic scenario data. We deal with this later. Should come back to fix this so we can automate.\n\n# economic_targets_2030 &lt;- macro_data %&gt;%\n#   filter(year == 2030, Variant == \"Medium\", scenario_id == \"baseline\") %&gt;%\n#   summarize(\n#     target_lab_agr = sum(lab_agr_1000p * 1000),\n#     target_lab_man = sum(lab_man_1000p * 1000),\n#     target_lab_ser = sum(lab_ser_1000p * 1000)\n#  )\n\nFor a better representation of the labor market, we will take into account the combination between labor status and economic sector of the employed and adjust that combination according to the macrodata so that we can accurately model changes in total employment, sector distribution of the employed and overall population changes.\n\npp_microsim &lt;- pp_microsim %&gt;% \n  mutate(lmarket = case_when(\n    lstatus == 1 & sector_w  == 1  ~ 1,   # Agriculture\n    lstatus == 1 & sector_w  == 2  ~ 2,   # Manufactures\n    lstatus == 1 & sector_w  == 3  ~ 3,   # Services\n    lstatus == 2 & is.na(sector_w) ~ 4,   # Unemployed\n    lstatus == 3 & is.na(sector_w) ~ 4,   # Unemployed\n    lstatus == 4 & is.na(sector_w) ~ 5,   # OLF\n    \n  ))\n\nNote that the differences between the totals of the survey and the macro file for the base year are very much different. We’ll adjust the survey only with relative growth instead of total numbers so that labor income doesn’t change completely.\n\n\n7.2 Reweigting\nWe use anesrake to calculate targets from known future proportions of sex, age, economic sector. We first create a target list.\n\n# Target for each variable\n\ngender_code &lt;- c(\n  sum(pop_targets_2030$male)   / \n    (sum(pop_targets_2030$male)+ sum(pop_targets_2030$female)), \n  sum(pop_targets_2030$female) / \n    (sum(pop_targets_2030$male)+ sum(pop_targets_2030$female)))\n\ncohort_code &lt;- pop_targets_2030$pct_total\n\n# Four digits are better than two in this case, raking is quite accurate.\nlmarket_baseline &lt;- c(0.1342, 0.0494, 0.2611, 0.2473, 0.3080)\nlmarket_dry_hot  &lt;- c(0.1369, 0.0489, 0.2593, 0.2473, 0.3076)\nlmarket_nzs      &lt;- c(0.1251, 0.0516, 0.2623, 0.2516, 0.3094)\n# Note how similar the scenarios are\n\n# Target list baseline\ntargets_baseline &lt;- list(gender_code\n                , cohort_code\n                , lmarket_baseline\n                )\n\nnames(targets_baseline) &lt;- c(\"gender_code\", \n                    \"cohort_code\", \n                    \"lmarket\"\n                    )\n\n# Target list Dry/Hot\ntargets_dry_hot &lt;- list(gender_code\n                , cohort_code\n                , lmarket_dry_hot\n                )\n\nnames(targets_dry_hot) &lt;- c(\"gender_code\", \n                    \"cohort_code\", \n                    \"lmarket\"\n                    )\n\n# Target list NZS\ntargets_nzs &lt;- list(gender_code\n                , cohort_code\n                , lmarket_nzs\n                )\n\nnames(targets_nzs) &lt;- c(\"gender_code\", \n                    \"cohort_code\", \n                    \"lmarket\"\n                    )\n\nAnd now we perform the reweighting, using the original weights. Initially we had used the default option type = “pctlim” combined with pctlim=0.05, because the method recommends that if reweighting changes for one variable according to its target are not of at least 5%, then it’s not worth burdening the procedure with it. It then ignored sex as a reweighting variable, leaving a small percentage difference between the target and the final population. However, we then tried removing this limitation and the procedure reached convergence in 33 iterations very efficiently.\n\n# Since this uses base R, we need to turn the data frame into base R object\nrakedata &lt;- as.data.frame(pp_microsim)\n\nanesrake::anesrakefinder(targets_baseline, rakedata, choosemethod = \"total\")\n\ngender_code cohort_code     lmarket \n 0.03626510  0.09677000  0.07172418 \n\noutsave &lt;- anesrake::anesrake(targets_baseline, \n                    rakedata, \n                    caseid = rakedata$person_id, \n                    #verbose = FALSE,\n                    choosemethod = \"total\",\n                    #type = \"pctlim\",\n                    type = \"nolim\",\n                    #cap = 100,\n                    #pctlim = 0.05,\n                    nlim = 3,\n                    iterate = TRUE,\n                    force1 = TRUE,\n                    verbose = TRUE,\n                    weightvec = rakedata$weight)\n\n[1] \"Raking...Iteration 1\"\n[1] \"Current iteration changed total weights by 2361.17708851471\"\n[1] \"Raking...Iteration 2\"\n[1] \"Current iteration changed total weights by 343.154685460483\"\n[1] \"Raking...Iteration 3\"\n[1] \"Current iteration changed total weights by 50.282147123565\"\n[1] \"Raking...Iteration 4\"\n[1] \"Current iteration changed total weights by 15.5324150051193\"\n[1] \"Raking...Iteration 5\"\n[1] \"Current iteration changed total weights by 4.9125427648284\"\n[1] \"Raking...Iteration 6\"\n[1] \"Current iteration changed total weights by 1.55433042098091\"\n[1] \"Raking...Iteration 7\"\n[1] \"Current iteration changed total weights by 0.491570475878754\"\n[1] \"Raking...Iteration 8\"\n[1] \"Current iteration changed total weights by 0.15548860353216\"\n[1] \"Raking...Iteration 9\"\n[1] \"Current iteration changed total weights by 0.0491861537407502\"\n[1] \"Raking...Iteration 10\"\n[1] \"Current iteration changed total weights by 0.0155594639669022\"\n[1] \"Raking...Iteration 11\"\n[1] \"Current iteration changed total weights by 0.00492206801523239\"\n[1] \"Raking...Iteration 12\"\n[1] \"Current iteration changed total weights by 0.0015570434063234\"\n[1] \"Raking...Iteration 13\"\n[1] \"Current iteration changed total weights by 0.000492553986909977\"\n[1] \"Raking...Iteration 14\"\n[1] \"Current iteration changed total weights by 0.00015581417334512\"\n[1] \"Raking...Iteration 15\"\n[1] \"Current iteration changed total weights by 4.92901435997922e-05\"\n[1] \"Raking...Iteration 16\"\n[1] \"Current iteration changed total weights by 1.55924099685123e-05\"\n[1] \"Raking...Iteration 17\"\n[1] \"Current iteration changed total weights by 4.9324913638793e-06\"\n[1] \"Raking...Iteration 18\"\n[1] \"Current iteration changed total weights by 1.56034048766351e-06\"\n[1] \"Raking...Iteration 19\"\n[1] \"Current iteration changed total weights by 4.93596604533852e-07\"\n[1] \"Raking...Iteration 20\"\n[1] \"Current iteration changed total weights by 1.56144157131832e-07\"\n[1] \"Raking...Iteration 21\"\n[1] \"Current iteration changed total weights by 4.93947955038099e-08\"\n[1] \"Raking...Iteration 22\"\n[1] \"Current iteration changed total weights by 1.56252071836782e-08\"\n[1] \"Raking...Iteration 23\"\n[1] \"Current iteration changed total weights by 4.94344779000677e-09\"\n[1] \"Raking...Iteration 24\"\n[1] \"Current iteration changed total weights by 1.56241772297783e-09\"\n[1] \"Raking...Iteration 25\"\n[1] \"Current iteration changed total weights by 4.95822036206128e-10\"\n[1] \"Raking...Iteration 26\"\n[1] \"Current iteration changed total weights by 1.56211973911802e-10\"\n[1] \"Raking...Iteration 27\"\n[1] \"Current iteration changed total weights by 5.09479958221704e-11\"\n[1] \"Raking...Iteration 28\"\n[1] \"Current iteration changed total weights by 1.42659634105868e-11\"\n[1] \"Raking...Iteration 29\"\n[1] \"Current iteration changed total weights by 6.37030705963326e-12\"\n[1] \"Raking...Iteration 30\"\n[1] \"Current iteration changed total weights by 2.28768393117917e-12\"\n[1] \"Raking...Iteration 31\"\n[1] \"Current iteration changed total weights by 2.2706836411146e-12\"\n[1] \"Raking converged in 31 iterations\"\n\n#summary(outsave)\n\n# add weights to the dataset\n\nrakedata$weight_2030_baseline  &lt;- unlist(outsave[1])\nn  &lt;- length(rakedata$sector)\n\n# Calculate the sum of original weights\noriginal_weight_sum &lt;- sum(rakedata$weight)\n\n# # Target scaling for original weights\n\noriginal_weight_scaling_factor &lt;-\n  pop_data$total_population[pop_data$year == 2030] /\n  pop_data$total_population[pop_data$year == 2022]\n\n# Scaled original weights\noriginal_weight_sum &lt;- (original_weight_sum \n                        * original_weight_scaling_factor)\n\n# Calculate the sum of the new weights\nnew_weight_sum &lt;- sum(rakedata$weight_2030_baseline)\n\n# Scale the new weights to match the sum of the original weights\nscaling_factor &lt;- original_weight_sum / new_weight_sum\nrakedata$weight_2030_baseline &lt;- rakedata$weight_2030_baseline * scaling_factor\n\n# Verify the adjustment\nhead(rakedata[, c(\"weight\", \"weight_2030_baseline\")])\n\n    weight weight_2030_baseline\n1 185.7685             175.0700\n2 185.7685             188.2168\n3 122.7176             101.5832\n4 185.7685             191.8185\n5 326.8796             264.5755\n6 326.8796             337.5253\n\nsummary(rakedata$weight_2030_baseline)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.87  102.86  142.59  153.52  194.69  519.78 \n\nsummary(rakedata$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.48  109.82  156.33  154.68  192.16  326.88 \n\nhh_size &lt;- rakedata %&gt;% \n  select(household_id, hhsize) %&gt;% \n  mutate(ones = 1,\n         hhsize_old = hhsize) %&gt;% \n  group_by(household_id) %&gt;% \n  summarize(hhsize = sum(ones, na.rm = TRUE)) %&gt;% \n  ungroup()\n\nrakedata &lt;- rakedata %&gt;%\n  rename(hhsize_old = hhsize) %&gt;% \n  left_join(hh_size, join_by(household_id)) %&gt;% \n  relocate(weight, .before = weight_2030_baseline) %&gt;% \n  mutate(hh_weight_2030_baseline = weight_2030_baseline / hhsize)\n\npp_microsim &lt;- tibble(rakedata)\nrm(rakedata)\n\nWe now do the Dry/Hot Scenario. The efficient way of doing this is through a loop or sapply, but as we’re strapped for time we will just repeat the code. (Needs rework.)\n\n# Since this uses base R, we need to turn the data frame into base R object\nrakedata &lt;- as.data.frame(pp_microsim)\n\nanesrake::anesrakefinder(targets_dry_hot, rakedata, choosemethod = \"total\")\n\ngender_code cohort_code     lmarket \n 0.03626510  0.09677000  0.06632418 \n\noutsave &lt;- anesrake::anesrake(targets_dry_hot, \n                    rakedata, \n                    caseid = rakedata$person_id, \n                    #verbose = FALSE,\n                    choosemethod = \"total\",\n                    #type = \"pctlim\",\n                    type = \"nolim\",\n                    #cap = 100,\n                    #pctlim = 0.05,\n                    nlim = 3,\n                    iterate = TRUE,\n                    force1 = TRUE,\n                    verbose = TRUE,\n                    weightvec = rakedata$weight)\n\n[1] \"Raking...Iteration 1\"\n[1] \"Current iteration changed total weights by 2346.39562345486\"\n[1] \"Raking...Iteration 2\"\n[1] \"Current iteration changed total weights by 325.326676946962\"\n[1] \"Raking...Iteration 3\"\n[1] \"Current iteration changed total weights by 47.6038687445994\"\n[1] \"Raking...Iteration 4\"\n[1] \"Current iteration changed total weights by 14.7805325726215\"\n[1] \"Raking...Iteration 5\"\n[1] \"Current iteration changed total weights by 4.67536358935004\"\n[1] \"Raking...Iteration 6\"\n[1] \"Current iteration changed total weights by 1.48066518168051\"\n[1] \"Raking...Iteration 7\"\n[1] \"Current iteration changed total weights by 0.468649510562274\"\n[1] \"Raking...Iteration 8\"\n[1] \"Current iteration changed total weights by 0.148353295202568\"\n[1] \"Raking...Iteration 9\"\n[1] \"Current iteration changed total weights by 0.046965177901018\"\n[1] \"Raking...Iteration 10\"\n[1] \"Current iteration changed total weights by 0.0148683286004776\"\n[1] \"Raking...Iteration 11\"\n[1] \"Current iteration changed total weights by 0.00470705840641751\"\n[1] \"Raking...Iteration 12\"\n[1] \"Current iteration changed total weights by 0.00149017464214521\"\n[1] \"Raking...Iteration 13\"\n[1] \"Current iteration changed total weights by 0.000471763960322724\"\n[1] \"Raking...Iteration 14\"\n[1] \"Current iteration changed total weights by 0.000149352453236176\"\n[1] \"Raking...Iteration 15\"\n[1] \"Current iteration changed total weights by 4.72824469837235e-05\"\n[1] \"Raking...Iteration 16\"\n[1] \"Current iteration changed total weights by 1.49688183763291e-05\"\n[1] \"Raking...Iteration 17\"\n[1] \"Current iteration changed total weights by 4.7388742776544e-06\"\n[1] \"Raking...Iteration 18\"\n[1] \"Current iteration changed total weights by 1.50024780687374e-06\"\n[1] \"Raking...Iteration 19\"\n[1] \"Current iteration changed total weights by 4.74952141560347e-07\"\n[1] \"Raking...Iteration 20\"\n[1] \"Current iteration changed total weights by 1.50361528755694e-07\"\n[1] \"Raking...Iteration 21\"\n[1] \"Current iteration changed total weights by 4.76025212442499e-08\"\n[1] \"Raking...Iteration 22\"\n[1] \"Current iteration changed total weights by 1.50707414187101e-08\"\n[1] \"Raking...Iteration 23\"\n[1] \"Current iteration changed total weights by 4.77042699786878e-09\"\n[1] \"Raking...Iteration 24\"\n[1] \"Current iteration changed total weights by 1.50992897351987e-09\"\n[1] \"Raking...Iteration 25\"\n[1] \"Current iteration changed total weights by 4.77459433101401e-10\"\n[1] \"Raking...Iteration 26\"\n[1] \"Current iteration changed total weights by 1.5339025882799e-10\"\n[1] \"Raking...Iteration 27\"\n[1] \"Current iteration changed total weights by 4.58121596214056e-11\"\n[1] \"Raking...Iteration 28\"\n[1] \"Current iteration changed total weights by 1.52340223769087e-11\"\n[1] \"Raking...Iteration 29\"\n[1] \"Current iteration changed total weights by 6.39277519809411e-12\"\n[1] \"Raking...Iteration 30\"\n[1] \"Current iteration changed total weights by 1.47665213390269e-12\"\n[1] \"Raking...Iteration 31\"\n[1] \"Current iteration changed total weights by 4.05270261794044e-12\"\n[1] \"Raking converged in 31 iterations\"\n\n#summary(outsave)\n\n# add weights to the dataset\n\nrakedata$weight_2030_dry_hot  &lt;- unlist(outsave[1])\n\n# Calculate the sum of original weights\noriginal_weight_sum &lt;- sum(rakedata$weight)\n\n# Target scaling for original weights\n\noriginal_weight_scaling_factor &lt;-\n  pop_data$total_population[pop_data$year == 2030] /\n  pop_data$total_population[pop_data$year == 2022]\n\n# Scaled original weights\noriginal_weight_sum &lt;- (original_weight_sum \n                        * original_weight_scaling_factor)\n\n# Calculate the sum of the new weights\nnew_weight_sum &lt;- sum(rakedata$weight_2030_dry_hot)\n\n# Scale the new weights to match the sum of the original weights\nscaling_factor &lt;- original_weight_sum / new_weight_sum\nrakedata$weight_2030_dry_hot &lt;- rakedata$weight_2030_dry_hot * scaling_factor\n\n# Verify the adjustment\nhead(rakedata[, c(\"weight\", \"weight_2030_dry_hot\")])\n\n    weight weight_2030_dry_hot\n1 185.7685            174.1517\n2 185.7685            188.2564\n3 122.7176            101.8126\n4 185.7685            190.5865\n5 326.8796            263.8479\n6 326.8796            335.3575\n\nsummary(rakedata$weight_2030_dry_hot)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.84  102.38  143.01  153.52  194.73  518.52 \n\nsummary(rakedata$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.48  109.82  156.33  154.68  192.16  326.88 \n\nrakedata &lt;- rakedata %&gt;% \n  mutate(hh_weight_2030_dry_hot = weight_2030_dry_hot / hhsize)\n\npp_microsim &lt;- tibble(rakedata)\nrm(rakedata)\n\nLet’s add the NZS scenario\n\n# Since this uses base R, we need to turn the data frame into base R object\nrakedata &lt;- as.data.frame(pp_microsim)\n\nanesrake::anesrakefinder(targets_nzs, rakedata, choosemethod = \"total\")\n\ngender_code cohort_code     lmarket \n 0.03626510  0.09677000  0.08132418 \n\noutsave &lt;- anesrake::anesrake(targets_nzs, \n                    rakedata, \n                    caseid = rakedata$person_id, \n                    #verbose = FALSE,\n                    choosemethod = \"total\",\n                    #type = \"pctlim\",\n                    type = \"nolim\",\n                    #cap = 100,\n                    #pctlim = 0.05,\n                    nlim = 3,\n                    iterate = TRUE,\n                    force1 = TRUE,\n                    verbose = TRUE,\n                    weightvec = rakedata$weight)\n\n[1] \"Raking...Iteration 1\"\n[1] \"Current iteration changed total weights by 2405.38337714935\"\n[1] \"Raking...Iteration 2\"\n[1] \"Current iteration changed total weights by 381.11433942213\"\n[1] \"Raking...Iteration 3\"\n[1] \"Current iteration changed total weights by 66.9715000528318\"\n[1] \"Raking...Iteration 4\"\n[1] \"Current iteration changed total weights by 20.6776651602141\"\n[1] \"Raking...Iteration 5\"\n[1] \"Current iteration changed total weights by 6.53725119305366\"\n[1] \"Raking...Iteration 6\"\n[1] \"Current iteration changed total weights by 2.0599945856778\"\n[1] \"Raking...Iteration 7\"\n[1] \"Current iteration changed total weights by 0.649111380457645\"\n[1] \"Raking...Iteration 8\"\n[1] \"Current iteration changed total weights by 0.204577406691525\"\n[1] \"Raking...Iteration 9\"\n[1] \"Current iteration changed total weights by 0.0644801211551565\"\n[1] \"Raking...Iteration 10\"\n[1] \"Current iteration changed total weights by 0.0203235937837268\"\n[1] \"Raking...Iteration 11\"\n[1] \"Current iteration changed total weights by 0.00640584165249312\"\n[1] \"Raking...Iteration 12\"\n[1] \"Current iteration changed total weights by 0.00201907300818058\"\n[1] \"Raking...Iteration 13\"\n[1] \"Current iteration changed total weights by 0.000636396628444766\"\n[1] \"Raking...Iteration 14\"\n[1] \"Current iteration changed total weights by 0.000200587436619007\"\n[1] \"Raking...Iteration 15\"\n[1] \"Current iteration changed total weights by 6.3223652922148e-05\"\n[1] \"Raking...Iteration 16\"\n[1] \"Current iteration changed total weights by 1.99276214380567e-05\"\n[1] \"Raking...Iteration 17\"\n[1] \"Current iteration changed total weights by 6.28103589818407e-06\"\n[1] \"Raking...Iteration 18\"\n[1] \"Current iteration changed total weights by 1.97973583832001e-06\"\n[1] \"Raking...Iteration 19\"\n[1] \"Current iteration changed total weights by 6.23997950852107e-07\"\n[1] \"Raking...Iteration 20\"\n[1] \"Current iteration changed total weights by 1.96680178823905e-07\"\n[1] \"Raking...Iteration 21\"\n[1] \"Current iteration changed total weights by 6.19911701138509e-08\"\n[1] \"Raking...Iteration 22\"\n[1] \"Current iteration changed total weights by 1.95397621555182e-08\"\n[1] \"Raking...Iteration 23\"\n[1] \"Current iteration changed total weights by 6.15833799233467e-09\"\n[1] \"Raking...Iteration 24\"\n[1] \"Current iteration changed total weights by 1.94083731031025e-09\"\n[1] \"Raking...Iteration 25\"\n[1] \"Current iteration changed total weights by 6.12484576945072e-10\"\n[1] \"Raking...Iteration 26\"\n[1] \"Current iteration changed total weights by 1.92554375222365e-10\"\n[1] \"Raking...Iteration 27\"\n[1] \"Current iteration changed total weights by 5.94729127056937e-11\"\n[1] \"Raking...Iteration 28\"\n[1] \"Current iteration changed total weights by 1.94124161190246e-11\"\n[1] \"Raking...Iteration 29\"\n[1] \"Current iteration changed total weights by 6.34514663033769e-12\"\n[1] \"Raking...Iteration 30\"\n[1] \"Current iteration changed total weights by 3.0190572264388e-12\"\n[1] \"Raking...Iteration 31\"\n[1] \"Current iteration changed total weights by 2.23793206188816e-12\"\n[1] \"Raking...Iteration 32\"\n[1] \"Current iteration changed total weights by 0\"\n[1] \"Raking...Iteration 33\"\n[1] \"Current iteration changed total weights by 0\"\n[1] \"Raking converged in 33 iterations\"\n\n#summary(outsave)\n\n# add weights to the dataset\n\nrakedata$weight_2030_nzs  &lt;- unlist(outsave[1])\n\n# Calculate the sum of original weights\noriginal_weight_sum &lt;- sum(rakedata$weight)\n\n# Target scaling for original weights\n\noriginal_weight_scaling_factor &lt;-\n  pop_data$total_population[pop_data$year == 2030] /\n  pop_data$total_population[pop_data$year == 2022]\n\n# Scaled original weights\noriginal_weight_sum &lt;- (original_weight_sum \n                        * original_weight_scaling_factor)\n\n# Calculate the sum of the new weights\nnew_weight_sum &lt;- sum(rakedata$weight_2030_nzs)\n\n# Scale the new weights to match the sum of the original weights\nscaling_factor &lt;- original_weight_sum / new_weight_sum\nrakedata$weight_2030_nzs &lt;- rakedata$weight_2030_nzs * scaling_factor\n\n# Verify the adjustment\nhead(rakedata[, c(\"weight\", \"weight_2030_nzs\")])\n\n    weight weight_2030_nzs\n1 185.7685        175.7182\n2 185.7685        188.4089\n3 122.7176        103.4375\n4 185.7685        192.4717\n5 326.8796        272.2376\n6 326.8796        338.6745\n\nsummary(rakedata$weight_2030_nzs)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.33  102.36  143.42  153.52  193.22  527.05 \n\nsummary(rakedata$weight)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.48  109.82  156.33  154.68  192.16  326.88 \n\nrakedata &lt;- rakedata %&gt;% \n  mutate(hh_weight_2030_nzs = weight_2030_nzs / hhsize)\n\nWeights for the household database\n\n# We calculate new weights for households in the hh database\nweights_scenarios &lt;- rakedata %&gt;% \n  group_by(household_id) %&gt;%\n  summarize(\n    hh_weight_2030_baseline = \n      sum(hh_weight_2030_baseline, na.rm = TRUE),\n    hh_weight_2030_dry_hot = \n      sum(hh_weight_2030_dry_hot, na.rm = TRUE),\n    hh_weight_2030_nzs = \n      sum(hh_weight_2030_nzs, na.rm = TRUE)\n    )\n\n# We return rakedata to data frame pp_microsim and get rid of rakedata\npp_microsim &lt;- tibble(rakedata)\nrm(rakedata)\n\n\n\n7.3 Rescaling labor income according to changes to the wage bill\nAs a last step, we rescale labor income according to changes to the wage bill in the macro scenario.\n\n# Wage rescale factor by sector from macro (Agriculture, Manufacturing, Services)\nwrf_2030_baseline  &lt;- c(1.250520168, 1.336828769, 1.378384149)\nwrf_2030_dry_hot   &lt;- c(1.287103700, 1.297391076, 1.343485236)\nwrf_2030_nzs       &lt;- c(1.054278195, 1.317612666, 1.272275437)\n\n# We check the wage bill by sector\nwages_by_sector &lt;- pp_microsim %&gt;%\n  filter(!is.na(sector_w)) %&gt;% \n  group_by(sector_w, .drop = TRUE) %&gt;% \n  summarize(\n    wages_2022 = sum(annual_labor_total * weight, na.rm = TRUE),\n    wages_2030_baseline = \n      sum(annual_labor_total * weight_2030_baseline, na.rm = TRUE),\n    wages2030_dry_hot   = \n      sum(annual_labor_total * weight_2030_dry_hot, na.rm = TRUE),\n    wages2030_nzs   = \n      sum(annual_labor_total * weight_2030_nzs, na.rm = TRUE)\n  )\n\n\n# Compare how much it changed with reweighting with how it should have changed\n# Derive coefficients (wtc_2030) from that\nwages_by_sector &lt;- wages_by_sector %&gt;% \n  mutate(\n    wages_target_2030_baseline = case_when(\n      sector_w == 1 ~ wages_2022 * wrf_2030_baseline[1],\n      sector_w == 2 ~ wages_2022 * wrf_2030_baseline[2],\n      sector_w == 3 ~ wages_2022 * wrf_2030_baseline[3],\n      .default = NA\n    ),\n    wages_target2030_dry_hot = case_when(\n      sector_w == 1 ~ wages_2022 * wrf_2030_dry_hot[1],\n      sector_w == 2 ~ wages_2022 * wrf_2030_dry_hot[2],\n      sector_w == 3 ~ wages_2022 * wrf_2030_dry_hot[3],\n      .default = NA\n    ),\n    wages_target2030_nzs = case_when(\n      sector_w == 1 ~ wages_2022 * wrf_2030_nzs[1],\n      sector_w == 2 ~ wages_2022 * wrf_2030_nzs[2],\n      sector_w == 3 ~ wages_2022 * wrf_2030_nzs[3],\n      .default = NA\n    ),\n    wtc_2030_baseline = wages_target_2030_baseline / wages_2030_baseline,\n    wtc_2030_dry_hot = wages_target2030_dry_hot / wages2030_dry_hot,\n    wtc_2030_nzs = wages_target2030_nzs / wages2030_nzs\n    ) \n\n# wages_by_sector %&gt;%\n#   gt()\n# write.table(wages_by_sector, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nWe then add the coefficient to rescale each wage by sector\n\n# Assign rescale the annual and monthly wage depending on the sector\n# Quick way, but needs to be put in a sapply statement or loop\npp_microsim &lt;- pp_microsim %&gt;% \n  rename(monthly_labor_income_2022 = monthly_labor_income,\n         annual_labor_total_2022 = annual_labor_total) %&gt;% \n  mutate(\n    monthly_labor_income_2030_baseline = case_when(\n      sector_w == 1 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_baseline[1],\n      sector_w == 2 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_baseline[2],\n      sector_w == 3 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_baseline[3],\n      TRUE ~ NA\n    ),\n    annual_labor_total_2030_baseline = case_when(\n      sector_w == 1 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_baseline[1],\n      sector_w == 2 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_baseline[2],\n      sector_w == 3 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_baseline[3],\n      TRUE ~ NA\n    ),\n    monthly_labor_income_2030_dry_hot = case_when(\n      sector_w == 1 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_dry_hot[1],\n      sector_w == 2 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_dry_hot[2],\n      sector_w == 3 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_dry_hot[3],\n      TRUE ~ NA\n    ),\n    annual_labor_total_2030_dry_hot = case_when(\n      sector_w == 1 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_dry_hot[1],\n      sector_w == 2 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_dry_hot[2],\n      sector_w == 3 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_dry_hot[3],\n      TRUE ~ NA\n    ),\n    monthly_labor_income_2030_nzs = case_when(\n      sector_w == 1 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_nzs[1],\n      sector_w == 2 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_nzs[2],\n      sector_w == 3 ~ monthly_labor_income_2022 * wages_by_sector$wtc_2030_nzs[3],\n      TRUE ~ NA\n    ),\n    annual_labor_total_2030_nzs = case_when(\n      sector_w == 1 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_nzs[1],\n      sector_w == 2 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_nzs[2],\n      sector_w == 3 ~ annual_labor_total_2022 * wages_by_sector$wtc_2030_nzs[3],\n      TRUE ~ NA\n    )\n    )\n\n# This takes care of different household members coming from different sectors\nhh_li &lt;- pp_microsim %&gt;% \n  group_by(household_id) %&gt;% \n  summarize(mli_2022 = sum(monthly_labor_income_2022, na.rm = TRUE),\n            mli_2030_baseline = sum(monthly_labor_income_2030_baseline, na.rm = TRUE),\n            mli_2030_dry_hot = sum(monthly_labor_income_2030_dry_hot, na.rm = TRUE),\n            mli_2030_nzs = sum(monthly_labor_income_2030_nzs, na.rm = TRUE),\n            mli_coef_2030_baseline = if_else(mli_2022 == 0, 1, mli_2030_baseline / mli_2022),\n            mli_coef_2030_dry_hot = if_else(mli_2022 == 0, 1, mli_2030_dry_hot / mli_2022),\n            mli_coef_2030_nzs = if_else(mli_2022 == 0, 1, mli_2030_nzs / mli_2022)\n            )%&gt;% \n  select(household_id,\n         mli_2022,\n         mli_2030_baseline,\n         mli_2030_dry_hot,\n         mli_2030_nzs,\n         mli_coef_2030_baseline, \n         mli_coef_2030_dry_hot,\n         mli_coef_2030_nzs)\n  \nic_microsim &lt;- ic %&gt;% \n  left_join(hh_li, join_by(household_id == household_id)) %&gt;%\n  left_join(weights_scenarios, join_by(household_id == household_id)) %&gt;% \n  rename(inc2_2022 = inc2,\n         inc3_2022 = inc3,\n         totalinc_2022 = totalinc) %&gt;% \n  mutate(\n    mli_coef_2030_baseline = \n      if_else(\n        is.na(mli_coef_2030_baseline), 1,mli_coef_2030_baseline),\n    mli_coef_2030_dry_hot = \n      if_else(\n        is.na(mli_coef_2030_dry_hot), 1,mli_coef_2030_dry_hot),\n    mli_coef_2030_nzs = \n      if_else(\n        is.na(mli_coef_2030_nzs), 1,mli_coef_2030_nzs)\n    ) %&gt;% \n  mutate(\n    inc2_2030_baseline = inc2_2022 * mli_coef_2030_baseline,\n    inc3_2030_baseline = inc3_2022 * mli_coef_2030_baseline,\n    inc2_2030_dry_hot  = inc2_2022 * mli_coef_2030_dry_hot,\n    inc3_2030_dry_hot  = inc3_2022 * mli_coef_2030_dry_hot,\n    inc2_2030_nzs  = inc2_2022 * mli_coef_2030_nzs,\n    inc3_2030_nzs  = inc3_2022 * mli_coef_2030_nzs\n    ) %&gt;% \n  mutate(\n    totalinc_2030_baseline = \n      totalinc_2022 - coalesce(inc2_2022,0) - coalesce(inc3_2022,0) + \n      coalesce(inc2_2030_baseline,0) + coalesce(inc3_2030_baseline,0),\n    totalinc_2030_dry_hot = \n      totalinc_2022 - coalesce(inc2_2022,0) - coalesce(inc3_2022,0) + \n      coalesce(inc2_2030_dry_hot,0) + coalesce(inc3_2030_dry_hot,0),\n    totalinc_2030_nzs_noctr = \n      totalinc_2022 - coalesce(inc2_2022,0) - coalesce(inc3_2022,0) + \n      coalesce(inc2_2030_nzs,0) + coalesce(inc3_2030_nzs,0)\n    ) \n\n# Calculate quantiles and create 'breaks'\nbreaks &lt;- Hmisc::wtd.quantile(ic_microsim$totalinc_2030_nzs_noctr, \n                             weights = ic_microsim$hh_weight_2030_nzs, \n                             probs = seq(0.1, 0.9, 0.1))\n\n# Assign decile groups directly without creating an intermediate income_decile column\nic_microsim &lt;- ic_microsim %&gt;%\n  mutate(totalinc_2030_nzs_noctr =\n           if_else(is.na(totalinc_2030_nzs_noctr), 0,totalinc_2030_nzs_noctr)) %&gt;% \n  mutate(income_decile_group = cut(totalinc_2030_nzs_noctr, \n                                   breaks = c(-Inf, breaks, Inf),\n                                   labels = 1:10, \n                                   include.lowest = TRUE))\n\nincome_decile_group &lt;- ic_microsim %&gt;% \n  select(household_id, income_decile_group)\n\n\nic_microsim &lt;- ic_microsim %&gt;% \n  mutate(\n    totinc_coef_2030_baseline = \n      if_else(\n        totalinc_2022 == 0, \n        1, \n        totalinc_2030_baseline / totalinc_2022),\n    totinc_coef_2030_dry_hot = \n      if_else(\n        totalinc_2022 == 0, \n        1, \n        totalinc_2030_dry_hot / totalinc_2022),\n    totinc_coef_2030_nzs_noctr = \n      if_else(\n        totalinc_2022 == 0, \n        1, \n        totalinc_2030_nzs_noctr / totalinc_2022)\n    ) %&gt;% \n  mutate(\n    totinc_coef_2030_baseline = \n      if_else(\n        is.na(totinc_coef_2030_baseline), \n        1, \n        totinc_coef_2030_baseline),\n    totinc_coef_2030_dry_hot = \n      if_else(\n        is.na(totinc_coef_2030_dry_hot), \n        1, \n        totinc_coef_2030_dry_hot),\n    totinc_coef_2030_nzs_noctr = \n      if_else(\n        is.na(totinc_coef_2030_nzs_noctr), \n        1, \n        totinc_coef_2030_nzs_noctr)\n    )\n\n\nic_coef_scenarios &lt;- ic_microsim %&gt;%\n  select(\n    household_id, \n    totinc_coef_2030_baseline, \n    totinc_coef_2030_dry_hot,\n    totinc_coef_2030_nzs_noctr\n    )\n\nWe check that our reweighting was successful\n\n# table &lt;- pp_microsim %&gt;%\n#   group_by(cohort) %&gt;%\n# #  group_by(lmarket) %&gt;%\n# #  group_by(gender) %&gt;%\n#   summarize(no_weight = sum(n(), na.rm = TRUE),\n#             total_pp = sum(weight, na.rm = TRUE)) %&gt;%\n#   ungroup()\n# \n# table %&gt;%\n#   gt() %&gt;%\n#   fmt_number(columns = total_pp, decimals = 0)\n# \n# write.table(\n#   table,\n#   \"clipboard\", sep=\"\\t\", row.names=FALSE\n#   )",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  },
  {
    "objectID": "supporting-materials/microsimulation.html#microsimulation",
    "href": "supporting-materials/microsimulation.html#microsimulation",
    "title": "Armenia CCDR Microsimulation",
    "section": "8 Microsimulation",
    "text": "8 Microsimulation\nWe now implement different shocks according to various scenarios.\n\n8.1 Macro scenarios without additional impacts\nFor the baseline we only adjust labor income according to the reweighting procedure and rescaling of the wage bill.\n\nca_microsim &lt;- ca %&gt;% \n  left_join(weights_scenarios, join_by(household_id == household_id)) %&gt;% \n  left_join(ic_coef_scenarios, join_by(household_id == household_id)) %&gt;% \n  # We adjust total consumption by the income coefficient\n  rename(\n    tc_2022 = tc,\n    poor_Avpovln2022_2022 = poor_Avpovln2022\n    ) %&gt;% \n  mutate(\n    tc_2030_baseline   = tc_2022 * totinc_coef_2030_baseline,\n    tc_2030_dry_hot    = tc_2022 * totinc_coef_2030_dry_hot,\n    tc_2030_nzs_noctr  = tc_2022 * totinc_coef_2030_nzs_noctr\n    )\n\nHere we add back the tax revenue\nAdd back carbon tax revenue. We estimated weighted income deciles above and mapped number of households by decile. We divided the revenue by that number and we add that amount by the first four income deciles in this manner. Again, another one calculated in Excel because of time constraints. Needs fixing to make automatic.\n\nca_microsim &lt;- ca_microsim %&gt;% \n  left_join(income_decile_group, join_by(household_id == household_id)) %&gt;% \n  mutate(\n    tc_2030_nzs = \n      # Urban 40% and Rural 60%\n      case_when(\n        # Urban\n        income_decile_group == 1 & urban_rural == 1 ~ \n          tc_2030_nzs_noctr + (1546.29 * hhsize),\n        income_decile_group == 2 & urban_rural == 1 ~ \n          tc_2030_nzs_noctr + (1610.65 * hhsize),\n        income_decile_group == 3 & urban_rural == 1 ~ \n          tc_2030_nzs_noctr + (1251.14 * hhsize),\n        income_decile_group == 4 & urban_rural == 1 ~ \n          tc_2030_nzs_noctr + (1134.21 * hhsize),\n        # Rural\n        income_decile_group == 1 & urban_rural == 2 ~ \n          tc_2030_nzs_noctr + (6323.09 * hhsize),\n        income_decile_group == 2 & urban_rural == 2 ~ \n          tc_2030_nzs_noctr + (6109.07 * hhsize),\n        income_decile_group == 3 & urban_rural == 2 ~ \n          tc_2030_nzs_noctr + (5599.16 * hhsize),\n        income_decile_group == 4 & urban_rural == 2 ~ \n          tc_2030_nzs_noctr + (4420.53 * hhsize),\n        .default = tc_2030_nzs_noctr\n      )\n  )\n\nAnd recalculate poverty.\n\nca_microsim &lt;- ca_microsim %&gt;% \n  rename(\n    aec_r_2022 = aec_r,\n    weight_2022 = weight,\n    weight_2030_baseline = hh_weight_2030_baseline,\n    weight_2030_dry_hot = hh_weight_2030_dry_hot,\n    weight_2030_nzs = hh_weight_2030_nzs\n    ) %&gt;% \n  mutate(\n    aec_r_2030_baseline = \n      tc_2030_baseline / ae_r / PI,\n    aec_r_2030_dry_hot = \n      tc_2030_dry_hot  / ae_r / PI,\n    aec_r_2030_nzs = \n      tc_2030_nzs  / ae_r / PI\n    ) %&gt;% \n  # Official poverty line\n  mutate(\n    poor_Avpovln2022_2030_baseline = \n      if_else(aec_r_2030_baseline &lt; 52883, 1, 0),\n    poor_Avpovln2022_2030_dry_hot  = \n      if_else(aec_r_2030_dry_hot  &lt; 52883, 1, 0),\n    poor_Avpovln2022_2030_nzs      = \n      if_else(aec_r_2030_nzs      &lt; 52883, 1, 0)\n    )\n\nTest\n\ntest_baseline &lt;- ca_microsim %&gt;%\n  rename(\n    poor_original = poor_Avpovln2022_2022,\n    poor_2030_baseline = poor_Avpovln2022_2030_baseline,\n    poor_2030_dry_hot = poor_Avpovln2022_2030_dry_hot,\n    poor_2030_nzs = poor_Avpovln2022_2030_nzs\n    ) %&gt;%\n  group_by(poor_original) %&gt;% \n  summarize(\n    no_hh_2022 = sum(weight_2022, na.rm = TRUE),\n    no_pp_2022 = sum(weight_2022 * hhsize, na.rm = TRUE),\n    # no_hh_baseline = sum(weight_2030_baseline, na.rm = TRUE),\n    # no_pp_baseline = sum(weight_2030_baseline * hhsize, na.rm = TRUE),\n    # no_hh_dry_hot = sum(weight_2030_dry_hot, na.rm = TRUE),\n    # no_pp_dry_hot = sum(weight_2030_dry_hot * hhsize, na.rm = TRUE),\n    # no_hh_nzs = sum(weight_2030_nzs, na.rm = TRUE),\n    # no_pp_nzs = sum(weight_2030_nzs * hhsize, na.rm = TRUE)\n    ) %&gt;% \n  ungroup()\n\ntest_baseline %&gt;% \n  gt()\n\n\n\n\n\n\n\nPoor, Avpovln2022\nno_hh_2022\nno_pp_2022\n\n\n\n\n0\n659638.8\n2149568.6\n\n\n1\n143688.2\n704957.6\n\n\n\n\n\n\nwrite.table(test_baseline, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nWe plot the distributions in Figure 2.\n\n\n\n\nListing 12: Plotting equivalized consumption per capita distribution by scenario\n\n\n# Basic density plot comparing equivalized consumption per capita\nggplot(ca_microsim) +\n  geom_density(\n    data = ca_microsim,\n    aes(x = aec_r_2022, fill = 'Baseline 2022'),\n    alpha = 0.4) +\n  geom_density(\n    data = ca_microsim,\n    aes(x = aec_r_2030_nzs, fill = 'NZS 2030'),\n    alpha = 0.4) +\n  geom_density(\n    data = ca_microsim,\n    aes(x = aec_r_2030_dry_hot, fill = 'Dry/Hot 2030'),\n    alpha = 0.4) +\n  geom_density(\n    data = ca_microsim,\n    aes(x = aec_r_2030_baseline, fill = 'Baseline 2030'),\n    alpha = 0.4) +\n  labs(\n    fill = \"Scenario Variant\",\n    # title = \"Comparison of Consumption Distributions\",\n    x = \"Equivalized consumption (Dram)\",\n    y = \"Probability\") +\n  theme_minimal()+\n  coord_cartesian(xlim = c(0, 500000),\n          #        ylim = c(0.000005,0.0000160)\n                  ) + # Zoom in without removing data\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)+\n  geom_vline(xintercept = 55883,\n             color = \"red\",\n             linetype = \"dotted\",\n             linewidth =0.8) +\n  annotate(\"text\",\n           x = 55883,\n           y = 0.0000025,\n           #label = \"Poverty line\\nAMD 55,883\",\n           label = \"Poverty line\",\n           color = \"black\",\n           hjust = -0.1,\n           # vjust = -3.5,\n           #angle = 90,\n           size = 3)\n\n\n\n\n\n\n\n\n\n\nFigure 2: Equivalized consumption per capita distribution by scenario\n\n\n\n\n\nAnd we also plot the cumulative distributions to make the distinctions more evident in Figure 3.\n\n\n\n\nListing 13: Plotting equivalized consumption per capita cumulative distribution by scenario\n\n\n# Plot the cumulative distribution with left-facing arrows\nggplot(ca_microsim)+\n  stat_ecdf(data = ca_microsim,\n            aes(x = aec_r_2030_nzs, color = 'NZS 2030')) +\n  stat_ecdf(data = ca_microsim,\n            aes(x = aec_r_2030_dry_hot, color = 'Dry/Hot 2030')) +\n  stat_ecdf(data = ca_microsim,\n            aes(x = aec_r_2030_baseline, color = 'Baseline 2030')) +\n  stat_ecdf(data = ca_microsim,\n            aes(x = aec_r_2022, color = 'Baseline 2022')) +\n  labs(\n    color = \"Scenario Variant\",\n    # title = \"Comparison of Cumulative Consumption Distributions\",\n    x = \"Equivalized consumption (Dram)\",\n    y = \"Cumulative Probability\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(40000, 110000)) +\n  scale_x_continuous(labels = scales::comma)\n  # annotate(\"segment\", \n  #          x = 70000, \n  #          xend = 65000, \n  #          y = 0.2, \n  #          yend = 0.2,\n  #          arrow = arrow(length = unit(0.3, \"cm\")), \n  #          color = \"black\") +\n  # annotate(\"text\", \n  #          x = 72500, \n  #          y = 0.2,\n  #          label = \"Shift due to scenario conditions\", \n  #          hjust = 0)\n\n\n\n\n\n\n\n\n\n\nFigure 3: Equivalized consumption per capita cumulative distribution by scenario\n\n\n\n\n\n\n\n8.2 Climate change\nIn these sections we use Administrative Level 1 data on yield losses and labor productivity losses due to climate change that are provided in the study commissioned for Armenia’s CCDR Estimating the Economic Impacts of Climate Change in Armenia (Strzepek et al., 2024).\nIn the climate change scenario, we ask ourselves, what would happen if agriculture revenues from crops and livestock are reduced due to losses in productivity due to heat? For this, we use crops data.\nWe add a moving window average and max value for our labor productivity data.\n\n# First calculate moving window average\nlabor_productivity &lt;- labor_productivity %&gt;%\n  group_by(ADM1_EN, \n           clim_scenario) %&gt;%\n  arrange(year) %&gt;%\n  # Moving window average 5 years before, 5 after\n  mutate(\n    moving_avg = rollapply(\n      pct_change_productivity,\n      width = 11,\n      FUN = mean,\n      partial = TRUE,\n      align = \"center\",\n      fill = NA,\n      na.rm = TRUE\n    ),\n    # Moving window max value 5 years before, 5 after\n    # Since it's expressed in negative values (min) is the maximum\n    moving_max = rollapply(\n      pct_change_productivity,\n      width = 11,\n      FUN = min,\n      partial = TRUE,\n      align = \"center\",\n      fill = NA,\n      na.rm = TRUE\n    )\n  ) %&gt;%\n  ungroup()\n\n# Clim scenarios to select\ncs &lt;- unique(labor_productivity$clim_scenario)\n\n# Moving average for year of interest\nlab_loss_avg &lt;- labor_productivity %&gt;%\n  filter(clim_scenario == cs[1], year == analysis_years[1]) %&gt;%\n  select(-pct_change_productivity,\n         -ADM1_PCODE,\n         -year,\n         -clim_scenario,\n         -moving_max) %&gt;%\n  pivot_wider(names_from = sector, values_from = moving_avg) %&gt;%\n  rename(agr_avg = Agriculture,\n         man_avg = Manufacturing,\n         ser_avg = Services)\n\n# Max value for year of interest\nlab_loss_max &lt;- labor_productivity %&gt;%\n  filter(clim_scenario == cs[1], year == analysis_years[1]) %&gt;%\n  select(-pct_change_productivity,\n         -ADM1_PCODE,\n         -year,\n         -clim_scenario,\n         -moving_avg) %&gt;%\n  pivot_wider(names_from = sector, values_from = moving_max) %&gt;%\n  rename(agr_max = Agriculture,\n         man_max = Manufacturing,\n         ser_max = Services)\n\nWe add a moving window average and max value for our crops and livestock productivity data.\n\n# First calculate moving window average\ncrops_productivity &lt;- crops_productivity %&gt;%\n  group_by(marz, climate_scenario) %&gt;%\n  arrange(year) %&gt;%\n  # Moving window average\n  mutate(\n    moving_avg = rollapply(\n      pct_change_prod,\n      width = 11,\n      # 5 years before, 5 after + reference year = 11\n      FUN = mean,\n      partial = TRUE,\n      align = \"center\",\n      fill = NA,\n      na.rm = TRUE\n    ),\n    # Moving window max value 5 years before, 5 after\n    # Since it's expressed in negative values (min) is the maximum\n    moving_max = rollapply(\n      pct_change_prod,\n      width = 11,\n      FUN = min,\n      partial = TRUE,\n      align = \"center\",\n      fill = NA,\n      na.rm = TRUE\n    )\n  ) %&gt;%\n  ungroup()\n\n# Clim scenarios to select\ncs &lt;- unique(crops_productivity$climate_scenario)\n\n# Moving average for year of interest\ncrops_pdcvty_loss &lt;- crops_productivity %&gt;%\n  filter(climate_scenario == cs[1], \n         year == analysis_years[1]) %&gt;%\n  select(-pct_change_prod, \n         -GID_1, \n         -year, \n         -climate_scenario) %&gt;%\n  rename(crops_avg_loss = moving_avg, \n         crops_max_loss = moving_max)\n\nAnd we do the same for livestock productivity.In this case, there is also disaggregation by Marz.\n\n# First calculate moving window average\nlivestock_productivity &lt;- livestock_productivity %&gt;%\n  group_by(marz, climate.scenario) %&gt;%\n  arrange(year) %&gt;%\n  # Moving window average\n  mutate(\n    moving_avg = rollapply(\n      pct_change_prod,\n      width = 11,\n      # 5 years before, 5 after + reference year = 11\n      FUN = mean,\n      partial = TRUE,\n      align = \"center\",\n      fill = NA,\n      na.rm = TRUE\n    ),\n    # Moving window max value 5 years before, 5 after\n    # Since it's expressed in negative values (min) is the maximum\n    moving_max = rollapply(\n      pct_change_prod,\n      width = 11,\n      FUN = min,\n      partial = TRUE,\n      align = \"center\",\n      fill = NA,\n      na.rm = TRUE\n    )\n  ) %&gt;%\n  ungroup()\n\n# Clim scenarios to select\ncs &lt;- unique(livestock_productivity$climate.scenario)\n\n# Moving average for year of interest\nlvstk_pdcvty_loss &lt;- livestock_productivity %&gt;%\n  filter(climate.scenario == cs[1], \n         year == analysis_years[1]) %&gt;%\n  select(-pct_change_prod, \n         -year, \n         -climate.scenario) %&gt;%\n  rename(lvstk_avg_loss = moving_avg, \n         lvstk_max_loss = moving_max)\n\nAnd then we introduce these values in our ag income and labor income data. First, we attach the percentage losses to the appropriate data set.\n\n# Persons processed dataset\npp_microsim_cc &lt;- pp_microsim %&gt;%\n  left_join(lab_loss_avg, \n            join_by(marz == ADM1_EN)) %&gt;%\n  left_join(lab_loss_max, \n            join_by(marz == ADM1_EN))\n\n# Household income processed dataset\nic_microsim_cc &lt;- ic_microsim %&gt;%\n  left_join(crops_pdcvty_loss, \n            join_by(marz == marz)) %&gt;% \n  left_join(lvstk_pdcvty_loss,\n            join_by(marz))\n\n##write.table(lab_loss_avg, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nAnd we first shock labor income.\n\n# Labor income according to sector\npp_microsim_cc &lt;- pp_microsim_cc %&gt;%\n  mutate(sector = as.numeric(sector)) %&gt;%\n  mutate(\n    mli_2030_baseline_lab_avg =\n      case_when(\n        sector == 1 ~\n          monthly_labor_income_2030_baseline * \n          (1 + agr_avg),\n        sector == 2 ~\n          monthly_labor_income_2030_baseline * \n          (1 + man_avg),\n        sector == 3 ~\n          monthly_labor_income_2030_baseline * \n          (1 + ser_avg),\n        TRUE ~ NA\n      )\n  ) %&gt;%\n  mutate(\n    mli_2030_baseline_lab_max =\n      case_when(\n        # * 1000 because its thousands of Dram\n        sector == 1 ~\n          monthly_labor_income_2030_baseline * \n          (1 + agr_max),\n        sector == 2 ~\n          monthly_labor_income_2030_baseline * \n          (1 + man_max),\n        sector == 3 ~\n          monthly_labor_income_2030_baseline * \n          (1 + ser_max),\n        TRUE ~ NA\n      )\n  )\n\nWe aggregate at household level and take note of the percent difference between the two labor incomes, so that we can impact labor income by that amount. We don’t do it with absolute numbers because we don’t know the assumptions made by the poverty team to construct the income variable.\n\nic_new_incomes &lt;- pp_microsim_cc %&gt;%\n  group_by(household_id) %&gt;%\n  summarize(\n    mli_2030_baseline_lab_avg = \n      sum(mli_2030_baseline_lab_avg, na.rm = TRUE),\n    mli_2030_baseline_lab_max = \n      sum(mli_2030_baseline_lab_max, na.rm = TRUE),\n    mli_original = \n      sum(monthly_labor_income_2030_baseline, na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    mli_2030_baseline_lab_avg_coef =\n      if_else(\n        mli_original == 0 | is.na(mli_original),\n        1,\n        mli_2030_baseline_lab_avg / mli_original\n      ),\n    mli_2030_baseline_lab_max_coef =\n      if_else(\n        mli_original == 0 | is.na(mli_original),\n        1,\n        mli_2030_baseline_lab_max / mli_original\n      )\n  ) %&gt;%\n  ungroup()\n\nic_microsim_cc &lt;- ic_microsim_cc %&gt;%\n  left_join(ic_new_incomes, \n            join_by(household_id == household_id)) %&gt;%\n  mutate(\n    inc2_2030_baseline_lab_avg = \n      inc2_2030_baseline * mli_2030_baseline_lab_avg_coef,\n    inc2_2030_baseline_lab_max = \n      inc2_2030_baseline * mli_2030_baseline_lab_max_coef,\n    inc3_2030_baseline_lab_avg = \n      inc3_2030_baseline * mli_2030_baseline_lab_avg_coef,\n    inc3_2030_baseline_lab_max = \n      inc3_2030_baseline * mli_2030_baseline_lab_max_coef\n  )\n\nAnd now we impact agricultural income cropinc and livestock incomelvstk.\n\nic_microsim_cc &lt;- ic_microsim_cc %&gt;% \n  mutate(\n    cropinc_2030_baseline_cc_avg = \n      cropinc * (1 + crops_avg_loss),\n    cropinc_2030_baseline_cc_max = \n      cropinc * (1 + crops_max_loss),\n    lvstk_2030_baseline_cc_avg = \n      lvstk * (1 + lvstk_avg_loss),\n    lvstk_2030_baseline_cc_max = \n      lvstk * (1 + lvstk_max_loss)\n    )\n\nAnd recalculate total income.\n\nic_microsim_cc &lt;- ic_microsim_cc %&gt;%\n  mutate(\n    totalinc_2030_baseline_lab_avg =\n      totalinc_2030_baseline -\n      rowSums(select(., c(inc2_2030_baseline, \n                          inc3_2030_baseline)), na.rm = TRUE) +\n      rowSums(select(\n        ., c(inc2_2030_baseline_lab_avg, \n             inc3_2030_baseline_lab_avg)), na.rm = TRUE),\n    totalinc_2030_baseline_lab_max =\n      totalinc_2030_baseline -\n      rowSums(select(., c(inc2_2030_baseline, \n                          inc3_2030_baseline)), na.rm = TRUE) +\n      rowSums(select(\n        ., c(inc2_2030_baseline_lab_max, \n             inc3_2030_baseline_lab_max)), na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    totalinc_2030_baseline_lab_avg_coef =\n      if_else(totalinc_2030_baseline == 0, \n              1, totalinc_2030_baseline_lab_avg /\n                totalinc_2030_baseline),\n    totalinc_2030_baseline_lab_max_coef =\n      if_else(totalinc_2030_baseline == 0, \n              1, totalinc_2030_baseline_lab_max /\n                totalinc_2030_baseline)\n  ) %&gt;%\n  mutate(\n    totalinc_2030_baseline_lab_avg_coef =\n      if_else(is.na(totalinc_2030_baseline_lab_avg_coef), \n              1, totalinc_2030_baseline_lab_avg_coef),\n    totalinc_2030_baseline_lab_max_coef =\n      if_else(is.na(totalinc_2030_baseline_lab_max_coef), \n              1, totalinc_2030_baseline_lab_max_coef)\n  )\n\nWe do the same for agriculture and livestock income alone\n\nic_microsim_cc &lt;- ic_microsim_cc %&gt;%\n  mutate(\n    totalinc_2030_baseline_cc_avg =\n      totalinc_2030_baseline -\n      rowSums(select(., c(cropinc,\n                          lvstk)), na.rm = TRUE) +\n      rowSums(select(\n        ., c(cropinc_2030_baseline_cc_avg,\n             lvstk_2030_baseline_cc_avg)), na.rm = TRUE),\n    totalinc_2030_baseline_cc_max =\n      totalinc_2030_baseline -\n      rowSums(select(., c(cropinc,\n                          lvstk)), na.rm = TRUE) +\n      rowSums(select(\n        ., c(cropinc_2030_baseline_cc_max,\n             lvstk_2030_baseline_cc_max)), na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    totalinc_2030_baseline_cc_avg_coef =\n      if_else(totalinc_2030_baseline == 0, \n              1, totalinc_2030_baseline_cc_avg \n              / totalinc_2030_baseline),\n    totalinc_2030_baseline_cc_max_coef =\n      if_else(totalinc_2030_baseline == 0, \n              1, totalinc_2030_baseline_cc_max \n              / totalinc_2030_baseline)\n  ) %&gt;%\n  mutate(\n    totalinc_2030_baseline_cc_avg_coef =\n      if_else(is.na(totalinc_2030_baseline_cc_avg_coef), \n              1, totalinc_2030_baseline_cc_avg_coef),\n    totalinc_2030_baseline_cc_max_coef =\n      if_else(is.na(totalinc_2030_baseline_cc_max_coef), \n              1, totalinc_2030_baseline_cc_max_coef)\n  )\n\nAnd yet again for the combined impacts\n\nic_microsim_cc &lt;- ic_microsim_cc %&gt;%\n  mutate(\n    totalinc_2030_baseline_lab_cc_avg =\n      totalinc_2030_baseline -\n      rowSums(select(., c(inc2_2030_baseline, \n                          inc3_2030_baseline,\n                          cropinc,\n                          lvstk)), na.rm = TRUE) +\n      rowSums(select(\n        ., c(inc2_2030_baseline_lab_avg, \n             inc3_2030_baseline_lab_avg,\n             cropinc_2030_baseline_cc_avg,\n             lvstk_2030_baseline_cc_avg)), na.rm = TRUE),\n    totalinc_2030_baseline_lab_cc_max =\n      totalinc_2030_baseline -\n      rowSums(select(., c(inc2_2030_baseline, \n                          inc3_2030_baseline,\n                          cropinc,\n                          lvstk)), na.rm = TRUE) +\n      rowSums(select(\n        ., c(inc2_2030_baseline_lab_max, \n             inc3_2030_baseline_lab_max,\n             cropinc_2030_baseline_cc_max,\n             lvstk_2030_baseline_cc_max)), na.rm = TRUE)\n  ) %&gt;%\n  mutate(\n    totalinc_2030_baseline_lab_cc_avg_coef =\n      if_else(totalinc_2030_baseline == 0, \n              1, totalinc_2030_baseline_lab_cc_avg /\n                totalinc_2030_baseline),\n    totalinc_2030_baseline_lab_cc_max_coef =\n      if_else(totalinc_2030_baseline == 0, \n              1, totalinc_2030_baseline_lab_cc_max /\n                totalinc_2030_baseline)\n  ) %&gt;%\n  mutate(\n    totalinc_2030_baseline_lab_cc_avg_coef =\n      if_else(is.na(totalinc_2030_baseline_lab_cc_avg_coef), \n              1, totalinc_2030_baseline_lab_cc_avg_coef),\n    totalinc_2030_baseline_lab_cc_max_coef =\n      if_else(is.na(totalinc_2030_baseline_lab_cc_max_coef), \n              1, totalinc_2030_baseline_lab_cc_max_coef)\n    )\n\nWe assume that the loss in income translates into a loss of expenditure.\n\nincome_losses &lt;- ic_microsim_cc %&gt;% \n  select(household_id,\n         totalinc_2030_baseline_lab_avg_coef, \n         totalinc_2030_baseline_lab_max_coef,\n         totalinc_2030_baseline_cc_avg_coef,\n         totalinc_2030_baseline_cc_max_coef,\n         totalinc_2030_baseline_lab_cc_avg_coef,\n         totalinc_2030_baseline_lab_cc_max_coef)\n\n\nca_microsim_cc &lt;- ca_microsim %&gt;% \n  left_join(income_losses, join_by(household_id == household_id))\n\n# And now reduce total consumption\n\nca_microsim_cc &lt;- ca_microsim_cc %&gt;% \n  mutate(tc_2030_baseline_lab_avg = tc_2030_baseline *\n           totalinc_2030_baseline_lab_avg_coef,\n         tc_2030_baseline_lab_max = tc_2030_baseline * \n           totalinc_2030_baseline_lab_max_coef,\n         tc_2030_baseline_cc_avg = tc_2030_baseline *\n           totalinc_2030_baseline_cc_avg_coef,\n         tc_2030_baseline_cc_max = tc_2030_baseline * \n           totalinc_2030_baseline_cc_max_coef,\n         tc_2030_baseline_lab_cc_avg = tc_2030_baseline *\n           totalinc_2030_baseline_lab_cc_avg_coef,\n         tc_2030_baseline_lab_cc_max = tc_2030_baseline * \n           totalinc_2030_baseline_lab_cc_max_coef\n         ) %&gt;% \n  mutate(aec_r_2030_baseline_lab_avg = \n           tc_2030_baseline_lab_avg / ae_r / PI,\n         aec_r_2030_baseline_lab_max = \n           tc_2030_baseline_lab_max / ae_r / PI,\n         aec_r_2030_baseline_cc_avg = \n           tc_2030_baseline_cc_avg / ae_r / PI,\n         aec_r_2030_baseline_cc_max = \n           tc_2030_baseline_cc_max / ae_r / PI,\n         aec_r_2030_baseline_lab_cc_avg = \n           tc_2030_baseline_lab_cc_avg / ae_r / PI,\n         aec_r_2030_baseline_lab_cc_max = \n           tc_2030_baseline_lab_cc_max / ae_r / PI) %&gt;% \n  mutate(poor_2030_baseline_lab_avg = \n           if_else(aec_r_2030_baseline_lab_avg &lt; 52883, 1, 0),\n         poor_2030_baseline_lab_max = \n           if_else(aec_r_2030_baseline_lab_max &lt; 52883, 1, 0),\n         poor_2030_baseline_cc_avg = \n           if_else(aec_r_2030_baseline_cc_avg &lt; 52883, 1, 0),\n         poor_2030_baseline_cc_max = \n           if_else(aec_r_2030_baseline_cc_max &lt; 52883, 1, 0),\n         poor_2030_baseline_lab_cc_avg = \n           if_else(aec_r_2030_baseline_lab_cc_avg &lt; 52883, 1, 0),\n         poor_2030_baseline_lab_cc_max = \n           if_else(aec_r_2030_baseline_lab_cc_max &lt; 52883, 1, 0)\n         )\n\n# We make a table to see who became poor. \n\ntest &lt;- ca_microsim_cc %&gt;%\n  rename(poor_original = poor_Avpovln2022_2030_baseline,\n         poor_cc = poor_2030_baseline_lab_avg) %&gt;%\n  group_by(income_decile_group, urban_rural) %&gt;% \n  summarize(no_hh = round(sum(weight_2030_nzs, na.rm = TRUE)),\n            no_pp = round(sum(weight_2030_nzs * hhsize, na.rm = TRUE))) %&gt;% \n  ungroup()\n\ntest %&gt;% \n  gt()\n\n\n\n\n\n\n\nincome_decile_group\nurban_rural\nno_hh\nno_pp\n\n\n\n\n1\n1\n55581\n101934\n\n\n1\n2\n27908\n74783\n\n\n2\n1\n49717\n97861\n\n\n2\n2\n28239\n77403\n\n\n3\n1\n51704\n124898\n\n\n3\n2\n26300\n84452\n\n\n4\n1\n50879\n140052\n\n\n4\n2\n27018\n106102\n\n\n5\n1\n52758\n181706\n\n\n5\n2\n24960\n105337\n\n\n6\n1\n48613\n177394\n\n\n6\n2\n29192\n131281\n\n\n7\n1\n53040\n203450\n\n\n7\n2\n25074\n115192\n\n\n8\n1\n49831\n209845\n\n\n8\n2\n28176\n137369\n\n\n9\n1\n49603\n225308\n\n\n9\n2\n28673\n151303\n\n\n10\n1\n52287\n245685\n\n\n10\n2\n25319\n141672\n\n\n\n\n\n\n##write.table(test, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n\n\n8.3 Food prices\nWe start by looking at the differences of food prices between scenarios.\n\n# We extract and reformat the price data\nprice_data &lt;- macro_data %&gt;% \n  select(year, scenario_id, starts_with( c(\"fpi\" , \"epi\") )) %&gt;% \n  rename(scenario = scenario_id) %&gt;% \n  pivot_longer(starts_with( c(\"fpi\" , \"epi\") ), \n               names_to = \"type_decile\", \n               values_to = \"index\") %&gt;%\n  mutate(decile = parse_number(type_decile)) %&gt;% \n  mutate(commodity_group = \n           case_when(\n             str_starts(type_decile, \"fpi\") ~ \"food\",\n             str_starts(type_decile, \"epi\") ~ \"energy\",\n             TRUE ~ NA_character_\n           )) %&gt;% \n  select(-type_decile) %&gt;% \n  relocate(index, .after = commodity_group)\n\n# We take a look at price information in 2030\nprice_data %&gt;% \n  filter(year == 2030) %&gt;% \n  group_by(commodity_group, scenario) %&gt;% \n  summarize(index = mean(index, na.rm = TRUE)) %&gt;% \n  gt()\n\n\n\n\n\n\n\nscenario\nindex\n\n\n\n\nenergy\n\n\nbaseline\n1.1291820\n\n\ndry_hot\n1.1248830\n\n\nnzs\n1.8014210\n\n\nfood\n\n\nbaseline\n1.0099860\n\n\ndry_hot\n1.0756960\n\n\nnzs\n0.9670056\n\n\n\n\n\n\n\nSo, we will assign a price index depending on which decile the household belonged to in the base year 2022. We will have a column for each scenario. So we manipulate our price data according to our years of interest (in this case, only 2030).\n\n# Filter `price_data` for the years of interest\nprice_data_analysis_years &lt;- price_data %&gt;%\n  filter(year %in% analysis_years)\n\n# Create a named vector for scenario indices\nscenario_indices &lt;- setNames(seq_along(scenarios), scenarios)\n\n# Create the composite string column\nprice_data_analysis_years &lt;- price_data_analysis_years %&gt;%\n  mutate(\n    scenario_index = scenario_indices[scenario],\n    composite_column = paste( scenario,year,commodity_group, sep = \"_\")\n  ) %&gt;% \n  select(decile,index,composite_column)\n\ncomposite_column_names &lt;- unique(price_data_analysis_years$composite_column)\n\nprice_data_analysis_years &lt;- price_data_analysis_years %&gt;% \n  pivot_wider(names_from=\"composite_column\", values_from = index)\n\nSo in this particular case, we don’t want to use the price index from the dry_hot scenario, but we want to use the difference between the baseline and that scenario, so we are going to do those columns by hand, but we actually have to find a way to do it programmatically against the baseline.\n\nprice_data_analysis_years &lt;- price_data_analysis_years %&gt;% \n  mutate(food_PI = dry_hot_2030_food - baseline_2030_food +1,\n         energy_PI = nzs_2030_energy - baseline_2030_energy + 1)\n\nAnd we join with our household’s dataset.\n\n# PP microsim already has decile information from previous join\nca_microsim_cc &lt;- ca_microsim_cc %&gt;% \n  left_join(price_data_analysis_years, join_by(decile==decile))\n\nSince we don’t have quantities for the aggregate food expenditure category or for the aggregate energy bundle, we assume a price of 1 in the survey year.\nWe will estimate price elasticities for a single “food” commodity from the consumption aggregate FOOD_with_prices dataset. We add decile data to the original.\n\nfood_summary &lt;- food_with_prices %&gt;% \n  left_join(deciles, join_by(household_id))\n\n# Step 1: Summarize the data at the household level\nfood_summary &lt;- food_summary %&gt;%\n  group_by(household_id, decile) %&gt;%\n  summarize(\n    total_quantity = sum(q, na.rm = TRUE),\n    weighted_price = sum(avrpr_mean * q, na.rm = TRUE) / sum(q, na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# Define a function to fit the model and extract the elasticity\nfit_model &lt;- function(data) {\n  model &lt;- lm(log(total_quantity) ~ log(weighted_price), data = data)\n  coef(model)[\"log(weighted_price)\"]\n}\n\n# Apply the model fitting function by decile\ndecile_models &lt;- food_summary %&gt;%\n  group_by(decile) %&gt;%\n  nest() %&gt;%\n  mutate(price_elasticity = map_dbl(data, fit_model)) %&gt;%\n  select(decile, price_elasticity) %&gt;% \n  mutate(price_elasticity = if_else(price_elasticity &gt;0,\n                                    price_elasticity *(-1),\n                                    price_elasticity))\n\ndecile_models\n\n# A tibble: 10 × 2\n# Groups:   decile [10]\n   decile price_elasticity\n    &lt;dbl&gt;            &lt;dbl&gt;\n 1     10         -0.206  \n 2      8         -0.00939\n 3      2         -0.360  \n 4      3         -0.428  \n 5      1         -0.483  \n 6      9         -0.118  \n 7      6         -0.116  \n 8      4         -0.299  \n 9      5         -0.253  \n10      7         -0.0598 \n\n\nLet’s add back the elasticity data to the analysis dataset.\n\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  left_join(decile_models, by = \"decile\")\n\nLet’s apply the elasticities to the new data.\n\n# Calculate the implicit price\n# Assuming implicit_price can be calculated from the expenditure (food1)\n# If we assume baseline quantity consumed is proportional to expenditure/price\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(implicit_price = food1 / food1,  # This is 1 as we don't have baseline price\n         food_quantity = food1 / implicit_price)\n\n# Calculate the percentage change in prices for each decile\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(food_1_dprice  = (baseline_2030_food - 1),\n         food_2_dprice  = (dry_hot_2030_food - 1),\n         food_PI_dprice = (food_PI - 1))\n\n# Estimate the new food consumption levels\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(\n    food_q1_sim = food_quantity * \n      (1 + food_1_dprice * price_elasticity),\n    food_q2_sim = food_quantity * \n      (1 + food_2_dprice * price_elasticity),\n    food_qPI_sim = food_quantity *\n      (1 + food_PI_dprice * price_elasticity))\n\n# Calculate the new expenditure levels\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(food_exp1_sim = food_q1_sim * baseline_2030_food,\n         food_exp2_sim = food_q2_sim * dry_hot_2030_food,\n         food_exp3_sim = food_qPI_sim * food_PI)\n\n# View the results\nprint(ca_microsim_cc %&gt;% select(decile, food1, baseline_2030_food, food_1_dprice, food_q1_sim, food_exp1_sim, food_2_dprice, food_q2_sim, food_exp2_sim,food_exp2_sim, food_PI_dprice))\n\n# A tibble: 5,184 × 10\n   decile   food1 baseline_2030_food food_1_dprice food_q1_sim food_exp1_sim\n    &lt;dbl&gt;   &lt;dbl&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n 1      9  92178.               1.01       0.00973      92073.        92969.\n 2      7 105290.               1.01       0.00963     105229.       106243.\n 3      2  22733.               1.01       0.0102       22650.        22881.\n 4      2 163379.               1.01       0.0102      162778.       164442.\n 5      4 102718.               1.01       0.0104      102400.       103461.\n 6     10 208995.               1.01       0.0104      208548.       210712.\n 7      8 116151.               1.01       0.00995     116140.       117296.\n 8      5  67911.               1.01       0.00965      67745.        68398.\n 9      6 152144.               1.01       0.00978     151971.       153457.\n10      8  80033.               1.01       0.00995      80025.        80822.\n# ℹ 5,174 more rows\n# ℹ 4 more variables: food_2_dprice &lt;dbl&gt;, food_q2_sim &lt;dbl&gt;,\n#   food_exp2_sim &lt;dbl&gt;, food_PI_dprice &lt;dbl&gt;\n\n\nLet’s plot the distributions to see changes:\n\n# Basic density plot comparing food1 and food_exp_sim\nggplot(ca_microsim_cc, aes(x = food1, fill = 'Initial Food Expenditure')) + \n  geom_density(alpha = 0.3) + \n  geom_density(\n    data = ca_microsim_cc, \n    aes(x = food_exp3_sim, fill = 'Baseline + Dry-Hot prices'), \n    alpha = 0.3) +\n  labs(\n    fill = \"Consumption Type\", \n    title = \"Comparison of Food Expenditure Distributions\", \n    x = \"Food Expenditure\", \n    y = \"Density\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 300000)) + # Adjust the xlim for zoom\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\nCalculate losses in consumer surplus and purchasing power loss.\n\n# Calculate Consumer Surplus loss for food1 and food2 scenarios\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(\n    across(\n      c(food1,\n        food_exp1_sim,\n        food_exp2_sim,\n        food_exp3_sim,\n        tc_2030_baseline_lab_cc_avg), ~replace_na(., 0))) %&gt;%\n  mutate(food1_CSloss = ((food_quantity * implicit_price) / \n                           tc_2030_baseline) * food_1_dprice * (\n                             1 + (price_elasticity / 2) * food_1_dprice),\n         food2_CSloss = ((food_quantity * implicit_price) /\n                           tc_2030_dry_hot) * food_2_dprice * (\n                             1 + (price_elasticity / 2) * food_2_dprice),\n         foodPI_CSloss = ((food_quantity * implicit_price) /\n                           tc_2030_baseline_lab_cc_avg) * food_PI_dprice * (\n                             1 + (price_elasticity / 2) * food_PI_dprice),\n         ttl_CSloss_1 = food1_CSloss,\n         ttl_CSloss_2 = food2_CSloss,\n         ttl_CSloss_PI = foodPI_CSloss)\n\n# Calculate Purchasing Power loss for food1 and food2 and PI scenarios\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(food1_PPloss = (food1 / \n                           tc_2030_baseline) * food_1_dprice,\n         food2_PPloss = (food1 / \n                           tc_2030_dry_hot) * food_2_dprice,\n         foodPI_PPloss = (food1 / \n                            tc_2030_baseline_lab_cc_avg) * food_PI_dprice,\n         ttl_PPloss_1 = food1_PPloss,\n         ttl_PPloss_2 = food2_PPloss,\n         ttl_PPloss_PI = foodPI_PPloss\n         )\n\n# Adjust total expenditure (tc) based on the purchasing power loss\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(tc_2030_baseline_food1 = tc_2030_baseline * (1 - ttl_PPloss_1),\n         tc_2030_dry_hot_food2 = tc_2030_dry_hot * (1 - ttl_PPloss_2),\n         tc_2030_baseline_lab_cc_foodPI2 = tc_2030_baseline_lab_cc_avg * (1 - ttl_PPloss_PI),\n         tc_2030_baseline_lab_cc_foodPI = tc_2030_baseline_lab_cc_avg - (food_exp3_sim - food1)\n         )\n\n# View the results\nprint(ca_microsim_cc %&gt;% select(decile, tc_2030_baseline, tc_2030_baseline_food1, tc_2030_dry_hot_food2, tc_2030_baseline_lab_cc_foodPI))\n\n# A tibble: 5,184 × 5\n   decile tc_2030_baseline tc_2030_baseline_food1 tc_2030_dry_hot_food2\n    &lt;dbl&gt;            &lt;dbl&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;\n 1      9          298269.                297372.               291365.\n 2      7          345619.                344605.               331893.\n 3      2          130502.                130269.               126319.\n 4      2          365831.                364161.               348542.\n 5      4          285869.                284805.               272549.\n 6     10          525042.                522873.               499595.\n 7      8          232452.                231296.               223637.\n 8      5          326739.                326084.               315646.\n 9      6          479418.                477930.               458896.\n10      8          197781.                196985.               188009.\n# ℹ 5,174 more rows\n# ℹ 1 more variable: tc_2030_baseline_lab_cc_foodPI &lt;dbl&gt;\n\n\nOkay so now we estimate new welfare and poverty.\n\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(aec_r_2030_baseline_food1 = tc_2030_baseline_food1 / ae_r / PI,\n         aec_r_2030_dry_hot_food2 = tc_2030_dry_hot_food2 / ae_r / PI,\n         aec_r_2030_baseline_foodPI = tc_2030_baseline_lab_cc_foodPI / ae_r / PI) %&gt;%\n  mutate(poor_2030_baseline_food1 =\n           if_else(aec_r_2030_baseline_food1 &lt; 52883, 1, 0),\n         poor_2030_dry_hot_food2 =\n           if_else(aec_r_2030_dry_hot_food2 &lt; 52883, 1, 0),\n         poor_2030_baseline_lab_cc_foodPI =\n           if_else(aec_r_2030_baseline_foodPI &lt; 52883, 1, 0))\n\nAnd now we see who became poor\n\n# We make a table to see who became poor. \ntest &lt;- ca_microsim_cc\n\ntest &lt;- test%&gt;%\n  rename(poor_original = poor_Avpovln2022_2030_baseline,\n         poor_cc = poor_2030_baseline_lab_cc_avg,\n         poor_food1 = poor_2030_baseline_food1,\n         poor_food2 = poor_2030_dry_hot_food2,\n         poor_foodPI =  poor_2030_baseline_lab_cc_foodPI) %&gt;%\n  group_by(poor_original) %&gt;% \n  summarize(no_hh = round(sum(weight_2030_baseline, na.rm = TRUE)),\n            no_pp = round(sum(weight_2030_baseline*hhsize, na.rm = TRUE)))\n\ntest %&gt;% \n  gt()\n\n\n\n\n\n\n\npoor_original\nno_hh\nno_pp\n\n\n\n\n0\n697740\n2412949\n\n\n1\n85849\n420079\n\n\n\n\n\n\n##write.table(test, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nAnd we map these results.\n\n# foodpoor &lt;- ca_microsim_cc %&gt;%\n#   mutate(new_poor_food_base = if_else(\n#     poor_cc_avg_food2 == 1 & poor_cc_avg == 0, 1, 0),\n#          new_poor_food_dryhot = if_else(\n#            poor_cc_avg_food1 == 1 & poor_cc_avg == 0, 1, 0),\n#     marz = as_factor(marz)) %&gt;% \n#   mutate(marz = if_else(marz == \"VayotsDzor\", \"Vayots Dzor\", marz)) %&gt;% \n#   mutate(marz = if_else(marz == \"Sjunik\", \"Syunik\", marz)) %&gt;% \n#   select(marz, poor_Avpovln2022, poor_cc_avg, poor_cc_max,\n#          poor_cc_avg_food1, poor_cc_avg_food2, new_poor_food_base,\n#          new_poor_food_dryhot, weight, hhsize)\n# \n# fp &lt;-foodpoor %&gt;% \n#   group_by(marz) %&gt;% \n#   summarize(new_poor = round(sum(new_poor_food_dryhot * weight*hhsize, na.rm = TRUE))) %&gt;% \n#   mutate(label = paste0(marz,\" (\", new_poor, \")\"))\n# \n# \n# ##write.table(fp, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n# fp_map &lt;- adm1 |&gt; \n#   left_join(fp, join_by(marz == marz))\n# \n# fp_map &lt;-tm_shape(fp_map)+\n#   tm_polygons(\"new_poor\", legend.show = FALSE) +\n#   tm_text(\"label\", size = .7, col = \"black\")+\n#   tm_layout(legend.position = c(\"right\", \"top\"), \n#             title= \"Additional Poor Dry-Hot Scenario\", \n#             title.position = c('left', 'bottom'),\n#             title.size = 0.9)\n# \n# fp_map\n\nLet’s plot how the distribution moves with all these measures.\n\n# Basic density plot comparing equivalized consumption per capita\nggplot(ca_microsim_cc, \n       aes(x = aec_r_2030_baseline_foodPI, fill = 'Direct CC + Food Price')) +\n  geom_density(alpha = 0.5) +\n  # geom_density(\n  #   data = ca_microsim_cc,\n  #   aes(x = aec_r_2030_dry_hot, fill = 'Dry/Hot'),\n  #   alpha = 0.5) +\n  # geom_density(\n  #   data = ca_microsim_cc,\n  #   aes(x = aec_r_2030_dry_hot_food2, fill = 'Dry/Hot + Food Price'),\n  #   alpha = 0.5) +\n  geom_density(\n    data = ca_microsim_cc,\n    aes(x = aec_r_2030_baseline_lab_cc_avg, fill = 'Direct CC'),\n    alpha = 0.5) +\n  geom_density(\n    data = ca_microsim_cc,\n    aes(x = aec_r_2030_baseline, fill = 'Baseline'),\n    alpha = 0.5) +\n  labs(\n    fill = \"Scenario Variant\", \n    # title = \"Comparison of Consumption Distributions\", \n    x = \"Equivalized consumption (Dram)\", \n    y = \"Probability\") +\n  theme_minimal()+\n  coord_cartesian(xlim = c(20000, 150000),\n                  ylim = c(0.000005,0.0000160)) + # Zoom in without removing data\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)+\n  geom_vline(xintercept = 55883, \n             color = \"red\", \n             linetype = \"dotted\", \n             linewidth =0.8) +\n  annotate(\"text\", \n           x = 55883, \n           y = 0.0000110, \n           #label = \"Poverty line\\nAMD 55,883\", \n           label = \"Poverty line\", \n           color = \"black\", \n           hjust = -0.1, \n           # vjust = -3.5,\n           #angle = 90, \n           size = 3)\n\n\n\n\n\n\n\n\nAnd we also plot the cumulative distributions.\n\n# Plot the cumulative distribution with left-facing arrows\nggplot(ca_microsim_cc, \n       aes(x = aec_r_2030_baseline_foodPI, color = 'Direct CC + Food Price')) +\n  stat_ecdf(geom = \"step\") +\n  # stat_ecdf(data = ca_microsim_cc, \n  #           aes(x = aec_r_2030_baseline_lab_avg, color = 'Baseline + Labor Productivity')) +\n  stat_ecdf(data = ca_microsim_cc, \n            aes(x = aec_r_2030_baseline_lab_cc_avg, color = 'Direct CC')) +\n  stat_ecdf(data = ca_microsim_cc, \n            aes(x = aec_r_2030_baseline, color = 'Baseline')) +\n  labs(\n    color = \"Scenario Variant\", \n    # title = \"Comparison of Cumulative Consumption Distributions\", \n    x = \"Equivalized consumption (Dram)\", \n    y = \"Cumulative Probability\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(40000, 110000)) + \n  scale_x_continuous(labels = scales::comma) +\n  # geom_vline(xintercept = 55883, \n  #            color = \"red\", \n  #            linetype = \"dotted\", \n  #            linewidth = 0.8) +\n  # annotate(\"text\", \n  #          x = 55883, \n  #          y = 0.5, \n  #          label = \"Poverty line\", \n  #          color = \"black\", \n  #          hjust = -0.1, \n  #          size = 3) +\n  annotate(\"segment\", x = 70000, xend = 65000, y = 0.2, yend = 0.2, \n           arrow = arrow(length = unit(0.3, \"cm\")), color = \"black\") +\n  annotate(\"text\", x = 72500, y = 0.2, label = \"Shift due to shocks\", hjust = 0) \n\n\n\n\n\n\n\n  # annotate(\"segment\", x = 80000, xend = 75000, y = 0.4, yend = 0.4, \n  #          arrow = arrow(length = unit(0.3, \"cm\")), color = \"black\") +\n  # annotate(\"text\", x = 82500, y = 0.4, label = \"Shift due to shocks\", hjust = 0)\n\n\n\n8.4 Energy prices\nWe first establish energy elasticities. We only have quantities for liquefied gas hous_29_a and their purchase value hous_29_b with which we can compute price. Unfortunately there is no quantity for electricity, so we will use the same elasticity. We do not compute an elasticity by decile, because there are too few observations per decile, so we estimate an overall elasticity for all the distribution.\n\n# We extract the liquefied gas (hous_29), natural gas (hous_38) \n# and electricity (hous_23) information\nenergy_summary_all &lt;- hh %&gt;% \n  mutate(l_gas_price = \n           if_else(hous_29_a == 0, 0, hous_29_b/hous_29_a),\n         n_gas_price = \n           if_else(hous_36_a == 0, 0, hous_36_b/hous_36_a)) %&gt;%\n  select(household_id, weight, hous_29_a, hous_29_b,hous_23,\n         hous_36_a, hous_36_b, l_gas_price, n_gas_price)\n\n# We estimate the weighted mean of liquefied gas prices\navg_l_gas_price &lt;- weighted.mean(energy_summary_all$l_gas_price,\n                               energy_summary_all$weight,\n                               na.rm=TRUE)\n\n# And do the same for natural gas\navg_n_gas_price &lt;- weighted.mean(energy_summary_all$n_gas_price,\n                               energy_summary_all$weight,\n                               na.rm=TRUE)\n\n# We replace missing 0 values with average gas price\nenergy_summary_all &lt;- energy_summary_all %&gt;% \n  mutate(l_gas_price = if_else(l_gas_price==0.0,\n                             avg_l_gas_price,\n                             l_gas_price),\n         n_gas_price = if_else(n_gas_price==0.0,\n                             avg_n_gas_price,\n                             n_gas_price))\n\n# We subset to compute a single elasticity value for the entire distribution\n# Summarize the data at the household level\nl_energy_summary &lt;- energy_summary_all %&gt;%\n  filter(!is.na(l_gas_price))# %&gt;% \n \n# Filter out rows with non-positive values in hous_29_a or l_gas_price\nl_energy_summary &lt;- l_energy_summary[l_energy_summary$hous_29_a &gt; 0 & l_energy_summary$l_gas_price &gt; 0, ]\n\n# Compute the log of quantity and price\nl_energy_summary$log_gas_quantity &lt;- log(l_energy_summary$hous_29_a)\nl_energy_summary$log_l_gas_price &lt;- log(l_energy_summary$l_gas_price)\n\n# Estimate a single price elasticity for the entire dataset\nmodel &lt;- lm(log_gas_quantity ~ log_l_gas_price, data = l_energy_summary)\nsummary_model &lt;- summary(model)\n\n# Extract the price elasticity (coefficient of log_l_gas_price)\nl_gas_price_elasticity &lt;- coef(summary_model)[\"log_l_gas_price\", \"Estimate\"]\n\n# Print the results\nprint(summary_model)\n\n\nCall:\nlm(formula = log_gas_quantity ~ log_l_gas_price, data = l_energy_summary)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.79267 -0.42107 -0.01872  0.50991  1.87661 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      2.33837    0.63292   3.695  0.00025 ***\nlog_l_gas_price -0.08781    0.10115  -0.868  0.38587    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5834 on 412 degrees of freedom\nMultiple R-squared:  0.001826,  Adjusted R-squared:  -0.0005972 \nF-statistic: 0.7535 on 1 and 412 DF,  p-value: 0.3859\n\nprint(paste(\"Estimated price elasticity of gas quantity demanded:\", l_gas_price_elasticity))\n\n[1] \"Estimated price elasticity of gas quantity demanded: -0.0878078581985716\"\n\n\nWe see that this commodity is highly inelastic at -0.088781.The estimated price elasticity of -0.086 suggests that the demand for gas is inelastic. This means that a 1% increase in the price of gas would lead to only a 0.09% decrease in the quantity of gas demanded. The absolute value of the elasticity is much less than 1, indicating that consumers do not significantly reduce their gas consumption in response to price increases. This could be because gas is a necessity for many households, and they cannot easily reduce their usage or switch to alternative sources. We expect electricity, being so universal in the dataset to behave in the same manner. We wanted to use natural gas to compute a similar metric, but there is hardly any variation in prices. Everybody experiences the same price and so there is not enough variation to compute a valid model. We will use the elasticity from liquefied gas for our purposes.\nLet’s add back the elasticity data to the analysis dataset.\n\nca_microsim_cc$l_gas_price_elasticity &lt;- l_gas_price_elasticity\nca_microsim_cc &lt;- ca_microsim_cc %&gt;% \n  left_join(energy_summary_all, join_by(household_id==household_id))\n\nLet’s apply the elasticities to the new data.\n\n# Calculate the implicit price\n# Assuming implicit_price can be calculated from the expenditure\n# If we assume baseline quantity consumed is proportional to expenditure/price\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(\n    # This is 1 as we don't have baseline price\n    electricity_implicit_price = if_else(hous_23 == 0,1,hous_23 / hous_23),  \n    electricity_quantity = hous_23 / electricity_implicit_price,\n    energy_price_elasticity = l_gas_price_elasticity)\n\nEM_elec_price &lt;- 1.071353 # Price increase in Energy Model\nEM_gas_price &lt;- 1.025514\n\n# Calculate the percentage change in prices by decile\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(energy_baseline_dprice  = (energy_PI - 1),\n         energy_nzs_dprice = (nzs_2030_energy - 1),\n         EM_elec_dprice = (EM_elec_price-1), # Interpolation from Energy Model\n         EM_gas_dprice = (EM_gas_price-1)) \n\n# Estimate the new energy consumption levels\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(\n    electricity_baseline_q_sim = electricity_quantity * \n      (1 + energy_baseline_dprice * energy_price_elasticity),\n    electricity_nzs_q_sim = electricity_quantity *\n      (1 + energy_nzs_dprice * energy_price_elasticity),\n    electricity_EM_q_sim = electricity_quantity *\n      (1 + EM_elec_dprice * energy_price_elasticity),\n    l_gas_baseline_q_sim = hous_29_a * \n      (1 + energy_baseline_dprice * energy_price_elasticity),\n    l_gas_nzs_q_sim = hous_29_a *\n      (1 + energy_nzs_dprice * energy_price_elasticity),\n    l_gas_EM_q_sim = hous_29_a *\n      (1 + EM_gas_dprice * energy_price_elasticity),\n    n_gas_baseline_q_sim = hous_36_a * \n      (1 + energy_baseline_dprice * energy_price_elasticity),\n    n_gas_nzs_q_sim = hous_36_a *\n      (1 + energy_nzs_dprice * energy_price_elasticity),\n    n_gas_EM_q_sim = hous_36_a *\n      (1 + EM_gas_dprice * energy_price_elasticity)\n    )\n\n# Calculate the new expenditure levels\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(electricity_baseline_sim = \n           electricity_baseline_q_sim * energy_PI, #because e-price = 1\n         l_gas_baseline_sim = \n           l_gas_baseline_q_sim * l_gas_price * energy_PI,\n         n_gas_baseline_sim = \n           n_gas_baseline_q_sim * n_gas_price * energy_PI,\n         electricity_nzs_sim = \n           electricity_nzs_q_sim * nzs_2030_energy,#because e-price = 1\n         l_gas_nzs_sim = \n           l_gas_nzs_q_sim * l_gas_price * nzs_2030_energy,\n         n_gas_nzs_sim = \n           n_gas_nzs_q_sim * n_gas_price * nzs_2030_energy,\n         electricity_EM_sim = \n           electricity_EM_q_sim * EM_elec_price,#because e-price = 1\n         l_gas_EM_sim = \n           l_gas_EM_q_sim * l_gas_price * EM_gas_price,\n         n_gas_EM_sim = \n           n_gas_EM_q_sim * n_gas_price * EM_gas_price)\n\nLet’s plot the distributions to see changes:\n\n# Basic density plot comparing food1 and food_exp_sim\nggplot(ca_microsim_cc, aes(x =  hous_23, fill = 'Baseline')) + \n  geom_density(alpha = 0.3) + \n  geom_density(\n    data = ca_microsim_cc,\n    aes(x = electricity_baseline_sim, fill = 'Baseline + NSZ Energy Prices'),\n    alpha = 0.3) +\n   geom_density(\n    data = ca_microsim_cc,\n    aes(x = electricity_EM_sim, fill = 'NSZ + NSZ Energy Prices'),\n    alpha = 0.3) +\n  #facet_wrap(~decile)+\n  labs(\n    fill = \"Consumption Type\", \n    title = \"Comparison of Electricity Expenditure Distributions\", \n    x = \"Energy Expenditure\", \n    y = \"Density\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 40000)) + # Adjust the xlim for zoom\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\n\n# Basic density plot comparing food1 and food_exp_sim\n# ggplot(ca_microsim_cc, aes(x =  hous_23+hous_29_b+hous_36_b, fill = 'Baseline')) + \n#   geom_density(alpha = 0.3) + \n#   geom_density(\n#     data = ca_microsim_cc,\n#     aes(x = electricity_baseline_sim+l_gas_baseline_sim+n_gas_baseline_sim, fill = 'Baseline + NSZ Energy Prices'),\n#     alpha = 0.3) +\n#    geom_density(\n#     data = ca_microsim_cc,\n#     aes(x = electricity_nzs_sim+l_gas_nzs_sim+n_gas_nzs_sim, fill = 'NSZ + NSZ Energy Prices'),\n#     alpha = 0.3) +\n#   # facet_wrap(~decile)+\n#   labs(\n#     fill = \"Consumption Type\", \n#     title = \"Comparison of Energy Expenditure Distributions\", \n#     x = \"Energy Expenditure\", \n#     y = \"Density\") +\n#   theme_minimal() +\n#   coord_cartesian(xlim = c(0, 100000)) + # Adjust the xlim for zoom\n#   scale_x_continuous(labels = scales::comma) +\n#   scale_y_continuous(labels = scales::comma)\n\nCalculate losses in consumer surplus and purchasing power loss.\n\n# Calculate Purchasing Power loss for food1 and food2 and PI scenarios\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(\n    across(\n      c(hous_23,\n        hous_29_b,\n        hous_36_b, \n        tc_2030_baseline,\n        tc_2030_nzs,\n        l_gas_baseline_sim,\n        n_gas_baseline_sim,\n        electricity_baseline_sim,\n        l_gas_nzs_sim,\n        n_gas_nzs_sim,\n        electricity_nzs_sim,\n        l_gas_EM_sim,\n        n_gas_EM_sim,\n        electricity_EM_sim), ~replace_na(., 0))) %&gt;%\n  mutate(energy_baseline_PPloss = ((hous_23 + hous_29_b + hous_36_b) / \n                           tc_2030_baseline) * energy_baseline_dprice,\n         energy_nzs_PPloss = ((hous_23 + hous_29_b + hous_36_b) / \n                           tc_2030_nzs) * energy_nzs_dprice,\n         ttl_PPloss_1 = energy_baseline_PPloss,\n         ttl_PPloss_2 = energy_nzs_PPloss\n         )\n\n# Adjust total expenditure (tc) based on the purchasing power loss\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  # mutate(nzs_scale_coef = if_else(\n  #   tc_2030_baseline==0, 1, tc_2030_nzs/tc_2022)) %&gt;% \n  mutate(#tc_2030_baseline_energy2 = tc_2030_baseline * (1 - ttl_PPloss_1),\n         tc_2030_baseline_energy = \n           tc_2030_baseline -\n           ((electricity_baseline_sim +\n               l_gas_baseline_sim + \n               n_gas_baseline_sim) - \n              (hous_23 + \n                 hous_36_b + \n                 hous_29_b)),\n         #tc_2030_nzs_energy2 = tc_2030_nzs * (1 - ttl_PPloss_2),\n         tc_2030_nzs_energy = \n           tc_2030_nzs -\n           ((electricity_nzs_sim +\n               l_gas_nzs_sim + \n               n_gas_nzs_sim) - \n              (hous_23 + \n                 hous_36_b + \n                 hous_29_b)),\n         tc_2030_EM_baseline_energy = \n           tc_2030_baseline -\n           ((electricity_EM_sim +\n               l_gas_EM_sim + \n               n_gas_EM_sim) - \n              (hous_23 + \n                 hous_36_b + \n                 hous_29_b)),\n         tc_2030_EM_nzs_energy = \n           tc_2030_nzs -\n           ((electricity_EM_sim +\n               l_gas_EM_sim + \n               n_gas_EM_sim) - \n              (hous_23 + \n                 hous_36_b + \n                 hous_29_b))\n         )\n\n# View the results\nprint(ca_microsim_cc %&gt;% select(decile, tc_2030_baseline, tc_2030_baseline_energy, tc_2030_nzs_energy, tc_2030_EM_baseline_energy,tc_2030_EM_nzs_energy))\n\n# A tibble: 5,184 × 6\n   decile tc_2030_baseline tc_2030_baseline_energy tc_2030_nzs_energy\n    &lt;dbl&gt;            &lt;dbl&gt;                   &lt;dbl&gt;              &lt;dbl&gt;\n 1      9          298269.                 250374.            242384.\n 2      7          345619.                 326164.            297265.\n 3      2          130502.                 128173.            120542.\n 4      2          365831.                 332695.            305471.\n 5      4          285869.                 261826.            234349.\n 6     10          525042.                 484837.            438555.\n 7      8          232452.                 203798.            203659.\n 8      5          326739.                 294712.            262859.\n 9      6          479418.                 439166.            395670.\n10      8          197781.                 185234.            167080.\n# ℹ 5,174 more rows\n# ℹ 2 more variables: tc_2030_EM_baseline_energy &lt;dbl&gt;,\n#   tc_2030_EM_nzs_energy &lt;dbl&gt;\n\n\nOkay so now we estimate new welfare and poverty.\n\nca_microsim_cc &lt;- ca_microsim_cc %&gt;%\n  mutate(aec_r_2030_baseline_energy = tc_2030_baseline_energy / ae_r / PI,\n         aec_r_2030_nzs_energy = tc_2030_nzs_energy / ae_r / PI,\n         aec_r_2030_EM_baseline_energy =\n           tc_2030_EM_baseline_energy / ae_r / PI,\n         aec_r_2030_EM_nzs_energy =\n           tc_2030_EM_nzs_energy / ae_r / PI) %&gt;%\n  mutate(poor_2030_baseline_energy =\n           if_else(aec_r_2030_baseline_energy &lt; 52883, 1, 0),\n         poor_2030_nzs_energy =\n           if_else(aec_r_2030_nzs_energy &lt; 52883, 1, 0),\n         poor_2030_EM_baseline_energy = \n           if_else(aec_r_2030_EM_baseline_energy &lt; 52883, 1, 0),\n         poor_2030_EM_nzs_energy = \n           if_else(aec_r_2030_EM_nzs_energy &lt; 52883, 1, 0))\n\nAnd now we see who became poor\n\n# We make a table to see who became poor. \ntest &lt;- ca_microsim_cc\n\ntest &lt;- test%&gt;%\n  rename(poor_original = poor_Avpovln2022_2030_baseline,\n         poor_baseline_energy = poor_2030_baseline_energy\n         ) %&gt;%\n  group_by(poor_original) %&gt;% \n  summarize(no_hh = round(sum(weight_2030_baseline, na.rm = TRUE)),\n            no_pp = round(sum(weight_2030_baseline*hhsize, na.rm = TRUE)))\n\ntest %&gt;% \n  gt()\n\n\n\n\n\n\n\npoor_original\nno_hh\nno_pp\n\n\n\n\n0\n697740\n2412949\n\n\n1\n85849\n420079\n\n\n\n\n\n\n#write.table(price_data_analysis_years, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nAnd we map these results.\n\n# foodpoor &lt;- ca_microsim_cc %&gt;%\n#   mutate(new_poor_food_base = if_else(\n#     poor_cc_avg_food2 == 1 & poor_cc_avg == 0, 1, 0),\n#          new_poor_food_dryhot = if_else(\n#            poor_cc_avg_food1 == 1 & poor_cc_avg == 0, 1, 0),\n#     marz = as_factor(marz)) %&gt;% \n#   mutate(marz = if_else(marz == \"VayotsDzor\", \"Vayots Dzor\", marz)) %&gt;% \n#   mutate(marz = if_else(marz == \"Sjunik\", \"Syunik\", marz)) %&gt;% \n#   select(marz, poor_Avpovln2022, poor_cc_avg, poor_cc_max,\n#          poor_cc_avg_food1, poor_cc_avg_food2, new_poor_food_base,\n#          new_poor_food_dryhot, weight, hhsize)\n# \n# fp &lt;-foodpoor %&gt;% \n#   group_by(marz) %&gt;% \n#   summarize(new_poor = round(sum(new_poor_food_dryhot * weight*hhsize, na.rm = TRUE))) %&gt;% \n#   mutate(label = paste0(marz,\" (\", new_poor, \")\"))\n# \n# \n# ##write.table(fp, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n# fp_map &lt;- adm1 |&gt; \n#   left_join(fp, join_by(marz == marz))\n# \n# fp_map &lt;-tm_shape(fp_map)+\n#   tm_polygons(\"new_poor\", legend.show = FALSE) +\n#   tm_text(\"label\", size = .7, col = \"black\")+\n#   tm_layout(legend.position = c(\"right\", \"top\"), \n#             title= \"Additional Poor Dry-Hot Scenario\", \n#             title.position = c('left', 'bottom'),\n#             title.size = 0.9)\n# \n# fp_map\n\nLet’s plot how the distribution moves with all these measures.\nFood prices\n\n# Basic density plot comparing equivalized consumption per capita\nggplot(ca_microsim_cc, \n       aes(x = aec_r_2030_nzs_energy, fill = 'NZS + Energy Price')) +\n  geom_density(alpha = 0.5) +\n  # geom_density(\n  #   data = ca_microsim_cc,\n  #   aes(x = aec_r_2030_dry_hot, fill = 'Dry/Hot'),\n  #   alpha = 0.5) +\n  geom_density(\n    data = ca_microsim_cc,\n    aes(x = aec_r_2030_baseline_energy, fill = 'Baseline + Energy Price'),\n    alpha = 0.5) +\n  # geom_density(\n  #   data = ca_microsim_cc,\n  #   aes(x = aec_r_2030_baseline_lab_cc_avg, fill = 'Direct CC'),\n  #   alpha = 0.5) +\n  geom_density(\n    data = ca_microsim_cc,\n    aes(x = aec_r_2030_baseline, fill = 'Baseline'),\n    alpha = 0.5) +\n  labs(\n    fill = \"Scenario Variant\", \n    # title = \"Comparison of Consumption Distributions\", \n    x = \"Equivalized consumption (Dram)\", \n    y = \"Probability\") +\n  theme_minimal()+\n  coord_cartesian(xlim = c(00000, 300000))+\n                 # ylim = c(0.000005,0.0000160)) + # Zoom in without removing data\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)+\n  geom_vline(xintercept = 55883, \n             color = \"red\", \n             linetype = \"dotted\", \n             linewidth =0.8) +\n  annotate(\"text\", \n           x = 55883, \n           y = 0.0000055, \n           #label = \"Poverty line\\nAMD 55,883\", \n           label = \"Poverty line\", \n           color = \"black\", \n           hjust = -0.1, \n           # vjust = -3.5,\n           #angle = 90, \n           size = 3)\n\n\n\n\n\n\n\n\nEnergy\n\n# Basic density plot comparing equivalized consumption per capita\nggplot(ca_microsim_cc, \n       aes(x = aec_r_2030_nzs_energy, fill = 'NZS + Energy Price')) +\n  geom_density(alpha = 0.5) +\n  # geom_density(\n  #   data = ca_microsim_cc,\n  #   aes(x = aec_r_2030_dry_hot, fill = 'Dry/Hot'),\n  #   alpha = 0.5) +\n  geom_density(\n    data = ca_microsim_cc,\n    aes(x = aec_r_2030_baseline_energy, fill = 'Baseline + Energy Price'),\n    alpha = 0.5) +\n  # geom_density(\n  #   data = ca_microsim_cc,\n  #   aes(x = aec_r_2030_baseline_lab_cc_avg, fill = 'Direct CC'),\n  #   alpha = 0.5) +\n  geom_density(\n    data = ca_microsim_cc,\n    aes(x = aec_r_2030_baseline, fill = 'Baseline'),\n    alpha = 0.5) +\n  labs(\n    fill = \"Scenario Variant\", \n    # title = \"Comparison of Consumption Distributions\", \n    x = \"Equivalized consumption (Dram)\", \n    y = \"Probability\") +\n  theme_minimal()+\n  coord_cartesian(xlim = c(20000, 150000),\n                  ylim = c(0.000005,0.0000160)) + # Zoom in without removing data\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::comma)+\n  geom_vline(xintercept = 55883, \n             color = \"red\", \n             linetype = \"dotted\", \n             linewidth =0.8) +\n  annotate(\"text\", \n           x = 55883, \n           y = 0.0000110, \n           #label = \"Poverty line\\nAMD 55,883\", \n           label = \"Poverty line\", \n           color = \"black\", \n           hjust = -0.1, \n           # vjust = -3.5,\n           #angle = 90, \n           size = 3)\n\n\n\n\n\n\n\n\nAnd we also plot the cumulative distributions.\nFor food prices and cc\n\n# Plot the cumulative distribution with left-facing arrows\nggplot(ca_microsim_cc, \n       aes(x = aec_r_2030_baseline_foodPI, color = 'Direct CC + Food Price')) +\n  stat_ecdf(geom = \"step\") +\n  # stat_ecdf(data = ca_microsim_cc, \n  #           aes(x = aec_r_2030_baseline_lab_avg, color = 'Baseline + Labor Productivity')) +\n  stat_ecdf(data = ca_microsim_cc, \n            aes(x = aec_r_2030_dry_hot_food2, color = 'Dry/Hot + Food Price')) +\n  stat_ecdf(data = ca_microsim_cc, \n            aes(x = aec_r_2030_baseline_lab_cc_avg, color = 'Direct CC')) +\n  stat_ecdf(data = ca_microsim_cc, \n            aes(x = aec_r_2030_baseline, color = 'Baseline')) +\n  labs(\n    color = \"Scenario Variant\", \n    # title = \"Comparison of Cumulative Consumption Distributions\", \n    x = \"Equivalized consumption (Dram)\", \n    y = \"Cumulative Probability\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(40000, 110000)) + \n  scale_x_continuous(labels = scales::comma) +\n  # geom_vline(xintercept = 55883, \n  #            color = \"red\", \n  #            linetype = \"dotted\", \n  #            linewidth = 0.8) +\n  # annotate(\"text\", \n  #          x = 55883, \n  #          y = 0.5, \n  #          label = \"Poverty line\", \n  #          color = \"black\", \n  #          hjust = -0.1, \n  #          size = 3) +\n  annotate(\"segment\", x = 70000, xend = 65000, y = 0.2, yend = 0.2, \n           arrow = arrow(length = unit(0.3, \"cm\")), color = \"black\") +\n  annotate(\"text\", x = 72500, y = 0.2, label = \"Shift due to shocks\", hjust = 0) \n\n\n\n\n\n\n\n  # annotate(\"segment\", x = 80000, xend = 75000, y = 0.4, yend = 0.4, \n  #          arrow = arrow(length = unit(0.3, \"cm\")), color = \"black\") +\n  # annotate(\"text\", x = 82500, y = 0.4, label = \"Shift due to shocks\", hjust = 0)\n\nFor energy prices\n\n# Plot the cumulative distribution with left-facing arrows\nggplot(ca_microsim_cc, \n       aes(x = aec_r_2030_EM_nzs_energy, color = 'NZS + Energy Price')) +\n  stat_ecdf(geom = \"step\") +\n  # stat_ecdf(data = ca_microsim_cc, \n  #           aes(x = aec_r_2030_baseline_lab_avg, color = 'Baseline + Labor Productivity')) +\n  stat_ecdf(data = ca_microsim_cc, \n            aes(x = aec_r_2030_EM_baseline_energy, color = 'Baseline + Energy Price')) +\n  stat_ecdf(data = ca_microsim_cc, \n            aes(x = aec_r_2030_baseline, color = 'Baseline')) +\n  labs(\n    color = \"Scenario Variant\", \n    # title = \"Comparison of Cumulative Consumption Distributions\", \n    x = \"Equivalized consumption (Dram)\", \n    y = \"Cumulative Probability\") +\n  theme_minimal() +\n  coord_cartesian(xlim = c(40000, 110000)) + \n  scale_x_continuous(labels = scales::comma) +\n  # geom_vline(xintercept = 55883, \n  #            color = \"red\", \n  #            linetype = \"dotted\", \n  #            linewidth = 0.8) +\n  # annotate(\"text\", \n  #          x = 55883, \n  #          y = 0.5, \n  #          label = \"Poverty line\", \n  #          color = \"black\", \n  #          hjust = -0.1, \n  #          size = 3) +\n  annotate(\"segment\", x = 70000, xend = 65000, y = 0.2, yend = 0.2, \n           arrow = arrow(length = unit(0.3, \"cm\")), color = \"black\") +\n  annotate(\"text\", x = 72500, y = 0.2, label = \"Shift due to shocks\", hjust = 0) \n\n\n\n\n\n\n\n  # annotate(\"segment\", x = 80000, xend = 75000, y = 0.4, yend = 0.4, \n  #          arrow = arrow(length = unit(0.3, \"cm\")), color = \"black\") +\n  # annotate(\"text\", x = 82500, y = 0.4, label = \"Shift due to shocks\", hjust = 0)\n\n\n\n8.5 Disaggregation of poverty measures\nWe bring back poverty to the people’s dataset.\n\n# We extract household poverty designations from the data\nnew_poor &lt;- ca_microsim_cc %&gt;% \n  select(household_id,\n         weight_2030_baseline,\n         weight_2030_dry_hot,\n         poor_Avpovln2022_2022, \n         poor_Avpovln2022_2030_baseline, \n         poor_Avpovln2022_2030_dry_hot,\n         poor_Avpovln2022_2030_nzs,\n         poor_2030_baseline_lab_avg, \n         poor_2030_baseline_lab_max,\n         poor_2030_baseline_cc_avg,\n         poor_2030_baseline_cc_max, \n         poor_2030_baseline_lab_cc_avg, \n         poor_2030_baseline_lab_cc_max, \n         poor_2030_baseline_food1,\n         poor_2030_dry_hot_food2,\n         poor_2030_baseline_lab_cc_foodPI,\n         poor_2030_baseline_energy,\n         poor_2030_nzs_energy,\n         poor_2030_EM_baseline_energy,\n         poor_2030_EM_nzs_energy\n         )\n\n# And merge them back into the people dataset\npp_microsim_cc &lt;- pp_microsim_cc %&gt;% \n  select(-c(poor_Avpovln2022,\n           weight_2030_baseline,\n           weight_2030_dry_hot)) %&gt;% \n  left_join(new_poor, join_by(household_id)) %&gt;% \n  mutate(female = if_else(gender == 2, 1,0),\n         youth = if_else(age &lt; 15, 1, 0))\n\nLet’s find homes where more than 50% of income comes from agriculture. We first find the fraction of household labor income that comes from agriculture.\n\nag_labinc_fraction &lt;- pp_microsim_cc %&gt;% \n  mutate(\n    ag_lab_income =\n      if_else(\n        lmarket == 1, \n        monthly_labor_income_2030_baseline, NA)\n  ) %&gt;% \n  group_by(household_id) %&gt;%\n  summarize(\n    ag_labinc =\n      sum(ag_lab_income, na.rm = TRUE),\n    hh_labinc = \n      sum(monthly_labor_income_2030_baseline, na.rm = TRUE)) %&gt;%\n  mutate(ag_lab_fraction = if_else(hh_labinc == 0, 0, ag_labinc/ hh_labinc)) %&gt;% \n  select(household_id, ag_lab_fraction)\n\nAnd then we add ag income sources and evaluate if they are at least 50% of total income\n\nag_income_50 &lt;- ic_microsim_cc %&gt;%\n  left_join(ag_labinc_fraction, join_by(household_id==household_id)) %&gt;% \n  rename(household_id = household_id) %&gt;% \n  mutate(\n    across(\n      c(inc2_2030_baseline,\n        inc3_2030_baseline,\n        inc4, \n        totalinc_2030_baseline), ~replace_na(., 0))) %&gt;%\n  mutate(\n    ag_income =\n      inc2_2030_baseline +\n      inc3_2030_baseline +\n      inc4 * ag_lab_fraction,\n    ag_fraction = if_else(\n      totalinc_2030_baseline == 0, 0, ag_income / totalinc_2030_baseline)\n    ) %&gt;%\n  mutate(\n    is_ag_home = if_else(ag_fraction &gt;= 0.5, \"Ag. HH (&gt;= 50%)\", \"Other HH\")\n  ) %&gt;% \n  select(household_id, is_ag_home)\n\nWe make a table to see who became poor.\n\n\n8.6 The table\n\ntest &lt;- pp_microsim_cc\n\ntest &lt;- test%&gt;%\n  left_join(ag_income_50, join_by(household_id)) %&gt;% \n#  filter(rural_dummy ==1 & is_ag_home == \"Ag. HH (&gt;= 50%)\") %&gt;%  \n  rename(poor_original = poor_Avpovln2022_2030_baseline,\n         poor_dh = poor_Avpovln2022_2030_dry_hot,\n         poor_nzs = poor_Avpovln2022_2030_nzs,\n         poor_lab = poor_2030_baseline_lab_avg,\n         poor_cc = poor_2030_baseline_cc_avg,\n         poor_lab_cc = poor_2030_baseline_lab_cc_avg,\n         poor_foodPI = poor_2030_baseline_lab_cc_foodPI,\n         poor_dh_food = poor_2030_dry_hot_food2,\n         poor_b_energy = poor_2030_baseline_energy,\n         poor_nzs_energy = poor_2030_nzs_energy,\n         poor_EM_b_energy = poor_2030_EM_baseline_energy,\n         poor_EM_nzs_energy = poor_2030_EM_nzs_energy\n         ) %&gt;%\n  group_by(poor_nzs, poor_EM_nzs_energy) %&gt;% \n  summarize(no_pp = sum(weight_2030_nzs, na.rm = TRUE),\n            female = sum(female*weight_2030_nzs, na.rm = TRUE),\n            male = no_pp - female,\n            youth = sum(youth*weight_2030_nzs, na.rm = TRUE),\n            non_youth = no_pp - youth,\n            rural = sum(rural_dummy *weight_2030_nzs, na.rm = TRUE),\n            urban = no_pp - rural\n            )\n\ntest %&gt;% \n  gt()\n\n\n\n\n\n\n\npoor_EM_nzs_energy\nno_pp\nfemale\nmale\nyouth\nnon_youth\nrural\nurban\n\n\n\n\n0\n\n\n0\n2377717.408\n1305259.026\n1072458.382\n450512.2559\n1927205.152\n906939.674\n1470777.734\n\n\n1\n4246.363\n2355.321\n1891.042\n678.3038\n3568.059\n3626.866\n619.497\n\n\n1\n\n\n1\n451064.184\n246546.973\n204517.211\n120979.0711\n330085.113\n214327.792\n236736.392\n\n\n\n\n\n\nwrite.table(test, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n\n# foodpoor &lt;- ca_microsim_cc %&gt;%\n#   mutate(new_poor_food_base = if_else(\n#     poor_2030_baseline_lab_cc_foodPI == 1 &\n#       poor_2030_baseline_lab_cc_avg == 0, 1, 0),\n#          new_poor_food_dryhot = if_else(\n#            poor_cc_avg_food1 == 1 & poor_cc_avg == 0, 1, 0),\n#     marz = as_factor(marz)) %&gt;% \n#   mutate(marz = if_else(marz == \"VayotsDzor\", \"Vayots Dzor\", marz)) %&gt;% \n#   mutate(marz = if_else(marz == \"Sjunik\", \"Syunik\", marz)) %&gt;% \n#   select(marz, poor_Avpovln2022_2022, \n#          poor_Avpovln2022_2030_baseline, \n#          poor_Avpovln2022_2030_dry_hot,\n#          poor_2030_baseline_lab_avg, \n#          poor_2030_baseline_lab_max,\n#          poor_2030_baseline_cc_avg,\n#          poor_2030_baseline_cc_max, \n#          poor_2030_baseline_lab_cc_avg, \n#          poor_2030_baseline_lab_cc_max, \n#          poor_2030_baseline_food1,\n#          poor_2030_dry_hot_food2,\n#          poor_2030_baseline_lab_cc_foodPI,\n#          weight_2030_baseline,\n#          weight_2030_dry_hot,\n#          hhsize)\n# \n# fp &lt;-foodpoor %&gt;% \n#   group_by(marz) %&gt;% \n#   summarize(new_poor = round(sum(new_poor_food_dryhot * weight*hhsize, na.rm = TRUE))) %&gt;% \n#   mutate(label = paste0(marz,\" (\", new_poor, \")\"))\n# \n# \n# ##write.table(fp, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n# fp_map &lt;- adm1 |&gt; \n#   left_join(fp, join_by(marz == marz))\n# \n# fp_map &lt;-tm_shape(fp_map)+\n#   tm_polygons(\"new_poor\", legend.show = FALSE) +\n#   tm_text(\"label\", size = .7, col = \"black\")+\n#   tm_layout(legend.position = c(\"right\", \"top\"), \n#             title= \"Additional Poor Dry-Hot Scenario\", \n#             title.position = c('left', 'bottom'),\n#             title.size = 0.9)\n# \n# fp_map\n\n\nnew_poor_scenarios &lt;- pp_microsim_cc %&gt;%\n  left_join(ag_income_50, join_by(household_id)) %&gt;% \n  mutate(\n    poor_baseline = poor_Avpovln2022_2030_baseline,\n    poor_dry_hot = poor_Avpovln2022_2030_dry_hot,\n    poor_nzs = poor_Avpovln2022_2030_nzs,\n    new_poor_lab_cc = if_else(\n      poor_Avpovln2022_2030_baseline == 0 &\n        poor_2030_baseline_lab_cc_avg == 1,\n      1,\n      0\n    ),\n    new_poor_lab_cc_foodPI = if_else(\n      poor_Avpovln2022_2030_baseline == 0 &\n        poor_2030_baseline_lab_cc_foodPI == 1,\n      1,\n      0\n    ),\n    new_poor_dry_hot_food2 = if_else(\n      poor_Avpovln2022_2030_dry_hot == 0 &\n        poor_2030_dry_hot_food2 == 1,\n      1,\n      0\n    ),\n    new_poor_b_energy = if_else(\n      poor_Avpovln2022_2030_baseline == 0 &\n        poor_2030_baseline_energy == 1,\n      1,\n      0\n    ),\n    new_poor_nzs_energy = if_else(\n      poor_Avpovln2022_2030_nzs == 0 &\n        poor_2030_nzs_energy == 1,\n      1,\n      0\n    ),\n    new_poor_EM_b_energy = if_else(\n      poor_Avpovln2022_2030_baseline == 0 &\n        poor_2030_EM_baseline_energy == 1,\n      1,\n      0\n    ),\n    new_poor_EM_nzs_energy = if_else(\n      poor_Avpovln2022_2030_nzs == 0 &\n        poor_2030_EM_nzs_energy == 1,\n      1,\n      0\n    )\n  ) %&gt;%\n  group_by(marz) %&gt;% # has to be marz for the next chunk to work\n  summarize(\n    total_population = sum(weight_2030_baseline, na.rm = TRUE),\n    poor_baseline =\n      sum(poor_baseline * weight_2030_baseline, na.rm = TRUE),\n    poor_dry_hot =\n      sum(poor_dry_hot * weight_2030_dry_hot, na.rm = TRUE),\n    poor_nzs =\n      sum(poor_nzs * weight_2030_nzs, na.rm = TRUE),\n    new_p_lab_cc =\n      sum(new_poor_lab_cc * weight_2030_baseline, na.rm = TRUE),\n    new_p_lab_cc_foodPI =\n      sum(new_poor_lab_cc_foodPI * weight_2030_baseline, na.rm = TRUE),\n    new_p_dry_hot_food2 =\n      sum(new_poor_dry_hot_food2 * weight_2030_dry_hot, na.rm = TRUE),\n    new_p_baseline_energy =\n      sum(new_poor_b_energy * weight_2030_baseline, na.rm = TRUE),\n    new_p_nzs_energy = \n      sum(new_poor_nzs_energy * weight_2030_nzs, na.rm = TRUE),\n    new_p_EM_baseline_energy =\n      sum(new_poor_EM_b_energy * weight_2030_baseline, na.rm = TRUE),\n    new_p_EM_nzs_energy = \n      sum(new_poor_EM_nzs_energy * weight_2030_nzs, na.rm = TRUE)\n  )\n\nwrite.table(new_poor_scenarios, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\n##write.table(test, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\nAnd we create labels for our map.\n\nnew_poor_map &lt;- adm1 %&gt;% # previous chunk has to be grouped by marz\n  left_join(new_poor_scenarios, join_by(marz)) %&gt;%\n  mutate(\n    new_p_lab_cc_pct = new_p_lab_cc / total_population *\n          100,\n    new_poor_lab_cc_foodPI_pct = new_p_lab_cc_foodPI /\n         total_population * 100,\n    new_poor_dry_hot_food2_pct = new_p_dry_hot_food2 /\n         total_population * 100,\n    new_poor_baseline_energy_pct = new_p_baseline_energy /\n      total_population*100,\n    new_poor_nzs_energy_pct = new_p_nzs_energy / total_population * 100,\n    new_poor_EM_baseline_energy_pct = new_p_EM_baseline_energy /\n      total_population*100,\n    new_poor_EM_nzs_energy_pct = new_p_EM_nzs_energy / total_population * 100\n  ) %&gt;% \n  mutate(\n    new_p_lab_cc_label    = paste0(marz, \"\\n(\", sprintf(\"%.1f%%\", new_p_lab_cc_pct), \")\"),\n    new_p_lab_cc_foodPI_label = paste0(marz, \"\\n(\", sprintf(\"%.1f%%\", new_poor_lab_cc_foodPI_pct), \")\"),\n    new_p_dry_hot_food2_label = paste0(marz, \"\\n(\", sprintf(\"%.1f%%\", new_poor_dry_hot_food2_pct), \")\"),\n    new_p_b_energy_label = paste0(marz, \"\\n(\", sprintf(\"%.1f%%\", new_poor_baseline_energy_pct), \")\"),\n    new_p_nzs_energy_label = paste0(marz, \"\\n(\", sprintf(\"%.1f%%\", new_poor_nzs_energy_pct), \")\"),\n    new_p_EM_b_energy_label = paste0(marz, \"\\n(\", sprintf(\"%.1f%%\", new_poor_EM_baseline_energy_pct), \")\"),\n    new_p_EM_nzs_energy_label = paste0(marz, \"\\n(\", sprintf(\"%.1f%%\", new_poor_EM_nzs_energy_pct), \")\")\n  )\n\nLet’s map different scenarios.\n\ntm_shape(new_poor_map)+\n  tm_polygons(\"new_p_lab_cc_pct\", title=\"Percent\", legend.show = TRUE) +\n  tm_text(c(\"new_p_lab_cc_label\"), size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"),\n            #legend.outside = TRUE,\n            title= \"New poor as a percentage of Marz population\\nDirect CC\",\n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\n\n\n\n\n\n\n\nAnd the second variant\n\ntm_shape(new_poor_map)+\n  tm_polygons(\"new_poor_lab_cc_foodPI_pct\",\n              title=\"Percent\", \n              legend.show = TRUE) +\n  tm_text(c(\"new_p_lab_cc_foodPI_label\"), size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"New poor as a percentage of Marz population\\nDirect CC + Food Price\",\n#            outer.margins=c(.10,.10, .10, .10), \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\n\n\n\n\n\n\n\nSecond variant b\n\ntm_shape(new_poor_map)+\n  tm_polygons(\"new_poor_dry_hot_food2_pct\",\n              title=\"Percent\", \n              legend.show = TRUE) +\n  tm_text(c(\"new_p_dry_hot_food2_label\"), size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"New poor as a percentage of Marz population\\nDry/Hot + Food Price\",\n#            outer.margins=c(.10,.10, .10, .10), \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\n\n\n\n\n\n\n\nAnd the third variant\n\ntm_shape(new_poor_map)+\n  tm_polygons(\"new_poor_baseline_energy_pct\",\n              title=\"Percent\", \n              legend.show = TRUE) +\n  tm_text(c(\"new_p_b_energy_label\"), size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"New poor as a percentage of Marz population\\nBaseline + Energy Price\",\n            # outer.margins=c(.10,.10, .10, .10), \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\n\n\n\n\n\n\n\nAnd the fourth variant\n\ntm_shape(new_poor_map)+\n  tm_polygons(\"new_poor_nzs_energy_pct\",\n              title=\"Percent\", \n              legend.show = TRUE) +\n  tm_text(c(\"new_p_nzs_energy_label\"), size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"New poor as a percentage of Marz population\\nNZS + Energy Price\",\n            #outer.margins=c(.10,.10, .10, .10), \n            title.position = c('left', 'bottom'),\n            title.size = 0.9,\n            asp = 1)\n\n\n\n\n\n\n\n\nFifth\n\ntm_shape(new_poor_map)+\n  tm_polygons(\"new_poor_EM_baseline_energy_pct\",\n              title=\"Percent\", \n              legend.show = TRUE) +\n  tm_text(c(\"new_p_EM_b_energy_label\"), size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"New poor as a percentage of Marz population\\nBaseline + Energy Model Price\",\n#            outer.margins=c(.10,.10, .10, .10), \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\n\n\n\n\n\n\n\nSixth\n\ntm_shape(new_poor_map)+\n  tm_polygons(\"new_poor_EM_nzs_energy_pct\",\n              title=\"Percent\", \n              legend.show = TRUE) +\n  tm_text(c(\"new_p_EM_nzs_energy_label\"), size = .7, col = \"black\")+\n  tm_layout(legend.position = c(\"right\", \"top\"), \n            title= \"New poor as a percentage of Marz population\\nNZS + Energy Model Price\",\n#            outer.margins=c(.10,.10, .10, .10), \n            title.position = c('left', 'bottom'),\n            title.size = 0.9)\n\n\n\n\n\n\n\n\nNow let’s show average losses by decile as a percentage of total spending.\n\navg_scenario_losses &lt;- ca_microsim_cc %&gt;% \n  mutate(\n    tc_loss_lab_cc = if_else(\n      (tc_2030_baseline_lab_cc_avg - tc_2030_baseline)&lt; 0,\n      (tc_2030_baseline_lab_cc_avg - tc_2030_baseline)/\n        tc_2030_baseline,NA),\n    tc_loss_lab_cc_foodPI = if_else(\n      (tc_2030_baseline_lab_cc_foodPI - tc_2030_baseline)&lt; 0,\n      (tc_2030_baseline_lab_cc_foodPI - tc_2030_baseline)/\n        tc_2030_baseline,NA)\n         ) %&gt;%\n  group_by(decile) %&gt;% \n  summarize(no_hh = \n              round(\n                sum(\n                  weight_2030_baseline, na.rm = TRUE), digits = 0\n                ),\n            avg_tc = \n              round(\n                weighted.mean(\n                  tc_2030_baseline, weight_2030_baseline, na.rm = TRUE), digits = 2\n                ),\n            avg_tc_usd = \n              round(\n                weighted.mean(\n                  tc_2030_baseline, weight_2030_baseline, na.rm = TRUE)*er, digits = 1\n                ),\n            avg_loss_lab_cc = \n              round(\n                weighted.mean(\n                  tc_loss_lab_cc, \n                  weight_2030_baseline, \n                  na.rm = TRUE), \n                digits = 4\n                ),\n            avg_loss_lab_cc_foodPI = \n              round(\n                weighted.mean(\n                  tc_loss_lab_cc_foodPI, \n                  weight_2030_baseline, \n                  na.rm = TRUE), digits = 4),\n            # avg_loss_b_energy = \n            #   round(\n            #     weighted.mean(\n            #       tc_loss_lab_cc_foodPI, \n            #       weight_2030_baseline, \n            #       na.rm = TRUE), digits = 4),\n            # avg_loss_nzs_energy = \n            #   round(\n            #     weighted.mean(\n            #       tc_loss_lab_cc_foodPI, \n            #       weight_2030_baseline, \n            #       na.rm = TRUE), digits = 4),\n            # avg_loss_EM_b_energy = \n            #   round(\n            #     weighted.mean(\n            #       tc_loss_lab_cc_foodPI, \n            #       weight_2030_baseline, \n            #       na.rm = TRUE), digits = 4),\n            # avg_loss_EM_nzs_energy = \n            #   round(\n            #     weighted.mean(\n            #       tc_loss_lab_cc_foodPI, \n            #       weight_2030_baseline, \n            #       na.rm = TRUE), digits = 4)\n            )\n\n##write.table(avg_scenario_losses, \"clipboard\", sep=\"\\t\", row.names=FALSE)\n\navg_scenario_losses %&gt;% \n  gt()\n\n\n\n\n\n\n\nDecile of aec_r, with pweight\nno_hh\navg_tc\navg_tc_usd\navg_loss_lab_cc\navg_loss_lab_cc_foodPI\n\n\n\n\n1\n54366\n150243.5\n347.1\n-0.0088\n-0.0221\n\n\n2\n59969\n173070.1\n399.8\n-0.0109\n-0.0271\n\n\n3\n61790\n197390.9\n456.0\n-0.0100\n-0.0238\n\n\n4\n66999\n208086.0\n480.7\n-0.0091\n-0.0266\n\n\n5\n73341\n210330.6\n485.9\n-0.0110\n-0.0288\n\n\n6\n80796\n213555.3\n493.3\n-0.0113\n-0.0329\n\n\n7\n80304\n236748.0\n546.9\n-0.0097\n-0.0327\n\n\n8\n88195\n244053.8\n563.8\n-0.0119\n-0.0362\n\n\n9\n96945\n254344.1\n587.5\n-0.0090\n-0.0290\n\n\n10\n120886\n310811.9\n718.0\n-0.0105\n-0.0245\n\n\n\n\n\n\n\nEnergy price index interpolation\n\n# Sample data\ndata &lt;- data.frame(\n  year = c(2020, 2030),\n  value = c(100.00000,103.18920)  \n)\n\n# Define the years for interpolation\nyears &lt;- seq(2020, 2030, by = 1)\n\n# Perform linear interpolation\ninterpolated_values &lt;- approx(data$year, data$value, xout = years)\n\n# Create a data frame with the interpolated results\ninterpolated_data &lt;- data.frame(\n  year = interpolated_values$x,\n  value = interpolated_values$y\n)\n\n# Display the result\nprint(interpolated_data)\n\n   year    value\n1  2020 100.0000\n2  2021 100.3189\n3  2022 100.6378\n4  2023 100.9568\n5  2024 101.2757\n6  2025 101.5946\n7  2026 101.9135\n8  2027 102.2324\n9  2028 102.5514\n10 2029 102.8703\n11 2030 103.1892\n\n\nThe inquiry is who is getting hit the most from changes to energy prices.\n\nenergy_shares &lt;- ca_microsim_cc %&gt;% \n  left_join(ag_income_50, join_by(household_id)) %&gt;% \n  mutate(\n    shr_energy = if_else(tc_2022==0,NA, (hous_23+hous_36_b+hous_29_b)/tc_2022)\n  ) %&gt;% \n  group_by(decile, is_ag_home) %&gt;% \n  summarize(no_hh= sum(weight_2022, na.rm = TRUE),\n            shr_e = weighted.mean(shr_energy, weight_2022, na.rm=TRUE))\n\nwrite.table(energy_shares, \"clipboard\", sep=\"\\t\", row.names=FALSE)",
    "crumbs": [
      "Home",
      "Supporting materials",
      "Armenia CCDR Microsimulation"
    ]
  }
]